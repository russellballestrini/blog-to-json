{
  "the-barrymores-stole-my-heart-then-crushed-it": {
    "name": "the-barrymores-stole-my-heart-then-crushed-it",
    "id": "1",
    "link": "http://russell2.ballestrini.net/the-barrymores-stole-my-heart-then-crushed-it/",
    "title": "The Barrymores stole my heart then crushed it",
    "content": "[caption id=\"attachment_5\" align=\"alignright\" width=\"309\" caption=\"The Barrymores\"]<img src=\"/wp-content/uploads/2010/11/thebarrymores.png\" alt=\"The Barrymores\" title=\"The Barrymores\" width=\"309\" height=\"169\" class=\"size-full wp-image-5\" />[/caption]\n\n<p>\n<strong>Recently while surfing pandora</strong> I stumbled upon a new favorite song, \"Think Straight Again\", composed by a ska band named The Barrymores. \u00a0I embraced the bands alluring melodies after the first play.\n</p>\n\n<p>\n<strong>What caught my attention?</strong>\n</p>\n\n<p>\nWell the whole band rocks, <em>hard</em>. \u00a0The main components of The Barrymores shape a full and energetic sound. \u00a0They have a drummer, a\u00a0bassist, \u00a0a lead\u00a0guitarist and two horn players (trumpet\u00a0and trombone). \u00a0Oh, I almost forgot to mention they\u00a0contrast most ska bands by featuring a kickass lead female singer.\n</p>\n\n<p>\n<strong>What compelled me to write about The Barrymores? </strong>\n</p>\n<p>\nThough The Barrymores are a\u00a0small town band, their technical abilities and prowess mimic more a band of a major record label. \u00a0Their rocking upbeat style is similar to other ska bands. \u00a0Each track creates a euphoric\u00a0sensation and\u00a0even the \"sad\"\u00a0songs feel uplifting. When The Barrymores are on my stereo I typically\u00a0end up chanting the lyrics while skanking around my office. \u00a0I feel they bring the ska punk rock genre into another level of excellence.\n</p>\n<p>\n<strong>And then I found the depressing news ...</strong>\n</p>\n<p>\n<strong> </strong>I started researching The Barrymores because I was\u00a0fascinated\u00a0with their music. \u00a0This research\u00a0inevitably\u00a0lead to some depressing news, The Barrymores are no longer together! <em>\"I Am Jack's Broken Heart\" -Fight Club.</em> Not often do I get excited about new music. \u00a0In moments I grew attached to the group only to get crushed. \u00a0I\u00a0decided\u00a0to investigate to uncover more.\n</p>\n<p>\nApparently The Barrymores went into\u00a0hiatus after one of the founding members passed away.\u00a0\u00a0\u00a0The Barrymore\u00a0<a href=\"http://www.myspace.com/thebarrymores\">myspace</a> page still seems active and continues to hosts a few of the bands\u00a0tracks.\n</p>\n<p>\nSorry about the bummer, but you should still give the music a listen.\n</p>\n<div>\n<embed style='display:inline;' quality='high' wmode='transparent' id='FlashDiv' FlashVars='songId=50337238&pid=-6532191072447473425' AllowScriptAccess='always' src='http://lads.myspace.com/Embeds/SongEmbed/SongEmbed.swf' width='400' height='77'/>\n</embed>\n</div>\n\n<div>\n<embed style=\"display:inline;\" type=\"application/x-shockwave-flash\" width=\"400\" height=\"77\" src=\"http://lads.myspace.com/Embeds/SongEmbed/SongEmbed.swf\" quality=\"high\" wmode=\"transparent\" flashvars=\"songId=26356256&amp;pid=-4931290963471645042\">\n</embed>\n</div>",
    "date": "2010-11-30 04:37:44",
    "timestamp": 1291109864,
    "comments": [
      {
        "id": 4,
        "parent_id": 0,
        "author_ip": "204.209.209.129",
        "author": "Danny Ransom",
        "email": "dannyransom@gmail.com",
        "content": "Wow...I am (was...) the lead guitar player of the Barrymores.  It's cool to see people still discovering us, we've all went our seperate ways, 3 of the founding members are still in an active band called Subcity, and I've meandered through several punk bands you can look into with a quick google search (the Crackdown, the Blackout Brigade, High Class Low Lifes).  Thanks so much for the love man, nice not to be forgotten after all these years :)",
        "date": "2010-12-27 22:42:49",
        "timestamp": 1293507769
      },
      {
        "id": 6,
        "parent_id": 4,
        "author_ip": "192.168.1.23",
        "author": "Russell Ballestrini",
        "email": "russell.ballestrini@gmail.com",
        "content": "Thanks for the drop by and the comment,  I'm looking forward to hearing your new material.  Keep up the good work, and keep in touch.",
        "date": "2010-12-31 20:46:52",
        "timestamp": 1293846412
      },
      {
        "id": 25,
        "parent_id": 0,
        "author_ip": "69.253.89.177",
        "author": "jason",
        "email": "jaymolloy23@yahoo.com",
        "content": "Indeed.  If you didn't know, you should check out their sorta unknown/hidden album called \"New Invasion.\"  More punky than ska-like but still super duper. \n\nHey, if anyone from the band is reading this, you are awesome.  Get New Invasion up on iTunes! (I'm not even an Apple enthusiast, but that's where I buy my music)",
        "date": "2011-01-14 23:58:12",
        "timestamp": 1295067492
      },
      {
        "id": 2236,
        "parent_id": 0,
        "author_ip": "38.105.132.130",
        "author": "Matthew",
        "email": "mjkess@gmail.com",
        "content": "The Barrymores came up on Pandora while I was listening to the Big D &amp; The Kids Table channel, and since I hadn't heard of them I naturally consulted Google which lead me here. I can relate to your pain of finding out about a band ex post facto. My first experience of this came with hearing Edna's Goldfish for the first time in 2006 when they had already been broken up for more than 5 years. Then, they played the Bamboozle in 2009 and, of course, I couldn't make it because of final exams so now I'm left waiting for them to return once again. \n\nI wanted to direct your attention to another relatively unknown ska band called Upstanding Youth, from Hawaii. I first heard their music just 2 months ago and I was mesmerized. I was surprised they have been around for 10 years, but they have never been on a label and only toured the Southwest, out of Hawaii. Unfortunately, they announced a few weeks ago they will be ending their run with a farewell show on May 25, 2012 since the singer is moving to the East Coast for a new job.\n\n--Another ska fan with a crushed heart",
        "date": "2012-05-16 10:07:36",
        "timestamp": 1337177256
      },
      {
        "id": 2438,
        "parent_id": 0,
        "author_ip": "24.142.153.25",
        "author": "Tyler",
        "email": "munn.20@gmail.com",
        "content": "I came across \"The Sweetest Song\" on Pandora in much the same manner. I've been a punk/ska guy my whole life, but that song is the best, most chill love song I've ever heard. When it pops up on my playlist, I can't just listen to it once. And I agree, \"Think Straight Again\" kicks a whole lot of ass.",
        "date": "2012-06-02 19:00:22",
        "timestamp": 1338678022
      },
      {
        "id": 15821,
        "parent_id": 0,
        "author_ip": "127.0.0.1",
        "author": "Jeff",
        "email": "jdruedger@yahoo.com",
        "content": "I cut my teeth on the album \"New Invasion\".  I love the take no shit attitude of Jolene.  She has such a unique voice.  I too was devastated when I found out that I had fallen for a ska band that no longer existed.  I hope if the band is listening, they can make New Invasion more available.  A reunion would be awesome.",
        "date": "2013-02-07 00:19:58",
        "timestamp": 1360214398
      },
      {
        "id": 15791,
        "parent_id": 0,
        "author_ip": "127.0.0.1",
        "author": "ET",
        "email": "Chefet127@mail.com",
        "content": "Always love SKA-punk they just popped on my pandora. Glad this link was up to find out more info on them. Let there be a SKA revival!",
        "date": "2013-01-09 17:04:09",
        "timestamp": 1357769049
      },
      {
        "id": 19022,
        "parent_id": 0,
        "author_ip": "127.0.0.1",
        "author": "Cheryl",
        "email": "tonic4@me.com",
        "content": "I, too, feel the pain.  However in the past few years many ska bands have been reforming and touring. I think kickstarter has helped connect bands with fans and see they're still loved.  I run the Austin Ska Collective and have met many ska fans, perhaps a revival is in the works. Dallas had a pretty solid scene.",
        "date": "2014-01-13 15:53:28",
        "timestamp": 1389646408
      },
      {
        "id": 162859,
        "parent_id": 0,
        "author_ip": "127.0.0.1",
        "author": "El Fabio",
        "email": "fabiogualtieri@web.de",
        "content": "And now i am the one who listen barrymores for the first time,google for information about this Band and see they do not exist anymore....The voice of this female singer is so awesome and sounds like the Lady from 'Scrapy',another ska band, from germany, that do not exist anymore too.",
        "date": "2015-08-13 22:24:31",
        "timestamp": 1439519071
      },
      {
        "id": 178848,
        "parent_id": 4,
        "author_ip": "127.0.0.1",
        "author": "Sarah McAdam",
        "email": "PandaBeeCatastrophe@gmail.com",
        "content": "Hey! A decade or more ago I booked yoy guys in Fredericton Nb. Joelle (I think... Name?) cleaned my house and made me breakfast. My band is coming through in August and I'd like to return the favor! Let's connect!!!  PandaBeeCatastrophe@gmail.com",
        "date": "2016-02-06 17:15:40",
        "timestamp": 1454796940
      }
    ],
    "metadata": {
      "_edit_last": "1",
      "_wp_old_slug": "hello-world",
      "_stcr@_tonic4@me.com": "2014-01-13 15:53:28|Y",
      "_stcr@_PandaBeeCatastrophe@gmail.com": "2016-02-06 17:15:40|Y",
      "_stcr@_fabiogualtieri@web.de": "2015-08-13 22:24:31|Y"
    }
  },
  "four2go-a-new-spin-on-an-old-classic": {
    "name": "four2go-a-new-spin-on-an-old-classic",
    "id": "35",
    "link": "http://russell2.ballestrini.net/four2go-a-new-spin-on-an-old-classic/",
    "title": "four2go!: A new spin on an old classic",
    "content": "<a href=\"http://four2go.gumyum.com\"><img class=\"alignright\" style=\"border: 0px initial initial;\" title=\"four2go!\" src=\"http://russell.ballestrini.net/wp-content/uploads/2010/12/four2go.png\" alt=\"four2go! logo\"  /></a>\n<p>\n<strong>For the past two months</strong> I have feverishly worked on my side project.  Initially I set out to work on this application for submission to the Hacker News, lets make November \"Launch an App Month\".  \n</p>\n<p>\nThe whole project took longer than expected but I was please with my progress so I continued development.  Today, a month later,  I have added the finishing touches to the app.  It gives me great pleasure to announce the official launch of <a href=\"http://four2go.gumyum.com\">four2go!</a>\n</p>\n<p>\n<strong>A new spin ...</strong>\n</p>\n<p>\n<a href=\"http://four2go.gumyum.com\">four2go!</a> brings a new spin to the classic \"four in a row\" genre.  \n</p>\n<p>\nInstead of weighted tokens, <a href=\"http://four2go.gumyum.com\">four2go!</a> uses balloons which float up rather than down.  This new game mechanic requires players to retrain their brains and can easily throw off a novice player.\n</p>\n<p>\nAnother new spin, rather than playing on a coffee table, users play using a computer over the internet.  The four2go! web application requires a simple account registration but does not  require any download or installation!\n</p>\n<p>\nUsers can play with anyone, at anytime, anywhere in the world.  The game sessions are persistent meaning the board will not disappear.  This persistence allows players to check their games much like they would check their email.\n</p>\n<p>\n[caption id=\"\" align=\"alignleft\" width=\"230\" caption=\"Logo designed by Joey Ballestrini\"]<a href=\"http://gumyum.com\"><img class=\"  \" title=\"gumyum games logo designed by Joey Ballestrini\" src=\"http://russell.ballestrini.net/wp-content/uploads/2010/12/gumyumgameslogo.png\" alt=\"gumyum logo\" width=\"230\" align=\"right\" /></a>[/caption]\n</p>\n<p>\n<strong>The gumyum framework</strong>\n</p>\n<p>\nThe game itself was completed after the first weekend of coding.  Programming the game was fun and exciting and originally four2go! was a command prompt application with a text-based gui.  That version was not accessible and in order for people to play we needed to move to a different medium.  So I set out to code a web front end.\n</p>\n<p>\nAfter another weekend of coding I had the game working in the browser.  It was very clunky (It didn't refresh after player moves) and didn't have any authentication, anyone could play any game.\n</p>\n\n<blockquote><strong>Coding the game was simple, the framework was the difficult part...</strong></blockquote>\n\n<p>\n<em>Enter Stage Left:</em> The gumyum framework.  I needed a way to authentication users and a way to store and retrieve game sessions.  I also needed a way to track user statistics and player history.  Most importantly I needed a user interface that would be intuitive and fun to use!  The remainder of the project set out to solve each of those needs.  I needed the gumyum framework to make four2go! popular.  I spent the rest of the time (1.5 months) working on gumyum and I feel safe claiming that the framework appears complete.\n</p>\n\n<p>\nThe great part about having a framework is the code reusability, I should be able to \"plug\" new games or new mechanics behind the gumyum framework with little trouble.  My labor and efforts should pay off if I ever decide to build another game.\n</p>\n\n<p>\n<strong>Artwork and graphic design</strong>\n</p>\n<p>\nI was very fortunate to work with my brother <a href=\"http://joey.ballestrini.net\">Joey Ballestrini</a> on some of the concept and game art.  We designed the <a href=\"http://four2go.gumyum.com\">four2go!</a> logo, the <a href=\"http://gumyum.com\">gumyum</a> logo, and  the balloons.  Joey designed each of the \"create\" game icons.  The clouds, the grass, and the dirt were created for another game and I decided they were a good fit so they were re-purposed.  We both use gimp when creating graphical computer art.\n</p>\n\n<p>\n<strong>LETS PLAY!</strong>\n</p>\n\n<p>\nI have attached some screenshots and a video of the application, but I urge you to try <a href=\"http://four2go.gumyum.com\">four2go!</a> out for yourself!\n</p>\n\n<p>\n<center><iframe title=\"YouTube video player\" width=\"480\" height=\"338\" src=\"http://www.youtube.com/embed/wmB9PeKBAlA\" frameborder=\"0\"></iframe></center>\n</p>\n\n[gallery link=\"file\"]\n\n<strong>You should challenge me to a game of <a href=\"http://four2go.gumyum.com\">four2go! here</a></strong>",
    "date": "2010-12-31 20:33:06",
    "timestamp": 1293845586,
    "comments": [],
    "metadata": {
      "_edit_last": "1",
      "_wp_old_slug": null
    }
  },
  "null": {
    "name": null,
    "id": "4409",
    "link": "http://russell2.ballestrini.net/?p=4409",
    "title": "Why USA has shortage of Computer Scientists",
    "content": "The culture and environment of the United States of America is slowly destroying the hacker and tinkerer culture.  I feel punished for knowing how the Internet works, or rather how it was promised to work.\n\n<p>\n<strong>No competition for Internet service providers</strong>\n</p>\n\n<p>\nThis means they charge extortionate amounts of money for terrible service.  They keep track of usage so they may charge overages. They charge money to \"unlock\" key parts of the Internet, for example most ISPs tactically block port 80 and 443 to prevent serving websites from a residential connection and then attempt to up sell those who know how the Internet works into paying much more for much less just to have a real Internet connection.  They severely limit upload speeds.  Hold subscribers hostage because they are the only show in town, this happens because each ISP claims turf like gangsters on the street.\n</p>\n\n<p>What is the outcome of this?  Less and less people understand how the Internet really works because there are less and less opportunities to learn about it.</p>\n\n<p>My utopia solution would be for municipalities of cities and towns to offer up free or cheap Internet to tax payers.  This would force the ISP to change (for the better) or die.  Additionally I would free up some wireless spectrum for use in a new kind of network that gets more powerful as devices join!</p>\n\n<p>\n<strong>Operating systems on rails</strong>\n</p>\n\n<p>Phones and popular computer operating systems have become very dumb down and limiting.</p>\n\n<strong>Extreme and excessive punishment for computer \"attacks\"</strong>\n\nAaron \n\nThe outcome people avoid doing creative or tinkering behavior to avoid getting in trouble.\n\n\n<strong>Paranoia and locking down of computer networks</strong>\n\nprevents learning how it works",
    "date": "2015-05-20 10:03:42",
    "timestamp": 1432130622,
    "comments": [],
    "metadata": {
      "_edit_last": "1",
      "_spost_short_title": null
    }
  },
  "a-homegrown-python-bread-crumb-module": {
    "name": "a-homegrown-python-bread-crumb-module",
    "id": "98",
    "link": "http://russell2.ballestrini.net/a-homegrown-python-bread-crumb-module/",
    "title": "A homegrown python bread crumb module",
    "content": "<p><strong>I wrote <a href=\"https://bitbucket.org/russellballestrini/bread/raw/tip/bread.py\">bread.py</a> a few days ago.</strong> <a href=\"https://bitbucket.org/russellballestrini/bread/raw/tip/bread.py\">Bread.py</a> is a simple to use python breadcrumb module. \n</p>\n\n<p>\nThe bread object accepts a url string and grants access to the url crumbs (parts) or url links (list of hrefs to each crumb) .\n</p>\n\n<p>\nI have released <a href=\"https://bitbucket.org/russellballestrini/bread/raw/tip/bread.py\">bread.py</a> into the public domain and you may view the full source code here: <a href=\"https://bitbucket.org/russellballestrini/bread/src\">https://bitbucket.org/russellballestrini/bread/src</a>\n</p>\n\n<p>\n<strong>Update</strong>\n</p>\n\n<p>\nI recently revisited this module and wrote a tutorial on how to <a href=\"http://russell.ballestrini.net/add-a-breadcrumb-subscriber-to-a-pyramid-project-using-4-simple-steps/\">Add a Breadcrumb Subscriber to a Pyramid project using 4 simple steps</a>.\n</p>\n\n<ul>\n<li>Demo of bread.py: <a href=\"http://school.yohdah.com/\">http://school.yohdah.com/</a></li>\n<li>Pyrawiki will use bread.py</li> \n</ul>\n\n<br />\n\n<strong>You should follow me on twitter <a href=\"http://twitter.com/russellbal\" target=\"_blank\">here</a></strong>\n\n<span style=\"font-size: 10px;\">\n<script src=\"https://bitbucket.org/russellballestrini/bread/src/50a1a20fc3f3/bread.py?embed=t\"></script>\n</span>",
    "date": "2011-01-02 14:14:46",
    "timestamp": 1293995686,
    "comments": [
      {
        "id": 75,
        "parent_id": 0,
        "author_ip": "93.167.172.205",
        "author": "Kristian",
        "email": "kristian.nissen@gmail.com",
        "content": "Hi, this was just what I needed, did a few modifications but basically worked out of the box. Thanks for posting",
        "date": "2011-04-03 10:33:07",
        "timestamp": 1301841187
      },
      {
        "id": 76,
        "parent_id": 75,
        "author_ip": "192.168.1.23",
        "author": "Russell Ballestrini",
        "email": "russell.ballestrini@gmail.com",
        "content": "I'm interested in the modifications, I just placed the code into bitbucket.  Feel free to branch it.  \n\nI'm also interested in seeing your project that you used it in.  Thanks",
        "date": "2011-04-03 14:19:46",
        "timestamp": 1301854786
      }
    ],
    "metadata": {
      "_edit_last": "1",
      "_wp_old_slug": null
    }
  },
  "how-did-stack-overflow-get-initial-traction": {
    "name": "how-did-stack-overflow-get-initial-traction",
    "id": "118",
    "link": "http://russell2.ballestrini.net/how-did-stack-overflow-get-initial-traction/",
    "title": "How did Stack Overflow get initial traction?",
    "content": "<p>\n<strong>Stack Overflow was a progressive and natural evolution of the standard clunky forum.</strong>\n</p>\n\n<p>\nUsing ajax it created a more fun and clean user experience.  \n</p>\n\n<p>\nUsing badges and karma to gain responsibility allow forums post to become a game.  People naturally like to see progression and growth, being able to watch my karma go up was a rush, similar to a video game.  (some people plan elaborate flights to rack up frequent flyer miles, planning a trip properly could cost as little as 50 dollars to fly around the world in 24 hours, and gaining thousands of miles using point multipliers)\n</p>\n\n<p>\n<a href=\"http://russell.ballestrini.net/wp-content/uploads/2011/01/stack1.png\"><img src=\"http://russell.ballestrini.net/wp-content/uploads/2011/01/stack1.png\" alt=\"\" title=\"stack\" width=\"169\" height=\"54\" class=\"alignright size-full wp-image-127\" /></a>\n</p>\n\n<p>\nStack Overflow was search-able!  Here is a thought, create a forum that is easily index by search engines... One question per page, on topic with the most relevant posts at the top of the page.  Who cares about user bounce backs, if they see your URL enough (StackOverflow.com) they will end up creating an account and becoming a user who creates even more quality content!\n</p>\n\n<p>\nThe site was geared toward geeks, and programmers!  They adopt new tech the quickest.\n</p>\n\n<p>\nDid I miss any reasons?  Feel free to comment below.\n</p>\n\n<p>\n<em>Update - Check out my community at <a href=\"http://four2go.gumyum.com\">four2go.gumyum.com</a></em>\n</p>\n\n<p>\nYou should follow me on twitter <a href=\"http://twitter.com/russellbal\">here</a>\n</p>",
    "date": "2011-01-05 02:57:31",
    "timestamp": 1294214251,
    "comments": [
      {
        "id": 20,
        "parent_id": 0,
        "author_ip": "68.252.62.180",
        "author": "Mad Rapper X",
        "email": "RealMadRapperX@aol.com",
        "content": "yea you forgot the most important part - the two founders both had very popular blogs/followings.\n\nseriously?",
        "date": "2011-01-12 15:13:00",
        "timestamp": 1294863180
      },
      {
        "id": 21,
        "parent_id": 0,
        "author_ip": "74.113.160.14",
        "author": "Sanjay",
        "email": "sanjay.uttam@gmail.com",
        "content": "It also doesn't hurt that it was started by two individuals that were already huge in the DEV (esp. MS DEV) community...",
        "date": "2011-01-12 15:22:41",
        "timestamp": 1294863761
      },
      {
        "id": 22,
        "parent_id": 0,
        "author_ip": "66.162.141.154",
        "author": "squidbot",
        "email": "funtech@gmail.com",
        "content": "I think one of the most important things was that Jeff Atwood and Joel Spolsky talked about it on their blogs, and those blogs happened to be two of the most popular programming oriented blogs. This got a lot of traffic and buy in from the programming community as they obviously felt the two had credibility and reasonable ideas.",
        "date": "2011-01-12 17:46:16",
        "timestamp": 1294872376
      }
    ],
    "metadata": {
      "_edit_last": "1",
      "_wp_old_slug": null
    }
  },
  "graphic-design-powerhouse-wacom-bamboo-ubuntu-and-mypaint": {
    "name": "graphic-design-powerhouse-wacom-bamboo-ubuntu-and-mypaint",
    "id": "134",
    "link": "http://russell2.ballestrini.net/graphic-design-powerhouse-wacom-bamboo-ubuntu-and-mypaint/",
    "title": "Graphic Design Powerhouse: Wacom Bamboo, Ubuntu, and MyPaint",
    "content": "<a href=\"http://russell.ballestrini.net/wp-content/uploads/2011/01/pot.png\" target=\"_blank\"><img class=\"alignright\" src=\"http://russell.ballestrini.net/wp-content/uploads/2011/01/pot.png\" alt=\"pot\" width=\"217\" height=\"206\" /></a>\n\n<p>\n<strong>For Christmas this year my wife gifted me a <a href=\"http://www.amazon.com/gp/redirect.html?ie=UTF8&location=http%3A%2F%2Fwww.amazon.com%2Fs%3Fie%3DUTF8%26x%3D0%26ref_%3Dnb_sb_noss%26y%3D0%26field-keywords%3Dwacom%2520bamboo%26url%3Dsearch-alias%253Daps%23&tag=russellballes-20&linkCode=ur2&camp=1789&creative=390957\">Wacom Bamboo</a><img src=\"https://www.assoc-amazon.com/e/ir?t=russellballes-20&l=ur2&o=1\" width=\"1\" height=\"1\" border=\"0\" alt=\"\" style=\"border:none !important; margin:0px !important;\" /> pen tablet. </strong> She claimed to have researched many products and looked for a model that worked well with Ubuntu and had positive user reviews.  She ultimately chose the Wacom Bamboo model and I was so excited when I unwrapped it!\n</p>\n<p>\nFirst thing I did after opening the box was researched how to install under Linux.  I stumbled upon a great\u00a0article explaining the process <a href=\"http://sourceforge.net/apps/mediawiki/linuxwacom/index.php?title=Main_Page\" target=\"_blank\">here</a>.\n</p>\n\n<p>\n<a href=\"http://russell.ballestrini.net/wp-content/uploads/2011/01/treeman1.png\" target=\"_blank\"><img class=\"wp-image-142 alignleft\" title=\"treeman\" src=\"http://russell.ballestrini.net/wp-content/uploads/2011/01/treeman1.png\" alt=\"\" width=\"338\" height=\"486\" /></a>\n</p>\n<p>\nThe driver install was flawless and I was up and running after about 5 mins and the first application I attempted to use was Gimp.  Gimp worked but the flow felt \"clunky\".  I wanted an experience that better emulated a pencil or brush and canvas.  I went to bed that night feeling a little unsatisfied.\n</p>\n\n<p>\nThe next day I decided to give the tablet another try, but instead of using Gimp I decided to try my luck with other software called <a href=\"http://mypaint.intilinux.com/\">MyPaint</a>.  MyPaint's interface feels perfect and it is loaded with brushes, pencils, and inks.  MyPaint implements brush stroke pressure with unmatched precision!\n</p>\n\n\n\n<p>\nThe art on this page was created with my wacom BAMBOO tablet and MyPaint.\n</p>\n\n<p>\nYou should follow me on twitter <a href=\"http://twitter.com/russellbal\">here</a>.\n</p>\n\n<br>\n\n<p>\nThank you so much MyPaint developers for such a stellar piece of software!\n</p>\n\n<p>\n<blockquote><strong>MyPaint is a fast and easy open-source graphics application for digital painters. It lets you focus on the art instead of the program. You work on your canvas with minimum distractions, bringing up the interface only when you need it.</strong></blockquote>\n</p>\n\n[gallery link=\"file\" columns=\"2\"]",
    "date": "2011-01-09 00:14:01",
    "timestamp": 1294550041,
    "comments": [
      {
        "id": 43,
        "parent_id": 0,
        "author_ip": "64.120.31.133",
        "author": "Eiad Saeed A. Dahnim",
        "email": "EiadSaeedA.Dahnim@pen-tablet.net",
        "content": "If you were a user of Intous and think of buying Bamboo Pen then this review is for you. I had Intous3 before buying Wacom Cintiq 12 and Cintiq 21. I decided to buy Bamboo mainly for portability and the cost factor. The following are the major factors you have to think about.\n\nThe Pen:\nAlthough I searched Wacom to get a clear idea about the pen pressure I didn't get any answer. After buying Bamboo I didn't feel any difference in the pressure from any of the Wacom products I used. The tip of the pen squeaks when it contact the tablet like it does in Intous but it gives more like paper feeling. The pen doesn't slide easily on the tablet surface. It doesn't have an eraser like the others. The size of the pen is smaller than the one that comes with Intous and Cintiq. Personally I like everything about the pen, however, I wish it had an eraser. The buttons are customizable and have the same feeling of that in Grip Pen.\n\nThe Tablet:\nIt's more slim than the Intous and the surface is really nice. When I bought Bamboo I thought it has buttons but be aware it doesn't. The active surface has a natural paper feeling. The friction between the pen and the active surface prevents the pen from sliding fast which gives a natural feeling of real paper. The size can't be any perfect for my hand. However, I didn't like the fact that this tablet doesn't have any buttons. They, to me, are very essential in the tablet. Even when compared to the early Bamboo Tablets you will find plenty of buttons?? I really don't know why they didn't add them. \n \nAlthough Bamboo lack some important features, it remains very useful for artists. It's very portable and as simple as plug and play - only if the driver is installed-.",
        "date": "2011-02-08 04:57:11",
        "timestamp": 1297159031
      }
    ],
    "metadata": {
      "_edit_last": "1",
      "_wp_old_slug": null
    }
  },
  "i-just-used-google-to-purchase-chinese-food-takeout-for-two": {
    "name": "i-just-used-google-to-purchase-chinese-food-takeout-for-two",
    "id": "158",
    "link": "http://russell2.ballestrini.net/i-just-used-google-to-purchase-chinese-food-takeout-for-two/",
    "title": "I just used Google to purchase Chinese food takeout for two",
    "content": "<img class=\"aligncenter size-full wp-image-160\" title=\"plate of pork stir fry with vegetables\" src=\"http://russell.ballestrini.net/wp-content/uploads/2011/01/food1.jpg\" alt=\"\"/>\n\n\n<p>\nYes, it has been done.  Using my Google Chromium browser I searched Google Maps for the closest local Chinese restaurant.  The small town venue doesn't have a website or menu posted online.\n</p>\n<p>\nSo I used Google Web to search for the restaurant name.  I found http://www.takeouttonight.com which had the entire menu and VALID prices for the restaurant hosted online!  I choose 'Sweet and Sour chicken', my wife selected <del datetime=\"2011-11-27T19:03:38+00:00\">Beef and Broccoli</del> Chicken and Broccoli.\n</p>\n<p>\nI then logged into my Google Gmail account and clicked Call phone.  Using the mouse I dialed the number I previously researched and after 3 rings an Asian speaking fellow answered the phone.\n</p>\n<p>\nI said, \"Hello, I'd like to place a delivery order.\"  His reply, \"Ok.\" resonated out of my 5.1 speaker setup.\n</p>\n<p>\nI read off our selections and was super excited that it was WORKING!  In my titillated state, I ended up ordering an extra side of egg rolls just to keep the conversation going.\n</p>\n<p>\nHe asked for my phone number to complete the order.  I obliged by reading my Google phone number.\n</p>\n<p>\nGoogle might be spamy these days, but it's still useful.\n</p>\n<p>\nWho would have thought a lazy hacker could combine six different Google services to order Chinese food without leaving the desk?\n</p>\n<p>\nI know it's possible now, I've seen me do it.\n</p>\n<p>\nYou should follow me on twitter <a href=\"http://twitter.com/russellbal\">here</a>.\n</p>",
    "date": "2011-01-10 20:03:33",
    "timestamp": 1294707813,
    "comments": [
      {
        "id": 14,
        "parent_id": 0,
        "author_ip": "192.168.1.23",
        "author": "Ray",
        "email": "ray@rginzametrics.com",
        "content": "This made me realize that Google's click to call functionality that I've seen in their search results from time to time should be, if it's not, just be routed through your Google Voice account.\nThey could even allow vendors to run real-time Groupon-like promotions to certain targeted users as part of their AdWords campaigns.",
        "date": "2011-01-10 22:56:34",
        "timestamp": 1294718194
      },
      {
        "id": 15,
        "parent_id": 0,
        "author_ip": "192.168.1.23",
        "author": "Russell Ballestrini",
        "email": "russell.ballestrini@gmail.com",
        "content": "I'm glad you liked it. Yes that would be a crazy idea. Or how about Adword ads with \"realtime\" phone responses. Click the advert and you call the company using google voice.",
        "date": "2011-01-10 22:58:52",
        "timestamp": 1294718332
      }
    ],
    "metadata": {
      "_edit_last": "1",
      "_wp_old_slug": null
    }
  },
  "response-to-l-theanine-a-4000-year-old-mind-hack": {
    "name": "response-to-l-theanine-a-4000-year-old-mind-hack",
    "id": "179",
    "link": "http://russell2.ballestrini.net/response-to-l-theanine-a-4000-year-old-mind-hack/",
    "title": "Response to L-Theanine: a 4000 Year Old Mind-Hack",
    "content": "<p>\n<strong>RE: <a href=\"http://worldoftea.org/caffeine-and-l-theanine\" target=\"_blank\">http://worldoftea.org/caffeine-and-l-theanine</a></strong>\n</p>\n<p>\n<a href=\"http://russell.ballestrini.net/wp-content/uploads/2011/01/dream.jpg\"><img class=\"alignright size-full wp-image-182\" title=\"dream\" src=\"http://russell.ballestrini.net/wp-content/uploads/2011/01/dream.jpg\" alt=\"\" width=\"358\" height=\"175\" /></a>\n</p>\n<p>\nAs the original article speculates, the combination and amount of  L-Theanine and caffeine present in tea, appears to have notable affects on my programming and problem solving abilities.  It's difficult to show conclusive evidence to this claim, but I generally feel most alert and organized when coding under the influence of 2 - 3 cups.  In a typical day I drink about 4 cups, dosed an hour apart.  I feel compelled to introduce another \"mind hack\" for problem solving or programming,  which I call \"Dream coding\"\n</p>\n<p>\n\"Dream coding\" is just that, coding at night during sleep.  One can successfully invoke this \"mind hack\" by provoking thought about the problem while drifting to sleep.  Once sleeping I'm able to see my code and my brain seems to iteratively problem solve.  Most often I'm lucid during these events; aware of the problem I'm trying to solve and of the possible solutions.  Other times I don't recall performing the solutions, but when I wake and after my first cup of tea, I'm able to solve my problem with a optimal and beautiful solution.\n</p>\n<p>\nPBS NOVA published an episode about this phenomenon: <a href=\"http://www.pbs.org/wgbh/nova/dreams/\" target=\"_blank\">What are dreams? </a>\n</p>\n<p>\nDoes this happen to you?  Have you experienced this?\n</p>\n<p>\nYou should follow me on twitter <a href=\"http://twitter.com/russellbal\" target=\"_blank\">here</a>.\n</p>",
    "date": "2011-01-11 17:06:45",
    "timestamp": 1294783605,
    "comments": [
      {
        "id": 18,
        "parent_id": 0,
        "author_ip": "137.159.154.69",
        "author": "Reg-o-rama",
        "email": "reglevy@gmail.com",
        "content": "I've never coded\u2026either in my sleep or while awake (I don't count html as coding), but I do have this effect when sewing. When I have a particularly vexing\u2014or interesting\u2014problem, I have often found that I will solve the problem while I'm asleep\u2014<i>and</i> that I remember the solution when I wake. I don't know that tea has anything to do with it, but as long as I'm thinking of the problem when I fall asleep, I find that I know the answer when I awake.\n\nI should figure out how to refine it, though, it's a great mind-hack!!",
        "date": "2011-01-11 19:18:16",
        "timestamp": 1294791496
      },
      {
        "id": 19,
        "parent_id": 0,
        "author_ip": "76.231.162.221",
        "author": "Russell Ballestrini",
        "email": "russell.ballestrini@gmail.com",
        "content": "Yes Reg-o-rama! That exactly it, I'm grateful that you mentioned that.  As I fall asleep I always think about the problem, in my case code.  Then I miraculously have the answer in the morning.  I think provoking thought about the problem while falling asleep seems to be the trigger for this \"mind hack\".\n\nThanks for enlightening me with the missing ingredient to the hack!",
        "date": "2011-01-11 19:34:00",
        "timestamp": 1294792440
      }
    ],
    "metadata": {
      "_edit_last": "1",
      "_wp_old_slug": null
    }
  },
  "virt-back-a-python-libvirt-backup-utility-for-kvm-xen-virtualbox": {
    "name": "virt-back-a-python-libvirt-backup-utility-for-kvm-xen-virtualbox",
    "id": "231",
    "link": "http://russell2.ballestrini.net/virt-back-a-python-libvirt-backup-utility-for-kvm-xen-virtualbox/",
    "title": "virt-back: a python libvirt backup utility for kvm xen virtualbox",
    "content": "<center><img src=\"/wp-content/uploads/2011/03/virt-back.png\" alt=\"\" title=\"virt-back\" width=\"400\" /></center>\n\n<p>\n<strong>Over the weekend I wrote virt-back, a backup utility for QEMU, KVM, XEN, or Virtualbox guests.</strong>\n</p>\n<p>\nvirt-back is a python application that uses the libvirt API to safely shutdown, gzip, and restart guests.\n</p>\n<p>\nThe backup process logs to syslog for auditing and virt-back works great with cron for scheduling outages.  Virt-back is in active development so feel free to give suggestions or branch the source.\n</p>\n<p>\nvirt-back has been placed in the public domain and the latest version may be downloaded here: <a href=\"https://bitbucket.org/russellballestrini/virt-back\" target=\"_blank\"><strong>https://bitbucket.org/russellballestrini/virt-back</strong></a>\n</p>\n\n<strong><em>Installation: </em></strong>\n\n<p>The fastest way to install virt-back is to use pip or setuptools.</p>\n<p>Try <code>sudo pip install virt-back</code> or <code>sudo easy_install virt-back</code>\n</p>\n\nOtherwise you may manually install virt-back \n\n<pre lang=\"bash\">sudo wget https://bitbucket.org/russellballestrini/virt-back/raw/tip/virt-back -O  /usr/local/bin/virt-back\n</pre>\n\n<pre lang=\"bash\">sudo chmod 755 /usr/local/bin/virt-back\n</pre>\n\n<strong><em>Test installation: </em></strong>\n\n<pre lang=\"bash\">virt-back --help\n</pre>\n\n<strong><em>Example cronjob:</em></strong>\n<pre lang=\"bash\">15  2  *  *  1  /usr/local/bin/virt-back --quiet --backup sagat\n15  23 *  *  5  /usr/local/bin/virt-back --quiet --backup mbison</pre>\n\n<strong><em>Manual:</em></strong>\n\n<pre>russell@host:~$ virt-back --help\nUsage: virt-back [options]\nOptions:\n  -h, --help            show this help message and exit\n  -q, --quiet           prevent output to stdout\n  -d, --date            append date to tar filename [default: no date]\n  -g, --no-gzip         do not gzip or tar the resulting files\n  -a amount, --retention=amount\n                        backups to retain [default: 3]\n  -p 'PATH', --path='PATH'\n                        backup path [default: '/KVMBACK']\n  -u 'URI', --uri='URI'\n                        optional hypervisor uri\n\n  Actions for info testing:\n    These options display info or test a list of guests.\n\n    -i, --info          info/test a list of guests (space delimited dom names)\n    --info-all          attempt to show info on ALL guests\n\n  Actions for a list of dom names:\n    WARNING:  These options WILL bring down guests!\n\n    -b, --backup        backup a list of guests (space delimited dom names)\n    -r, --reboot        reboot a list of guests (space delimited dom names)\n    -s, --shutdown      shutdown a list of guests (space delimited dom names)\n    -c, --create        start a list of guests (space delimited dom names)\n\n  Actions for all doms:\n    WARNING:  These options WILL bring down ALL guests!\n\n    --backup-all        attempt to shutdown, backup, and start ALL guests\n    --reboot-all        attempt to shutdown and then start ALL guests\n    --shutdown-all      attempt to shutdown ALL guests\n    --create-all        attempt to start ALL guests\n\n</pre>",
    "date": "2011-03-20 11:32:30",
    "timestamp": 1300635150,
    "comments": [
      {
        "id": 113,
        "parent_id": 0,
        "author_ip": "193.49.86.8",
        "author": "Pascal",
        "email": "pascal.legrand@univ-orleans.fr",
        "content": "Hello,\ni'm trying your script to backup guest.\nI thought when the backup was done the script restarted the guest but it is not the case.\ncould you give me information?\nthanks for your script\nbye",
        "date": "2011-05-02 08:49:38",
        "timestamp": 1304340578
      },
      {
        "id": 114,
        "parent_id": 0,
        "author_ip": "192.168.1.23",
        "author": "Russell Ballestrini",
        "email": "russell.ballestrini@gmail.com",
        "content": "Hey Pascal,\n\nThe script has intelligence and keeps track of the guests state before backup.  \n\nIf the guest was running when the backup was initiated then the guest will be started when the procedure completes.\n\nHowever, if the guest was shut off when the backup was initiated, then the guest will not be started when the backup is finished.\n\nAlso make sure that the guest is defined in libvirt:\n\nsyntax: \n    virsh define /path/to/dom/xml\n\nexample:\n    virsh define /etc/libvirt/qemu/mbison.xml\n\nLet me know if you have any other questions and I'll be glad to troubleshoot further.",
        "date": "2011-05-02 22:43:27",
        "timestamp": 1304390607
      },
      {
        "id": 115,
        "parent_id": 0,
        "author_ip": "193.49.86.8",
        "author": "Pascal",
        "email": "pascal.legrand@univ-orleans.fr",
        "content": "hey Russel,\nfirst thanks for your answer.\ni create guest with the \"virt-install\" command. does this command define guest in libvirt ?\nwhat is the way to see if a guest is defined? when i do a virsh list --all i can see all my guest.\nwhen i use your script on a running guest, the backup works fine, but the guest is not re started.\nthanks again for your script and your answer.",
        "date": "2011-05-03 02:05:21",
        "timestamp": 1304402721
      },
      {
        "id": 116,
        "parent_id": 115,
        "author_ip": "192.168.1.21",
        "author": "Russell Ballestrini",
        "email": "russell.ballestrini@gmail.com",
        "content": "Could you try a test for me? \n\nCould you shut off your guest and then try to start it using virt-back?\n\n# shutdown\nvirsh shutdown \n\n# start\nvirt-back --verbose --create \n\nIf this doesn't work there might be something wrong...",
        "date": "2011-05-03 19:59:20",
        "timestamp": 1304467160
      },
      {
        "id": 122,
        "parent_id": 0,
        "author_ip": "193.49.86.8",
        "author": "Pascal",
        "email": "pascal.legrand@univ-orleans.fr",
        "content": "Hey Russel,\nit was a mistake from me.\ni let the script in my home directory.\nafter i put it in /usr/local/bin, it works\nthanks for your answers and your script",
        "date": "2011-05-10 02:09:39",
        "timestamp": 1305007779
      },
      {
        "id": 124,
        "parent_id": 0,
        "author_ip": "84.146.37.187",
        "author": "Tobias",
        "email": "seppenha@fim.uni-passau.de",
        "content": "Hey Russel,\n\nsomething seems to be wrong with your script.\n\n-------------------------------------------\nvirt-back -p /root/backup/ -b vm132\nTraceback (most recent call last):\n  File \"/usr/local/bin/virt-back\", line 242, in \n    if options.backup or options.backupall: backup( domList )\n  File \"/usr/local/bin/virt-back\", line 87, in backup\n    tar.add( disklist[0] ) # we could loop and tar each disk in disklist\nIndexError: list index out of range\n-------------------------------------------\n\nCreating the VM with \"virt-back -v --create vm132\" works like a charme.  Also see \n\nroot@host ~ # virsh 'list'\"\n\n Id Name                 State\n----------------------------------\n  5 vm132                running\n\nroot@host ~ # virt-back --info-all --verbose\n\nname: invoking name on vm132\nname: name returned vm132 on vm132\ninfo: invoking info on vm132\ninfo: info returned [1, 524288L, 524288L, 1, 11680000000L] on vm132",
        "date": "2011-05-17 10:13:19",
        "timestamp": 1305641599
      },
      {
        "id": 508,
        "parent_id": 462,
        "author_ip": "192.168.1.21",
        "author": "Russell Ballestrini",
        "email": "russell.ballestrini@gmail.com",
        "content": "I have written a procedure for restoring a system from a virt-back guest backup.\n\nhttp://russell.ballestrini.net/virt-back-restoring-from-backups/\n\nI hope this guide will help.",
        "date": "2011-08-10 23:25:22",
        "timestamp": 1313033122
      },
      {
        "id": 1098,
        "parent_id": 0,
        "author_ip": "81.20.195.83",
        "author": "Lenny",
        "email": "vermus.jabber@gmail.com",
        "content": "ok, i see: \"A snapshot volume only stores the delta (changes) since it was created. This is convenient for a point-in-time, well, snapshot, but if your original data is lost, you can't restore from a snapshot. \"\nWhat about first question?",
        "date": "2012-01-11 07:45:26",
        "timestamp": 1326285926
      },
      {
        "id": 1097,
        "parent_id": 0,
        "author_ip": "81.20.195.83",
        "author": "Lenny",
        "email": "vermus.jabber@gmail.com",
        "content": "Thanx! How to select for saving not all disks of VM?\nAnd what's the difference between an snapshot and ordinary backup?",
        "date": "2012-01-11 06:05:41",
        "timestamp": 1326279941
      },
      {
        "id": 1107,
        "parent_id": 1105,
        "author_ip": "192.168.1.21",
        "author": "Russell Ballestrini",
        "email": "russell.ballestrini@gmail.com",
        "content": "Yes, my intention was to backup all images.  I never got around to it...",
        "date": "2012-01-12 21:54:08",
        "timestamp": 1326423248
      },
      {
        "id": 1105,
        "parent_id": 0,
        "author_ip": "81.20.195.83",
        "author": "Lenny",
        "email": "vermus.jabber@gmail.com",
        "content": "ok, i see\n\norigfile = disklist[0]\n\nthat only first drive is being backup.",
        "date": "2012-01-12 04:04:11",
        "timestamp": 1326359051
      },
      {
        "id": 462,
        "parent_id": 0,
        "author_ip": "209.23.207.1",
        "author": "George",
        "email": "gmdune@gmail.com",
        "content": "Hi Russell,\n\nI was able to successfully backup my VM using your script, but I can't figure out how to restore it.  I just rebuild my system and need your help!\n\nThx",
        "date": "2011-08-04 14:06:29",
        "timestamp": 1312481189
      },
      {
        "id": 1728,
        "parent_id": 0,
        "author_ip": "70.89.119.49",
        "author": "James",
        "email": "james@simple-reliable.com",
        "content": "Excellent script.  Works like a charm.\n\nI will make very good use of it.\n\nGreat work!",
        "date": "2012-03-28 14:18:35",
        "timestamp": 1332958715
      },
      {
        "id": 1583,
        "parent_id": 0,
        "author_ip": "192.168.1.21",
        "author": "Russell Ballestrini",
        "email": "russell.ballestrini@gmail.com",
        "content": "virt-back can now backup all attached disk images.\n\nInstall the latest version today.\n\nhttps://bitbucket.org/russellballestrini/virt-back/changeset/b2bc219d6c6a",
        "date": "2012-03-17 00:20:10",
        "timestamp": 1331958010
      },
      {
        "id": 2209,
        "parent_id": 1107,
        "author_ip": "92.163.46.50",
        "author": "Guilhem Lettron",
        "email": "guilhem.lettron@youscribe.com",
        "content": "I do many work on your script before :\n- I already implement multi-disk support\n- support for qcow2 snapshot to minimise the downtime.\n- usage \"suspend\" instead of \"shutdown\"\n- add option to restore a (previously backup) VM\n- add ability to migrate a VM (supend, transfer and re-up on another server)\n\nhave a look here if you want this full feature version : https://bitbucket.org/guilhemfr/virt-back\n\n(I do pull-request for this...)",
        "date": "2012-05-14 14:59:35",
        "timestamp": 1337021975
      },
      {
        "id": 59456,
        "parent_id": 58412,
        "author_ip": "127.0.0.1",
        "author": "Jennifer Mehl",
        "email": "jennifer.mehl@ucsb.edu",
        "content": "Removing the --date option worked. Thanks!",
        "date": "2014-09-05 14:21:20",
        "timestamp": 1409941280
      },
      {
        "id": 9393,
        "parent_id": 0,
        "author_ip": "127.0.0.1",
        "author": "himuraken",
        "email": "himuraken@gmail.com",
        "content": "Excellent tool, thanks for making it! I have looked around but I dont seem to find anyway to increase the number of backups to retain. Any tips?",
        "date": "2012-10-03 11:49:52",
        "timestamp": 1349279392
      },
      {
        "id": 9395,
        "parent_id": 0,
        "author_ip": "127.0.0.1",
        "author": "himuraken",
        "email": "himuraken@gmail.com",
        "content": "In fact, I did a vim /usr/local/bin/virt-back and found:\nif path.isfile( tarpath ): # if file exists, rotate 3 versions\n\nCan I just increment rotate 3 to the number of revisions desired?\n\nThanks!",
        "date": "2012-10-03 11:54:40",
        "timestamp": 1349279680
      },
      {
        "id": 9427,
        "parent_id": 9393,
        "author_ip": "127.0.0.1",
        "author": "Russell Ballestrini",
        "email": "russell.ballestrini@gmail.com",
        "content": "himuraken,  Today is your lucky day.  I have implemented a retention flag for setting the amount of backups to keep when rotating.  Please use the latest version of virt-back.\n\n\n-a amount, --retention=amount\n backups to retain [default: 3]",
        "date": "2012-10-03 23:02:23",
        "timestamp": 1349319743
      },
      {
        "id": 10801,
        "parent_id": 0,
        "author_ip": "127.0.0.1",
        "author": "Dustin Lambert",
        "email": "dustin@dustinlambert.com",
        "content": "It would be great if the script would accept a shutdown time limit before it destroys the domain.  I always have to increase it in the source.\n\nThanks again!",
        "date": "2012-11-10 16:07:07",
        "timestamp": 1352581627
      },
      {
        "id": 58400,
        "parent_id": 0,
        "author_ip": "127.0.0.1",
        "author": "Jennifer Mehl",
        "email": "jennifer.mehl@ucsb.edu",
        "content": "Hello, \n\nThanks for this utility. It is really useful.\n\nI am using the script and have not had any success in limiting the number of retained backups.  This ends up filling up my backup disk (a NAS drive mounted via NFS), so I need to figure out how to resolve it.  Here is an example of the command I am using via cron job:\n\n<pre>\n30  1  *  *  *  /usr/local/bin/virt-back --quiet --backup --date --retention=2 -p /vmimages1/backups scre-ubuntu-log\n</pre>\n\nThe retention flag is being ignored.  (Currently there are 4 backups stored, and if I don't delete them manually, they will stay there.)  I am not seeing anything about the rotation in the logfile/syslog: \n\n<pre>\nSep  3 01:30:01 535-1201a-vm9 CRON[4486]: (root) CMD (/usr/local/bin/virt-back --quiet --backup --date --retention=2 -p /vmimages1/backups scre-ubuntu-log)\nSep  3 01:30:02 535-1201a-vm9 virt-back: invoking shutdown on scre-ubuntu-log\nSep  3 01:30:02 535-1201a-vm9 virt-back: waited 0 seconds for scre-ubuntu-log to shut off\nSep  3 01:30:12 535-1201a-vm9 virt-back: waited 10 seconds for scre-ubuntu-log to shut off\nSep  3 01:30:22 535-1201a-vm9 virt-back: waited 20 seconds for scre-ubuntu-log to shut off\nSep  3 01:30:32 535-1201a-vm9 virt-back: waited 30 seconds for scre-ubuntu-log to shut off\nSep  3 01:30:42 535-1201a-vm9 virt-back: waited 40 seconds for scre-ubuntu-log to shut off\nSep  3 01:30:52 535-1201a-vm9 virt-back: waited 50 seconds for scre-ubuntu-log to shut off\nSep  3 01:31:02 535-1201a-vm9 virt-back: invoking backup for scre-ubuntu-log\nSep  3 01:31:02 535-1201a-vm9 virt-back: copying /vmimages1/img/scre-ubuntu-log.vmdk to /vmimages1/backups/scre-ubuntu-log.vmdk for scre-ubuntu-log\nSep  3 01:32:46 535-1201a-vm9 virt-back: invoking create on scre-ubuntu-log\nSep  3 01:32:50 535-1201a-vm9 virt-back: archiving files for scre-ubuntu-log to /vmimages1/backups/scre-ubuntu-log-2014-09-03.tar.gz\nSep  3 01:32:50 535-1201a-vm9 virt-back: archiving /vmimages1/backups/scre-ubuntu-log.xml for scre-ubuntu-log\nSep  3 01:32:50 535-1201a-vm9 virt-back: archiving /vmimages1/backups/scre-ubuntu-log.vmdk for scre-ubuntu-log\nSep  3 01:52:58 535-1201a-vm9 virt-back: finished backup for scre-ubuntu-log\n</pre>\n\nI would appreciate your help in resolving this.  Thanks for this great utility.",
        "date": "2014-09-03 12:01:37",
        "timestamp": 1409760097
      },
      {
        "id": 58408,
        "parent_id": 58400,
        "author_ip": "127.0.0.1",
        "author": "Russell Ballestrini",
        "email": "russell.ballestrini@gmail.com",
        "content": "Hey Jennifer,\n\nJust a guess but I don't think the way I wrote the retention will work with the <code>--date</code> option.\nThis is because the filenames are always unique.  You must choose either to put dates into your filenames and find a different way to perform rotations, or use the <code>--retention</code> flag and let <code>virt-back</code> rotate the files.",
        "date": "2014-09-03 12:35:50",
        "timestamp": 1409762150
      },
      {
        "id": 58412,
        "parent_id": 58408,
        "author_ip": "127.0.0.1",
        "author": "Jennifer Mehl",
        "email": "jennifer.mehl@ucsb.edu",
        "content": "OK, thanks, I'll try removing the date option and see if retention works then.  Thanks for the quick reply.  Will let you know if that works.",
        "date": "2014-09-03 12:41:20",
        "timestamp": 1409762480
      },
      {
        "id": 8136,
        "parent_id": 0,
        "author_ip": "127.0.0.1",
        "author": "Dustin Lambert",
        "email": "d@lmbrt.com",
        "content": "This is awesome; These types of things should just be included out of the box.\n\nThanks for sharing!",
        "date": "2012-09-12 22:16:02",
        "timestamp": 1347502562
      },
      {
        "id": 166843,
        "parent_id": 161309,
        "author_ip": "127.0.0.1",
        "author": "Jennifer Mehl",
        "email": "jennifer.mehl@ucsb.edu",
        "content": "Just following up on this - not sure if you saw my initial comment regarding excluding certain disk images from backup.  Thanks!",
        "date": "2015-09-04 13:08:28",
        "timestamp": 1441386508
      },
      {
        "id": 168023,
        "parent_id": 166843,
        "author_ip": "127.0.0.1",
        "author": "Russell Ballestrini",
        "email": "russell.ballestrini@gmail.com",
        "content": "Hi Jennifer, I saw your comments, virt-back will always attempt to backup all disk images related to a particular guest.  Excluding disk / devices is currently not supported.",
        "date": "2015-09-11 22:50:47",
        "timestamp": 1442026247
      },
      {
        "id": 90486,
        "parent_id": 89406,
        "author_ip": "127.0.0.1",
        "author": "Russell Ballestrini",
        "email": "russell.ballestrini@gmail.com",
        "content": "You have an LVM partition on the hypervisor mounted on the guest?  Virt-back doesn't support that configuration.",
        "date": "2014-11-13 11:02:26",
        "timestamp": 1415894546
      },
      {
        "id": 89406,
        "parent_id": 0,
        "author_ip": "127.0.0.1",
        "author": "Marc",
        "email": "demhartner@cypres-it.com",
        "content": "Hello, getting this error:\n\nroot@cypdell1:/home/modias# ./virt-back --backup-all\nshutdown: invoking shutdown on oxygen\nshutdown: waited 0 seconds for oxygen to shut off\nshutdown: waited 10 seconds for oxygen to shut off\nbackup: Create XML file with configuration of oxygen\nbackup: invoking backup for oxygen\nbackup: copy of /var/lib/libvirt/images/oxygen.img to /home/modias/oxygen.img\nTraceback (most recent call last):\n  File \"./virt-back\", line 531, in \n    files = backup(dom)\n  File \"./virt-back\", line 134, in backup\n    copytmpFile = join(options.backpath, basename(tmpfile))\n  File \"/usr/lib/python2.7/posixpath.py\", line 121, in basename\n    i = p.rfind('/') + 1\nAttributeError: 'NoneType' object has no attribute 'rfind'\n\n\ni am running Ubuntu 14.04 LTS - the guest was running with a .img file under /lib/libvirt/images/os.img and a storage drive from a LVM",
        "date": "2014-11-11 12:20:29",
        "timestamp": 1415726429
      },
      {
        "id": 180086,
        "parent_id": 0,
        "author_ip": "127.0.0.1",
        "author": "aaron",
        "email": "aaaron@ukr.net",
        "content": "Hello!\nOne of the VMs does not start after the backup.\nHow to turn on debug logging level?\nThanks!",
        "date": "2016-03-18 04:03:37",
        "timestamp": 1458288217
      },
      {
        "id": 119035,
        "parent_id": 102100,
        "author_ip": "127.0.0.1",
        "author": "Russell Ballestrini",
        "email": "russell.ballestrini@gmail.com",
        "content": "If you don't want to zip the 500G file, use this flag.\n<pre>\n  -g, --no-gzip         do not gzip the resulting tar file\n</pre>",
        "date": "2015-01-21 12:37:02",
        "timestamp": 1421861822
      },
      {
        "id": 178656,
        "parent_id": 158787,
        "author_ip": "127.0.0.1",
        "author": "Russell Ballestrini",
        "email": "russell.ballestrini@gmail.com",
        "content": "https://bitbucket.org/russellballestrini/virt-back/src/2f3b7a422097966ec2246ea59ced28fe8cb3707d/virt-back?at=default&fileviewer=file-view-default#virt-back-153",
        "date": "2016-01-30 18:04:30",
        "timestamp": 1454195070
      },
      {
        "id": 176119,
        "parent_id": 168023,
        "author_ip": "127.0.0.1",
        "author": "Jennifer",
        "email": "jennifer.mehl@ucsb.edu",
        "content": "Thanks for the answer on that.  I would appreciate if the ability to exclude specific disk images would be supported in a future version. Thanks for this great utility.",
        "date": "2015-11-23 13:38:45",
        "timestamp": 1448303925
      },
      {
        "id": 174063,
        "parent_id": 0,
        "author_ip": "127.0.0.1",
        "author": "Sven",
        "email": "sleupold@checksum.ch",
        "content": "Thanks for sharing this script! Works like a charm!",
        "date": "2015-10-27 04:23:11",
        "timestamp": 1445934191
      },
      {
        "id": 102100,
        "parent_id": 0,
        "author_ip": "127.0.0.1",
        "author": "ich_wer_sonst",
        "email": "ich_wer_sonst1234@gmx.de",
        "content": "Virt-backup without tar.gz at the end?\n\n500Gb VM needs more like 5 hours to USB 3.0 disk",
        "date": "2014-12-11 15:30:29",
        "timestamp": 1418329829
      },
      {
        "id": 161309,
        "parent_id": 0,
        "author_ip": "127.0.0.1",
        "author": "Jennifer Mehl",
        "email": "jennifer.mehl@ucsb.edu",
        "content": "Hello - is it possible to exclude one (or more) of the disk images associated with a VM? For instance, I have a VM that has two disk images associated with it - one is the OS/Applications and the other is Data.  I only want to back up the Data image.\n\nIf this is not an option currently, please consider this a feature request.  Thank you!",
        "date": "2015-08-03 17:12:36",
        "timestamp": 1438636356
      },
      {
        "id": 158787,
        "parent_id": 0,
        "author_ip": "127.0.0.1",
        "author": "Bj\u00f6rn",
        "email": "bjoern@paulsen-familie.de",
        "content": "Hello Russell,\ni try your backup script, it seems to be very nice. But one thing is not very well. It use the normal gzip single threaded, that is very slow. I search your script, but python and I don't be friends ;) I don't find where you call gzip. Where can I change your script to use pigz?\nThank You",
        "date": "2015-07-14 12:01:24",
        "timestamp": 1436889684
      }
    ],
    "metadata": {
      "_stcr@_aaaron@ukr.net": "2016-03-18 04:03:37|Y",
      "_edit_last": "1",
      "_wp_old_slug": "virt-back-a-python-libvirt-backup-utility-for-kvm",
      "_series_part": "1",
      "_spost_short_title": null,
      "_stcr@_fcedmpenmpetuk1@gmail.com": "2014-07-28 05:03:21|Y",
      "_stcr@_jennifer.mehl@ucsb.edu": "2014-09-03 12:01:37|Y",
      "_stcr@_nrccxvookrgwi8l@gmail.com": "2014-10-29 16:01:05|Y",
      "_stcr@_insnshuim@hotmail.com": "2014-11-11 06:15:51|Y",
      "_stcr@_demhartner@cypres-it.com": "2014-11-11 12:20:29|Y",
      "_stcr@_ich_wer_sonst1234@gmx.de": "2014-12-11 15:30:29|Y",
      "_stcr@_rdpgholzm@hotmail.com": "2015-05-13 00:05:34|Y",
      "_stcr@_eroherohr@hotmail.com": "2015-05-13 00:05:34|Y",
      "_stcr@_hanzhanig@hotmail.com": "2015-05-13 00:05:34|Y",
      "_stcr@_iririqie@hotmail.com": "2015-07-16 05:12:12|Y",
      "_stcr@_erfzarlen@hotmail.com": "2015-07-16 05:11:11|Y",
      "_stcr@_bjoern@paulsen-familie.de": "2015-07-14 12:01:24|Y"
    }
  },
  "porting-the-chaostheory-wordpress-theme-to-pylowiki": {
    "name": "porting-the-chaostheory-wordpress-theme-to-pylowiki",
    "id": "309",
    "link": "http://russell2.ballestrini.net/porting-the-chaostheory-wordpress-theme-to-pylowiki/",
    "title": "Porting the ChaosTheory Wordpress theme to Pylowiki",
    "content": "<strong>As you may have noticed, I recently switched this blog over to the ChaosTheory Wordpress theme.</strong>\n<p>\nI had to tweak some of my pre-existing articles to make them look 'right' but upon completion I was very pleased with the aesthetics of each page. \n</p>\n<p>\nLater on I reviewed the theme of foxhop.net, my pylowiki demo site.  Pylowiki felt bland in comparison to this wordpress blog.\n</p>\n<p>\nSo, I made up my mind to build themes for Pylowik, and I started by porting ChaosTheory.  You can view the finished product at <a href=\" http://www.foxhop.net \" target=\"_blank\"> http://www.foxhop.net </a>.\n</p>\n<p>\nYou should also give Pylowiki a try.  Pylowiki has futuristic, live preview, same page, section edit functionality.  \n</p>\n\n<a href=\" http://www.foxhop.net\" target=\"_blank\"><img class=\"aligncenter size-full wp-image-310\" title=\"Screenshot\" src=\"/wp-content/uploads/2011/03/Screenshot.png\" alt=\"\" width=\"1041\" height=\"670\" /></a>\n\n<p>\n<em>Screen capture of Pylowiki with ChaosTheory theme</em>\n</p>\n: P Thanks for reading",
    "date": "2011-03-26 22:00:42",
    "timestamp": 1301191242,
    "comments": [],
    "metadata": {}
  },
  "voice-over-ip-with-teamspeak": {
    "name": "voice-over-ip-with-teamspeak",
    "id": "343",
    "link": "http://russell2.ballestrini.net/voice-over-ip-with-teamspeak/",
    "title": "Voice Over IP with TeamSpeak",
    "content": "<p>\n<strong>This article will cover running a Voice Over IP service like TeamSpeak on a VPS.</strong>\n</p>\n\n<p>\nVoice Over IP allows users to communicate using audio over the Internet.\n</p>\n\n<p>\nWhen planning for this article I originally was going to cover ventrilo, but their download link was obfuscated behind a heinous php session script. Ventrilo also does not have a Linux client although they have been promising one for quite some time.\n</p>\n\n<p>\nInstead we will cover how to install and configure <a href=\"http://www.TeamSpeak.com\">TeamSpeak</a>.\n</p>\n\n<p>\n<strong>Installing a teamspeak server on your VPS</strong>\n</p>\n\n<ol>\n<li> Create a user to run teamspeak.\n\n<pre>sudo adduser teamspeak\n</pre>\n\n<em>follow the prompts</em>\n\nPersonally I don't want the teamspeak user to have ssh access so I added the following to /etc/ssh/sshd_config:\n\n<pre>DenyUsers teamspeak\n</pre>\n\nThen reload ssh server config:\n\n<pre>sudo service ssh reload\n</pre>\n\n</li>\n<li>Setup the installation environment.\n\nThe following 4 commands will create a directory to hold your installation, change the ownership of the directory to the teamspeak user, change the working directory to the new folder and then become the teamspeak user:\n\n<pre>sudo mkdir /opt/teamspeak\nsudo chown teamspeak:teamspeak /opt/teamspeak\ncd /opt/teamspeak\nsudo su teamspeak\n</pre>\n\n</li>\n<li>Download and extract TeamSpeak server software.\n\nFind the proper package for your VPS and download it, in my case I ran:\n\n<pre>wget http://teamspeak.gameserver.gamed.de/ts3/releases/beta-30/teamspeak3-server_linux-amd64-3.0.0-beta30.tar.gz\n</pre>\n<em>For best results download the latest version of teamspeak.</em>\n\nA teamspeak tarball should now exist in your present working directory.  We can extract the files from the tarball by issuing the following command:\n\n<pre>tar xvf teamspeak3-server_linux-amd64-3.0.0-beta30.tar.gz --strip-components=1\n</pre>\n<em>No you don't have to type that file name out! The bash shell has tab completion, type 'tar xvf teamsp' and then press tab.  : )</em>\n\n</li>\n<li>\nInstall the TeamSpeak server software.\n\nI had success running:\n\n<pre>./ts3server_startscript.sh start\n</pre>\n\nWrite down the auto generated serveradmin password.\n\n</li>\n<li>\nConfigure TeamSpeak to start at system bootup.\n\nCreate a cronjob under the teamspeak user:\n\n<pre>crontab -e\n</pre>\n\nPlace the following into teamspeak's crontab:\n\n<pre>@reboot /opt/teamspeak/ts3server_startscript.sh start\n</pre>",
    "date": "2011-04-03 12:11:18",
    "timestamp": 1301847078,
    "comments": [],
    "metadata": {
      "_edit_last": "1",
      "_series_part": "2",
      "_spost_short_title": null
    }
  },
  "introduction-to-the-vast-vps-guide": {
    "name": "introduction-to-the-vast-vps-guide",
    "id": "388",
    "link": "http://russell2.ballestrini.net/introduction-to-the-vast-vps-guide/",
    "title": "Introduction to The Vast VPS Guide",
    "content": "<strong>'The Vast VPS Guide', a series to document and display the fun things an administrator can do with a Linux Virtual Private Server.</strong>\n\nA VPS (Virtual Private Server) is a hosted virtual machine with root access.  Having root on the server allows the administrator to install or run ANYTHING they wish.  \n\nNote: This series expects the reader to understand how to use SSH and the Linux command line.  The following articles were written and tested using a ubuntu kvm guest and modifications may vary depending on guest operating system.",
    "date": "2011-04-02 13:32:55",
    "timestamp": 1301765575,
    "comments": [],
    "metadata": {
      "_series_part": "1",
      "_wp_old_slug": "the-vast-vps-guide",
      "_edit_last": "1"
    }
  },
  "dropbox-encryption-with-truecrypt": {
    "name": "dropbox-encryption-with-truecrypt",
    "id": "422",
    "link": "http://russell2.ballestrini.net/dropbox-encryption-with-truecrypt/",
    "title": "Dropbox Encryption with TrueCrypt",
    "content": "<p>\n<strong><a href=\"http://dereknewton.com/2011/04/dropbox-authentication-static-host-ids/\" target=\"_blank\">\nDerek Newton</a> recently invoked discussion about insecurities in Dropbox authentication.</strong> In his article he describes how an attacker could exploit Dropbox and gain access to unshared files.  The concerns he raised do appear accurate however we must remember that security is an onion.\n</p>\n\n<img class=\"alignright wp-image-446\" title=\"onion\" src=\"/wp-content/uploads/2011/04/onion.png\" alt=\"\" width=\"225px\" />\n\n<p>\nAn onion, like security, has layers to protect its vital parts.  The vital parts are more vulnerable when its security model only possess one layer.  As we add layers to our security model, our system's protection grows exponentially.\n</p>\n\n<p>\nIn the case of Dropbox, the username and password act as the first layer.  Experts agree that a simple authentication layer will provide enough protection for nonsensitive data.  However when attempting to protect sensitive data we must pair authorization with encryption.  \n</p>\n\n<p>\nGenerally speaking file systems have maintained a sense of insecurity, which makes them useful.  Not encrypting files on Dropbox is akin to not encrypting files on a shared PC.  Sensitive data should always be encrypted <em>regardless</em> of its location or media.  We should treat sensitive data-at-rest on Dropbox the same way we treat sensitive data on local, optical or flash disk. <em>We should encrypt it!</em>\n</p>\n\n\n<a href=\"http://www.truecrypt.org/\" target=\"_blank\">\n<img src=\"/wp-content/uploads/2011/04/truecrypt.png\" alt=\"\" title=\"truecrypt\" width=\"225px\" class=\"alignleft wp-image-502\" /></a>\n\n\n<p>\n<strong>So how does a user encrypt their Dropbox?</strong> \n</p>\n\n<p>\nMy strongly opinionated solution uses TrueCrypt to create an encrypted volume in the Dropbox directory.  Simply treat the Dropbox like a normal directory, follow the TrueCrypt documentation to build a volume, and give Dropbox a chance to sync the data.  When the sync completes, the TrueCrypt volume will be mountable on each of your Dropbox enabled computers. \n</p>\n\n<p>\nI have to admit at first I was skeptical, but the software cooperates surprisingly well and after the initial sync proceeding syncs occur quickly!  I prefer TrueCrypt because it is open source, cross platform, and free (both in freedom and cost).  TrueCrypt also functions and performs better then any other solution including commercial products like GuardianEdge or PGP both recently acquired by Symantec.\n</p>\n\n<p>\nAll security and encryption software should remain open sourced and peer reviewed to prevent harmful tampering.  Commercial software, written in a black-box vacuum, prevents customers from viewing its code and procedures.  We cannot trust software for security when we cannot view its source code.\n</p>\n\n<p>\n<strong>You should follow me on twitter <a href=\"http://twitter.com/russellbal\">here</a>.</strong>\n</p>",
    "date": "2011-04-16 12:50:38",
    "timestamp": 1302972638,
    "comments": [
      {
        "id": 98,
        "parent_id": 0,
        "author_ip": "192.168.1.23",
        "author": "HackerNews",
        "email": "HackerNews@HackerNews.com",
        "content": "HackerNews import\n\n\t\nalbertzeyer 6 days ago | link\n\nTrueCrypt doesn't really work because you can't mount it from several places at the same time.\nI looked up for other solutions which allow this (for example by doing the encryption separately for each file; but I could also think about other things).\nI only found http://www.arg0.net/encfs so far but I didn't tried yet. It does the encryption separately for each file.\nreply\n\t\n\nkijinbear 5 days ago | link\n\nTrueCrypt is OK if you only use Dropbox for backup. But as soon as sync comes into the picture, all hell breaks loose. Remember, a TrueCrypt volume contains an entire file system inside. File systems seriously don't like it when you modify bits and pieces of it from under their feet. You can't use any file in the volume until you've finished using the volume in the other computer.\nEncFS is somewhat better in terms of sync, because it encrypts each file separately.\nHere's a nice tutorial: http://pragmattica.wordpress.com/2009/05/10/encrypting-your-...\nI have ~2.5GB of encrypted files in my Dropbox, and as long as I don't try to edit the same file in two places at the same time, it works like a charm. (Conflicts can be nasty, as another comment says. But it's not as bad as TrueCrypt, because EncFS works per-file.) Editing different files at the same time, on the other hand, is kosher.\nThere are, however, a few other problems with EncFS.\nFirst of all, you can't mount an EncFS directory tree from Windows. There's an actively developed Windows port of EncFS out there (which uses Dokan), but I've never been able to get it to work properly.\nAlso, Windows has a maximum path length limit of 260 chars. Since EncFS pads file names with unique IVs and base64 encodes them after encrypting them, you can reach this limit after only 3-4 levels of subdirectories. Paths which are too long for Windows will simply be omitted from the Windows copy of your Dropbox folder. So if you ever treat the Windows copy as an authoritative backup, you're in for a nasty surprise. Linux is OK up to 4096 chars. I'm not sure how OSX does it.\nLastly, Dropbox in Linux tends to choke when you add a large number of files with encrypted filenames. It's not uncommon for Dropbox to declare a \"case conflict\" in that situation when there actually isn't any case conflict. This can seriously damage your EncFS directory tree, because Dropbox adds the phrase \"(Case Conflict 1)\" to random files and folders. An easily solution is to stop Dropbox, add the files, and restart Dropbox again. But this can get annoying after a while.\nreply\n\t\n\nbobds 5 days ago | link\n\nThat didn't sound completely right, so I checked it out. You can have a path with ~32k characters actually.\nhttp://msdn.microsoft.com/en-us/library/aa365247%28VS.85%29....\nI know that not all software supports those long file/path names, so yeah, you are probably in for some surprises.\nreply\n\t\n\nkijinbear 5 days ago | link\n\nYes, it's possible to use up to 32K characters in the path if you use the \"\\\\?\\\" prefix. Support for this feature may be hit-or-miss, though, so I wouldn't count on it.\nreply\n\t\n\ncookiecaper 6 days ago | link\n\nYou can do this with ecryptfs, which is built into the kernel. It transparently encrypts each file in a \"lower\" directory; this directory is mounted to a \"higher\" directory where the files are transparently encrypted/decrypted as needed. It's a great solution for this kind of stuff and it lives in the kernel. You don't have to deal with FUSE or any of that.\nreply\n\t\n\npbh 6 days ago | link\n\nencfs didn't work for me either. See this comment from a year and a half ago:\nhttp://news.ycombinator.com/item?id=895660\nDropbox supposedly encrypts server side. If you want to encrypt client-side, use SpiderOak or TarSnap. These hacks on top of Dropbox don't mesh with the semantics of Dropbox properly and will cause problems days, weeks, or months down the line unless managed extremely carefully.\nreply\n\t\n\nalbertzeyer 5 days ago | link\n\nYes, I already thought about that case. But I don't think it should be complicated to extend EncFS to handle that well (to just provide both versions to the end user in that case).\nreply\n\t\n\ndermatthias 6 days ago | link\n\nI use encFS for the sensitive files inside my Dropbox and it works really well. Because it is a per file based encryption and most of my sensitive files are rather small (&lt;1MB), there are no (noticeable) speed problems.\nIn combination with some (really simple) bash scripts for mount- and unmounting, this is imho a good way to encrypyt files inside your Dropbox.\nreply\n\t\n\nLocke1689 6 days ago | link\n\nInstead of attempting a nasty (and possibly insecure) workaround like this, you should use http://www.tarsnap.com/\nreply\n\t\n\naceofspades19 5 days ago | link\n\nIf you could use tarsnap in Canada, I would totally use it\nreply\n\t\n\nknight99 6 days ago | link\n\nI am wondering how/if TrueCrypt would properly handle being synced if mounted on multiple machines at the same time. One of the biggest benefits of DropBox to me is the access the files instantly on all my computers. If I have to unmount and mount the TrueCrypt volume to change files, that benefit goes away.. What happens if I forget to unmount the volume at home, can I safely change the files while away?\nreply\n\t\n\ndanieldk 6 days ago | link\n\nI tried this, and got multiple versions of the truecrypt volume file, filling up my Dropbox account. Pretty nasty, since I had to merge the changes of each variation back in one volume by hand.\nThis article resembles the recent posts about using git on Dropbox - I wonder if people actually tried this for a longer period, because it just doesn&#039;t work, unless you mount the volume only one machine simultaneously and always sync after unmounting (easy to forget after mobile use).\nreply\n\t\n\njerrya 6 days ago | link\n\nYes, I took the plunge one day and put a truecrypt volume on dropbox and then put my quicken files in that truecrypt volume, and, ....\nMajor lossage ensued when I found dropbox gave me multiple versions of that truecrypt volume.\nreply\n\t\n\nGroxx 6 days ago | link\n\nThey&#039;re not. While the data spaces may not overlap, the file table very likely will (as much of it will fit in a single encryption block), so any parallel changes collide there in even the best circumstance.\nI program out of my TrueCrypt volume while in Windows, unmount it, and let it sync. It&#039;s a super-easy backup, and DropBox is the only one I&#039;ve used that handles a 1GB file correctly, and it does it without a hitch.\nWorst-case-scenario is you do cause a collision, and you just mount both volumes at once and copy stuff to the one you want to keep.\nreply\n\t\n\nhedgehog 6 days ago | link\n\nNope, you can only mount the disk one place at a time. It wouldn&#039;t be an easy thing to build.\nreply\n\t\n*\n\n1 point by foxhop 6 days ago | link\n\nI guess more testing would be required to know for sure. Its good news that both services are free so it wouldn&#039;t be difficult to test out your circumstances.\nI use Dropbox more like a thumbdrive for temporary transfers and I like to have the warm and fuzzies that my data at rest is protected with strong encryption.\nreply\n\t\n\nvibhavs 5 days ago | link\n\nI store sensitive data in Dropbox using an OS X encrypted disk image in my Dropbox directory. It&#039;s not an elegant solution, but it gets the job done. I mount the password-protected disk image when needed, access the data, and unmount when finished. Upon unmounting, Dropbox syncs the encrypted blocks to S3, other computers, etc.\n(Encrypted disk-images can be fairly handy. I picked up the trick from a friend and colleague who used them to protect email and other sensitive documents on his laptop. E.g. he sym-linked Mail.app&#039;s mail directory, ~/Library/Mail, to the disk image.)\nreply\n\t\n\nsunchild 5 days ago | link\n\nDoesn&#039;t this prevent backup/syncing while the image is open?\nreply\n\t\n\ndaydream 5 days ago | link\n\nSure, but in practice for a single-user dropbox account it&#039;s not a big deal, IF you unmount the volume when you&#039;re done with it.\nI do the same thing that the GP does - mount the disk image, work with the files, then unmount. It&#039;s been working great for me for a while, though as the GP says it&#039;s not very elegant.\nreply\n\t\n\nDerbasti 6 days ago | link\n\nAlso, this clearly breaks web access and mobile device access to your files.\nThat said, I have been using this scheme for my most valuable data sets for about a year without problems.\nreply\n\t\n\nchanux 6 days ago | link\n\nMy take on encrypting stuff on Dropbox..\nhttp://chanux.wordpress.com/2010/10/10/portable-encrypted-vi...\n(It&#039;s not not just aiming Dropbox but the only place I actually use it on is Dropbox.)\nreply\n\t\n\niam 6 days ago | link\n\nI don&#039;t profess to know much about security, but unless TrueCrypt does block-level encryption, AND dropbox does block-level syncing, wouldn&#039;t this scheme work out really poorly for storing large TrueCrypt volumes?\nreply\n\t\n\npsykotic 6 days ago | link\n\nAssuming a strong form of encryption with minimal information leakage, avalanching would imply that flipping even a single bit of plaintext in a volume of N bits would force ~N/2 bits of ciphertext to change with a distribution approaching maximum entropy. Flipping any number of bits should have the same statistical effect.\nOf course, that&#039;s infeasible for file system encryption and isn&#039;t how TrueCrypt works. An idea is to use a one-to-one mapping between TrueCrypt blocks and DropBox files. Write a file system watcher script that picks up changes to files in the DropBox folder and mirrors the changes to the corresponding blocks in a TrueCrypt volume that lives outside the DropBox folder. The other way around, too.\nDo you see anything obviously wrong-headed with this approach? I admittedly don&#039;t know the details of TrueCrypt&#039;s crypto. If it already does file-level rather than block-level encryption, these tricks wouldn&#039;t be necessary. But file-level encryption would seem to leak way too much information to satisfy the truly paranoid.\nreply\n\t\n*\n\n2 points by foxhop 6 days ago | link\n\nThat is a great question, If you find the answer let me know.\nAt this point I have been playing around with 700mb file volumes. When I make an addition to the volume, it takes about 2 minutes to sync. I have not tested deletions yet.\nreply\n\t\n\nhedgehog 6 days ago | link\n\nI used to store use TrueCrypt + Dropbox to store some important files. Dropbox will detect and sync only the changed portions of your TrueCrypt volume. It takes a while (I think it does some sort of rolling checksum thing to detect differences so it has to read through the entire thing locally) but works ok. It would only sync when the volume was unmounted though.\nreply",
        "date": "2011-04-22 18:05:44",
        "timestamp": 1303509944
      },
      {
        "id": 85,
        "parent_id": 0,
        "author_ip": "85.181.92.231",
        "author": "Marius",
        "email": "secretmail@blablabervliblu.com",
        "content": "I thought about using TrueCrypt on my DropBox but then I assumed it would be inefficient, since all the encrypted content would change if I just changed a small portion of the enclosed data.\nDo you have a configuration proposal that doesn't lead to DropBox having to sync for hours after changing 1KB of my 8GB of encrypted data?",
        "date": "2011-04-16 15:06:26",
        "timestamp": 1302980786
      },
      {
        "id": 86,
        "parent_id": 85,
        "author_ip": "192.168.1.21",
        "author": "Russell Ballestrini",
        "email": "russell.ballestrini@gmail.com",
        "content": "Don't use a 8 gb volume.  I use a volume size of 700mb and it works really well, the changes sync in about 4 minutes.\n\nI use 700mb because it allows me to place it onto a cdrw.  Its the perfect size for backing up if the need arises.",
        "date": "2011-04-16 15:54:30",
        "timestamp": 1302983670
      },
      {
        "id": 87,
        "parent_id": 0,
        "author_ip": "193.77.211.111",
        "author": "Matija",
        "email": "matija.polajnar@gmail.com",
        "content": "Obviously, encryption of single files (instead of the whole container) would be way better in this case: mounting an image (ie. the container) on multiple computers at once is probably a pretty dangerous thing. So I would rather suggest ecryptfs (for those with a sane OS), although I have admittedly not yet tried it with DropBox.",
        "date": "2011-04-16 19:06:54",
        "timestamp": 1302995214
      },
      {
        "id": 88,
        "parent_id": 0,
        "author_ip": "76.254.57.123",
        "author": "Evan",
        "email": "online@evan.co.nz",
        "content": "Mounting on multiple machines simultaneously does cause all kinds of madness. However, Dropbox syncs at the bit level, so you won't have any issues with updating the entire volume after you save an individual file inside it, AFAIK.",
        "date": "2011-04-16 20:31:24",
        "timestamp": 1303000284
      },
      {
        "id": 89,
        "parent_id": 0,
        "author_ip": "67.80.49.163",
        "author": "Crypto",
        "email": "fake@email.com",
        "content": "Using TrueCrypt on DropBox in the way you describe is insecure.  It leaves multiple backup copies of your TrueCrypt volume in DropBox's backups, and opens the volume key to discovery (not your passphrase necessarily, unless you change it often, just the volume/session key in the header)...\n\nIf you do this, for it to be secure, the volume key needs to change every time you edit it (i.e. you need a new volume for every upload).  You should create a new truecrypt volume per \"save\" (i.e. every time you edit something that is encrypted, create a new truecrypt volume and put it in there and remove the old one) so that the volume/session key changes.  Obviously this is more painful than the above.\n\nIn short: For stuff you need encrypted, encrypt with GPG from a local copy each time and replace the old one (new session key) or a new TrueCrypt volume (key) per instance of the data. \n\n(PREEMPTIVE NOTE: CHANGING YOUR PASSPHRASE/KEYFILE DOES NOT CHANGE THE VOLUME KEY - I.E. THE KEY THAT THE DATA IS ENCRYPTED WITH.)",
        "date": "2011-04-16 21:35:43",
        "timestamp": 1303004143
      },
      {
        "id": 91,
        "parent_id": 89,
        "author_ip": "20.132.64.141",
        "author": "Russell Ballestrini",
        "email": "russell.ballestrini@gmail.com",
        "content": "Your research appears correct.\n\nI would however argue that using some encryption is better then no encryption.\n\nTotal security does not exist.  Security eventually amounts to layers of abstraction and obfuscation.\n\nI could gain a bit more security by naming my TrueCrypt volume 'pictures.jpg' vs 'TrueCryptVolume'.\n\nKeep in mind the worse enemy to secure systems is the human element.",
        "date": "2011-04-18 14:17:09",
        "timestamp": 1303150629
      },
      {
        "id": 974,
        "parent_id": 0,
        "author_ip": "173.83.247.214",
        "author": "A Tale of Two Security Scandals | Irreal",
        "email": null,
        "content": "[...] nice solution proposed by Russell Ballestrini is to turn your Dropbox into a TrueCrypt volume. Then your data is automatically encrypted and [...]",
        "date": "2011-12-27 15:49:18",
        "timestamp": 1325018958
      },
      {
        "id": 256,
        "parent_id": 0,
        "author_ip": "50.8.185.145",
        "author": "Daniel",
        "email": "helsten@yahoo.com",
        "content": "Spideroak.com seems to have the best solution.  You create your own key on the client.",
        "date": "2011-07-02 07:14:35",
        "timestamp": 1309605275
      }
    ],
    "metadata": {
      "_edit_last": "1",
      "_stcr@_skyvanessabr@outlook.com": "2014-01-13 04:15:30|Y"
    }
  },
  "connecticut-killed-affliate-marketing-with-amazon-com": {
    "name": "connecticut-killed-affliate-marketing-with-amazon-com",
    "id": "593",
    "link": "http://russell2.ballestrini.net/connecticut-killed-affliate-marketing-with-amazon-com/",
    "title": "Connecticut killed affliate marketing with Amazon.com",
    "content": "<strong>Connecticut killed affliate marketing with Amazon.com </strong>\n\nI just opened my amazon account and read this:\n\n<blockquote>Account Closed\nThis account is closed and will not generate referrals. Access to this site is for historical purposes only.</blockquote>\n\nWhen I checked my email, I had this letter from Amazon.com:\n\n\n\n<blockquote>Hello,\n\nFor well over a decade, the Amazon Associates Program has worked with thousands of Connecticut residents. <strong>Unfortunately, the budget signed by Governor Malloy contains a sales tax provision that compels us to terminate this program for Connecticut-based participants effective immediately. </strong>It specifically imposes the collection of taxes from consumers on sales by online retailers - including but not limited to those referred by Connecticut-based affiliates like you - even if those retailers have no physical presence in the state.\n\n<strong>We opposed this new tax law because it is unconstitutional and counterproductive. It was supported by big-box retailers, most of which are based outside Connecticut, that seek to harm the affiliate advertising programs of their competitors.</strong> Similar legislation in other states has led to job and income losses, and little, if any, new tax revenue. We deeply regret that we must take this action.\n\n<strong>As a result of the new law, contracts with all Connecticut residents participating in the Amazon Associates Program will be terminated today, June 10, 2011. </strong>Those Connecticut residents will no longer receive advertising fees for sales referred to Amazon.com , Endless.com , MYHABIT.COM or SmallParts.com . Please be assured that all qualifying advertising fees earned on or before today, June 10, 2011, will be processed and paid in full in accordance with the regular payment schedule.\n\nYou are receiving this email because our records indicate that you are a resident of Connecticut. If you are not currently a resident of Connecticut, or if you are relocating to another state in the near future, you can manage the details of your Associates account here . And if you relocate to another state after June 10, 2011, please contact us for reinstatement into the Amazon Associates Program.\n\nTo avoid confusion, we would like to clarify that this development will only impact our ability to offer the Associates Program to Connecticut residents and will not affect their ability to purchase from www.amazon.com .\n\nWe have enjoyed working with you and other Connecticut-based participants in the Amazon Associates Program and, if this situation is rectified, would very much welcome the opportunity to re-open our Associates Program to Connecticut residents.\nRegards,\n\nThe Amazon Associates Team\n</blockquote>\n\n<strong>Summary</strong>\n\n<ul>\n\t<li>Amazon.com does not have a facility in CT. </li>\n        <li>CT passed a new bill.</li>\n        <li>The bill declares: if a customer from any state clicks on a CT resident's affiliate link and purchases an item, that customer/Amazon.com should have to pay CT tax.</li>\n</ul>\n\n<strong>My Opinion</strong>\n\n<ul>\n        <li> This seems bad.</li> \n        <li> I have less money to spend in Connecticut. </li> \n        <li> This smells like double taxation.</li>\n        <li> One more reason to move out of Connecticut when looking for Tech or IT jobs.</li>\n</ul>",
    "date": "2011-06-11 14:10:09",
    "timestamp": 1307815809,
    "comments": [
      {
        "id": 147,
        "parent_id": 0,
        "author_ip": "99.50.227.39",
        "author": "human mathematics",
        "email": "crasshopper@gmail.com",
        "content": "The title gives away your bias. Amazon, in fact, killed affiliate marketing in CT. The state of Connecticut did something the company didn't like.\n\nOne can infer that this is fisticuffs, because Amazon isn't terminating all *sales* to Connecticut, which would amount to a refusal to pay the income tax. Instead they've chosen to infuriate a minority group whom they hope will loudly oppose the tax measure.",
        "date": "2011-06-11 20:13:10",
        "timestamp": 1307837590
      },
      {
        "id": 141,
        "parent_id": 0,
        "author_ip": "76.16.211.30",
        "author": "damian",
        "email": "xdamiancx@gmail.com",
        "content": "Same thing happened in Illinois earlier this year.",
        "date": "2011-06-11 15:47:05",
        "timestamp": 1307821625
      },
      {
        "id": 152,
        "parent_id": 0,
        "author_ip": "184.91.132.53",
        "author": "Patrick Ballestrini",
        "email": "silentlord833@yahoo.com",
        "content": "Hey, Um, I don't live in Connecticut any more....Just throwing that out there... Hint hint.",
        "date": "2011-06-12 07:52:14",
        "timestamp": 1307879534
      },
      {
        "id": 150,
        "parent_id": 0,
        "author_ip": "173.216.25.148",
        "author": "jbratton",
        "email": "jbratton@classicnet.net",
        "content": "Same with Arkansas.  Not a big tech state,either.",
        "date": "2011-06-11 22:48:30",
        "timestamp": 1307846910
      },
      {
        "id": 148,
        "parent_id": 140,
        "author_ip": "98.210.164.172",
        "author": "Tom",
        "email": "tom@mailinator.com",
        "content": "agreed - if anything this kind of legislation helps small local businesses in CO as it means the megacorps like amazon have to pay the same taxes they do",
        "date": "2011-06-11 21:03:15",
        "timestamp": 1307840595
      },
      {
        "id": 144,
        "parent_id": 0,
        "author_ip": "93.186.22.244",
        "author": "matthew fedak",
        "email": "matthewfedak@hotmail.com",
        "content": "That sucks, I live in eu and a stupid laws been introduced this year which bans cookies effectively unless you prompt the user to accept them. Along with google panda its another hit for anyone who thought they could make a few bucks here or there in affiliate marketing!",
        "date": "2011-06-11 17:12:25",
        "timestamp": 1307826745
      },
      {
        "id": 143,
        "parent_id": 0,
        "author_ip": "158.143.173.125",
        "author": "Ian Wright",
        "email": "ian@instantcarinsuranceonline.com",
        "content": "I'm sorry whenever I hear this happening to people anywhere. It really sucks when a revenue source major or not is just taken away from you. As more and more states pass similar laws, I'm wondering at some point of Amazon will cave in or just give up on their affiliate program altogether.",
        "date": "2011-06-11 16:47:20",
        "timestamp": 1307825240
      },
      {
        "id": 140,
        "parent_id": 0,
        "author_ip": "74.72.58.243",
        "author": "mordicai",
        "email": "mordicai@livejournal.com",
        "content": "Wait-- so Amazon using its corporate clout against its users &amp; consumers is...Connecticut's fault?  For...what, collecting taxes?  For not kowtowing to Amazon's corporate interests?  Your headline is backwards...AMAZON killed the affiliate marketing program, not the state of Connecticut.  Though hey-- I'm all for tech &amp; IT people moving to New York...we need you here!",
        "date": "2011-06-11 15:40:28",
        "timestamp": 1307821228
      },
      {
        "id": 153,
        "parent_id": 0,
        "author_ip": "93.150.176.11",
        "author": "Luca Boccianti",
        "email": "luca.boccianti@gmail.com",
        "content": "Matthew Fedak: it's not a law but a directive and as any directive it's basically a suggestion to EU nations to make or change their own laws accordingly.  I do wish my nation, Italy, really take all EU directives seriously and legiferate accordingly.  Unfortunately that's not the case.\n\nAnyway, what's wrong with letting the users know what we developers are doing under the hood of their computers and let them choose if the spreading of *their* data is ok with them or not?  \n\nAre you afraid that having the users click another ok button would be too much for them and so they will abandon a transaction?  Well, have you ever booked an airplane ticket?  If you're interested in a transaction, you will do any effort required.\n\nOr are you afraid that if users really understand what you do with cookies (i.e. tracking them and spreading their data without their knowledge/consent and/or earning a commission on their purchases - if based on your suggestions - that would not increase their cost) then they will not accept that?  \n\nThen don't you think it would be anti-ethical to keep users in ignorance and refusing them the right of choice just because it better fits your own interest as a developer, or your clients' one?\n\nIf I should sacrifice your or mine right to \"earn some bucks here or there\" (or better, making a little harder to exercise it by warning users) in order to provide a better privacy for the general user (me included) then I think it's ok for me.",
        "date": "2011-06-12 08:27:37",
        "timestamp": 1307881657
      },
      {
        "id": 154,
        "parent_id": 140,
        "author_ip": "75.54.21.138",
        "author": "An Amazon Affiliate",
        "email": "markob17@hotmail.com",
        "content": "Mordicai, you obviously don't know what you are talking about.",
        "date": "2011-06-12 08:59:26",
        "timestamp": 1307883566
      },
      {
        "id": 167,
        "parent_id": 0,
        "author_ip": "192.168.1.21",
        "author": "Russell Ballestrini",
        "email": "russell.ballestrini@gmail.com",
        "content": "Amazon.com does not have a facility in CT. A new bill passed in CT declared that if a person in another state (like Florida) clicked on a Connecticut residents affiliate link and purchased an item, they should have to pay CT tax.",
        "date": "2011-06-16 17:15:56",
        "timestamp": 1308258956
      },
      {
        "id": 168,
        "parent_id": 0,
        "author_ip": "192.168.1.21",
        "author": "hackernews",
        "email": "hackernews@ycombinator.com",
        "content": "jonkelly 5 days ago | link\n\nI'm still kind of mystified as to how these laws keep getting passed. I think there is clearly room for debate about whether or not Amazon customers should pay sales tax in their home jurisdictions. But, there is no reasonable debate about these laws. They have perfectly predictable results: law is passed, Amazon nukes affiliates, state loses income tax from affiliates, gains nothing as a result. I find it incredible that state lawmakers either think that it will be different in their state or that it's worth destroying the incomes of their taxpayers to just make a symbolic gesture. Does anyone have a reasonable justification for the lawmakers' actions or is it really just a mix of idiocy, vindictiveness, and appeasement of local donors?\nreply\n\t\n\nDanielBMarkham 5 days ago | link\n\nI'll play devil's advocate.\nFirst, governments consistently think of the economy as a static model. So you add up how many online sales you have, add in a percentage tax, and expect to make that money.\nSecond, Amazon pulling out or not is simply a short-term obstacle. The long term strategy is to put a stake in the ground, then continue to lobby and work at the national level. This sets precedent, even if it doesn't work.\nThird, there's a lot of ignorance. You don't have to pass an economics test to become a legislator. I think sometimes in any group that deals with another group there's an \"us versus them\" mentality that takes over. In some folks' mind, if you're making money, they should get a share of it to spread around. Trying to adapt and configure your business model to optimize -- or simply just to survive -- is a sign of selfish, unethical behavior. Both sides of tax debates engage in a lot of emotional over the top rhetoric. It's easy to lose track of priorities and principles.\nreply\n\t\n\ngaius 5 days ago | link\n\nActually, it only goes as far as if you're making money, they should get a share of it. Pork barrels aren't just Federal.\nreply\n\t\n\nDanielBMarkham 4 days ago | link\n\nIndeed.\nI find it very difficult to underscore this point without going on a rant -- perhaps because I find it so bad.\nLet's say I'm living in IL and making 10K a month with Amazon sales. This is all because of a lot of hard work putting bits on a server somewhere in Austin or something. People come from all over the world to consume content and buy things.\nAside from the legal discussion, in practical terms what part of this equation involves the state of Illinois? I don't mean to be facile, but looks to me like the only reason they're coming for the money is because they can. In any other scenario we'd call this a shakedown. But because politicians do it we call it policy. Weird.\nI'm not by any means arguing for no government, just pointing out that most times when I pay a tax there's something concrete and direct in return. If I pay property taxes, it goes to the local school. If I pay a road toll, it goes for road maintenance. If I buy a fishing license, I pay for game inspectors. This payment and feedback loop allows me to be able to judge if a tax is working or not -- and form some kind of opinion about what rates might be best. Here I'm just paying the state because they have the power to hurt me if I don't pay them. Seems to me once you reach a certain point in decoupling taxes from benefits, there's no rational basis to have any kind of compromise any more. Instead you get these hugely polarized debates. This is not a good system for people to live in.\nreply\n\t\n\nearl 4 days ago | link\n\nWhat part of it involved IL? Are you being deliberately obtuse? You living in IL. States charge taxes to provide services to the people who live and enter the state. You live in the state, therefore it is entirely appropriate for IL to charge you income tax. If you don't like it, vote in different representatives because the other thing that you get to do in IL is vote for IL state representatives.\nreply\n\t\n\ndanking00 4 days ago | link\n\nIf I understand his metaphor and current law correctly: this law changes nothing about income tax. Regardless of this law, Mr. Amazon-Affiliate pays income tax to the State of Illinois on his Amazon affiliate income. I believe the poignant line from the blog post is:\n  ... imposes the collection of taxes from consumers on sales\n  by online retailers \u2013 including but not limited to those\n  referred by Connecticut-based affiliates like you \u2013 even if\n  those retailers have no physical presence in the state.\nI interpret this as saying \"if a Connecticut resident refers customer X to company Y, and X buys a product from Y, then company Y must pay tax on that sale.\" The only necessary tie to Connecticut is the referrer.\nOf course, they'd also tax Mr. Amazon-Affiliate's affiliate income as well. That smells like double taxation.\nAlso, I think your comment suggesting that \"If you don't like it, vote in different representatives,\" is rude. Obviously, Mr. Markham is aware of his Democratic rights and duties. He very well might vote against this sort of legislation, and he doesn't deserve to hear derisive comments when voicing his disapproval.\nedit: formatted the quote for easier reading\nreply\n\t\n\ncrasshopper 4 days ago | link\n\nI actually don't understand that part of it. How do sales tax on CT consumers -- who may or may not be driven to Amazon by a CT affiliate -- compel Amazon to terminate the affiliate programme?\nCT-based affiliates may be driving traffic from outside CT, and CT-based purchases -- the thing being taxed -- might have been driven by an affiliate in Iceland.\nTerminating _sales_ to Connecticut would amount to a refusal to pay the tax. Instead Amazon has chosen to raze the profits of a minority of CT constituents. They must be hoping that the ousted affiliates will march on New Haven.\n(If I were a successful affiliate, though, I'd just make an out-of-state LLC and pass income through.)\nreply\n\t\n\nSoftwareMaven 4 days ago | link\n\nI believe the problem comes from having a physical presence in the state. Most states have sales tax rules that say if you have a physical presence, you must charge sales tax. The rules were changed such that Connecticut is treating affiliates as a physical presence. If Amazon keeps the affiliates, they have to charge sales tax on everybody who purchases in Connecticut, which they really don't want to do.\nWhile I'm sure part of the reason they don't want to charge tax is that no tax makes Amazon cheaper, I bet a bigger portion is the headache of trying to manage every single municipalities convoluted sales tax rules. I know in my city (in Utah), there are dozens of rules for types of goods that determine different rates. Imagine doing that on a nationwide scale.\nreply\n\t\n\nmachinedog 3 days ago | link\n\nI think the problem is that they want to apply a sales tax to anyone that is referred to Amazon /worldwide/ by a Connecticut referrer. That is why Amazon argues that it is unconstitutional. If it were simply taxing Connecticut consumers it would be similarly complicated, but I think there is more constitutional grounds for this. I'd have to check.\nIt's not like Amazon is going to pull out of the state.\nreply\n\t\n\nvampirical 4 days ago | link\n\nThe whole point is that rather than the state happily collecting their income tax on any income generated by state residents, which they have always been able to do, they are attempting to levy a tax against all Amazon commerce transactions based on the fact that they have affiliates operating in the state.\nDanielBMarkham is not making a case that IL isn't entitled to tax his income from the Amazon affiliate program. It is taken for granted that they will tax the additional income, as they have been doing since before this new budget/law. What they are attempting to do is tax more than just income generated by affiliates living in the state, which is where the argument that they are not contributing to the flow of business which would warrant that.\nreply\n\t\n\ncrasshopper 4 days ago | link\n\nVampirical, I'm not sure I understand you. Illinois spends state funds to make Chicago, Peoria, Carbondale,... attractive to live in. People with money thus move to / stay in IL and spend some of their money online. Isn't that a contribution on Illinois' part?\nreply\n\t\n\njoelhaus 4 days ago | link\n\n&gt;Let's say I'm living in IL\n&gt;Aside from the legal discussion, in practical terms what part of this equation involves the state of Illinois?\nIn practical terms, the argument goes that you benefit from the safety and health services that are (at least in part) provided by the state during the hours you work, marketing for Amazon. By extension, Amazon is also benefiting from these services provided (at least in part) by the state, which can be proven by identifying a single payment made from Amazon to an affiliate that is a resident of IL.\nLegally, this could be grounds for one to argue that, by providing marketing services for Amazon while working in IL, nexus[1] has been created.\n[1] http://www.bna.com/state-tax-nexus-p9122/\nreply\n\t\n\njoelhaus 4 days ago | link\n\nCare to comment on why this was down-voted? Did I misunderstand Daniel's question or not appropriately address it?\nI would assume that some have taken this as advocating on behalf of states that impose sales tax on companies due to the residency of their affiliates. I carefully worded my comment so that this was not the case. On the other hand, if you feel I'm wrong about the position that these states have taken, please correct me...\nUPDATED: My position on tax policy is not germane to Daniel's question and based on the rest of this post, would just be redundant to include. When a thread strongly reflects one side of an issue, it's much less dull to try and articulate the counter-point of view... wish more people would try this.\nreply\n\t\n\njbooth 5 days ago | link\n\nThanks, that's the argument exactly, specifically the second part.\nRegarding the static model, it's not so much that it's a static model as that for the overwhelming majority of online purchases, it's displacing a local purchase. It's actually a more capitalist economy from a certain point of view to make online and local purchasing compete on the same ground, especially when the market distortion (which yes stems from sales taxes' existence in the first place, coupled with the commerce clause) discriminates against your voters.\nreply\n\t\n\nyummyfajitas 4 days ago | link\n\nOnline and local purchasing do compete on the same ground. A local store uses CT state services and pays taxes to support them. An online store does not use CT state services and does not pay taxes to support them.\nSimilarly, many companies use Microsoft software and pay license fees. My company does not.\nWould it be \"more capitalist\" if we were forced to pay a Microsoft tax, to eliminate the \"market distortion\" of using lower cost software?\nreply\n\t\n\ncrasshopper 4 days ago | link\n\nyummyfajitas, I think by \"more capitalist\" jbooth means \"more competitive\". You are obviously right that forcing companies to use higher-cost inputs is inefficient, but taxes are not an input -- they're a necessary burden meant to be shared.\nTo take an extreme example, using a tax shelter could lower the cost of a company's services while decreasing the efficiency of the economy.\nreply\n\t\n\nyummyfajitas 4 days ago | link\n\nOf course taxes pay for inputs - the inputs are public goods. Taxes are compulsory only to prevent freeloading - you might enjoy the benefits of the CT state police without paying for them.\nAmazon doesn't enjoy the benefit of the CT state police. Hence, it is not efficient to force them to pay for it.\nA tax shelter is only inefficient if it allows a company to consume services while forcing others to pay for them. If a company does not consume those services, it is inefficient to force the company to pay for them.\nreply\n\t\n\ncrasshopper 4 days ago | link\n\nyummyfajitas, can you give a real-world example of a tax haven which is Pareto equivalent or superior to the company paying its taxes?\nAlso, who says Amazon pays the tax? Insert elasticity argument.\nreply\n\t\n\nyummyfajitas 4 days ago | link\n\nIn the real world nothing is Pareto equivalent or superior to anything else, except maybe for gifts given to hermits. Pareto efficiency is a purely theoretical construct.\nreply\n\t\n\ncrasshopper 4 days ago | link\n\nYou're dodging the essence of the question. Give us one real world example of a case where tax evasion has been A Good Thing.\nyummyfajitas, it frankly sounds like you're econtrolling. I looked at your profile and you seem like a smart, educated person, so I'll respond in good faith so you can see how weak your arguments sound.\n&gt; Taxes are compulsory only to prevent freeloading\nSays who? This sounds like a stylised model from Econ 101.\n&gt; A tax shelter is only inefficient if it allows a company to consume services while forcing others to pay for them.\nProve it.\n&gt; Amazon doesn't enjoy the benefit of the CT state police.\nI don't use the Merritt Parkway but that doesn't mean I don't owe tax on it.\n&gt; Pareto efficiency is a purely theoretical construct.\nThis from someone who just boiled the entire political economy of Connecticut down to a prisoner's dilemma?\n&gt; If a company does not consume those services, it is inefficient to force the company to pay for them.\nWhat is your reasoning?\nPerhaps you are sharing these thoughts before giving them a sound-check? I don't mean to be rude but what you're saying doesn't make sense coming from someone of your educational background.\nreply\n\t\n\nyummyfajitas 4 days ago | link\n\nFirst of all, you are conflating tax avoidance (taking actions to avoid tax liability) with tax evasion (lying to the tax authorities about your liabilities).\nIn the real world, one efficient \"tax shelter\" is Amazon locating itself outside of CT. If Amazon were to pay taxes to CT, then CT would produce the public services necessary to support Amazon. Since Amazon is not located in CT, this would be wasteful. Thus, Amazon locating themselves outside of CT and avoiding CT taxes is A Good Thing.\n[edit: you appear to have edited your post extensively after I responded to it. It's generally polite to indicate when you do this.]\nreply\n\t\n\ncrasshopper 4 days ago | link\n\n&gt; you are conflating tax avoidance with tax evasion\nFair enough. I meant to use tax evasion as an extreme example to prove the inner case, but I can see how it would look like conflation.\n&gt; one efficient \"tax shelter\" is Amazon locating itself outside of CT\nPetitio principii.\n&gt; Since Amazon is not located in CT, this would be wasteful.\nDoes not follow. Where is the dead weight loss?\nreply\n\t\n\nSamReidHughes 4 days ago | link\n\nLiterally every case of tax evasion under a government where the tax rate is too high and the marginal utility of higher taxes is negative.\nreply\n\t\n\ncrasshopper 4 days ago | link\n\nCan you give a real example of a case where this has actually happened? (by marginal utility I assume you mean marginal revenue; governments do not have utility)\nreply\n\t\n\nSamReidHughes 4 days ago | link\n\nNo I mean marginal utility. Governments have utility. For example, they do things like give people money to maintain roads. The marginal utility of higher taxes would be the value of the extra amount of road maintainance, and all other changes to the physical universe, that happens as a result. (Edit: So compare the universes where the government rakes in $x and $x+epsilon of revenue. Which is a better universe? There are plenty of governments where $x+epsilon would produce a worse universe. It might depend on who it gets the extra epsilon from.)\nreply\n\t\n\ncrasshopper 4 days ago | link\n\nUtility is only defined for individuals. It sounds like you're trying to talk about how some sum of citizens' utility functions changes as a result of a change in government spending.\nResponse to your edit: you haven't defined \"better\". And I'm still waiting for a real world case where $x+$epsilon has been dominated by $x.\nreply\n\t\n\nSamReidHughes 4 days ago | link\n\nYes, that's what I'm talking about.\nSo wait, you're looking for a case where $x+$epsilon government revenue has greater utility than $x?\nBecause that's the opposite of what you were asking for before...\nreply\n\t\n\ncrasshopper 4 days ago | link\n\nFirst you have to define what \"better\" means since there are O(300 million) utility functions in the U.S.A.\nSecond, I want a real-world example of a \"negative marginal utility\" of taxation -- which tax rate was raised, who it affected, how they evaded it, and why it was good for the society at large.\nreply\n\t\n\nSamReidHughes 4 days ago | link\n\nHomeless people paying sales taxes.\nDo have any other questions?\nreply\n\t\n\ncrasshopper 4 days ago | link\n\nThis response does not answer either question.\nreply\n\t\n\nSamReidHughes 4 days ago | link\n\nI'm sure if you're dissatisfied to my answers then you'd have no problem coming up with some of your own examples.\nEdit: Wait, you want me to define a utility function for people? That was your first question? That's just a stupid question. Define your own utility function. I don't care what it is. And your second question is stupid too. Sorry, I'm not a keeper-tracker of tax evasion. So let's say \"any time a poor person evades taxes.\" Or many of those cases. If you want a specific example, too bad. If the lack of a specific counterexample of the kind you specified is actually the barrier preventing you from changing your opinion about this, then you shouldn't bother trying to have opinions about things. The fact that some people get more value in government services than they pay in taxes is proof that there are people whose tax evasion would benefit society at large. This is a simple mathematical truism.\nreply\n\t\n\ncrasshopper 4 days ago | link\n\n&gt; I'm sure if you're dissatisfied to my answers then you'd have no problem coming up with some of your own examples.\nI was challenging you on this point because I don't believe you can come up with an actual example of a company's tax evasion benefiting society.\n&gt; Wait, you want me to define a utility function for people?\nNo, I wanted to prompt you to think about defining a single utility function for 300 million people. There is no way to do it, because you cannot compare interpersonal utilities. Your statements about governments having utility suggests that your thinking on this topic is muddled.\n&gt; If you want a specific example, too bad.\nNo empirical evidence, then?\n&gt; The fact that some people get more value in government services than they pay in taxes is proof that there are people whose tax evasion would benefit society at large.\nIt would not benefit society at large; the benefits would be private to the evader and everyone else's tax bill would go up.\n&gt; This is a simple mathematical truism.\nNot only is it false, but I don't think you know what a truism is. A truism is a tautology.\n&gt; stupid ... stupid ... you shouldn't bother trying to have opinions about things\nSamReidHughes, there is a saying that to know a little economics is worse than to know none at all. I think you are overconfident in your theories and should be more humble and polite in dialogue with others.\nreply\n\t\n\njwhite 4 days ago | link\n\nIt may be displacing a local purchase... in a third, unrelated state. Only the affiliate lives in Connecticut, the purchaser may live in Colorado... or Czech Republic, for that matter. I think you have the argument about capitalism backwards. Any time someone says \"that's unfair, we should change the laws to help (inefficient business X) compete more effectively with (efficient business Y)\", they are directly contradicting the idea of capitalism.\nWhere the social good lies is sometimes hard to work out; but I'd guess in this case Connecticut residents would be better off with an efficient Amazon and the ability to become Amazon Affiliates than with the small amount of tax revenue that this legislation tries to capture.\nreply\n\t\n\ntemphn 4 days ago | link\n\nHmmm. \"More capitalist\" would be to eliminate local sales tax to allow businesses to compete.\nreply\n\t\n\nsutro 4 days ago | link\n\nYou don't have to pass an economics test to become a legislator.\nWould that it were so.\nreply\n\t\n\nchopsueyar 5 days ago | link\n\nThe part I have a hard time comprehending in these situations is that many legislators have legal backgrounds being attorneys, yet seem to completely forget about the constitution.\nreply\n\t\n\njbooth 5 days ago | link\n\nSeems that the collateral damage to affiliates from this law is specifically because those legislators understand the commerce clause of the constitution.\nreply\n\t\n\nOstiaAntica 4 days ago | link\n\nThe constitution explicitly prohibits states from interfering with interstate commerce, which is what Connecticut is doing (forcing regulation on a company outside their state), and is why Amazon is calling the law unconstitutional.\nreply\n\t\n\nearl 4 days ago | link\n\nExcept it doesn't occur outside their state if an amazon affiliate is in CT. Hence amazon withdrawing.\nreply\n\t\n\nnkassis 4 days ago | link\n\nExactly, they are bending they are pushing the rules just enough to get away with it and have something that's not unconstitutional (at least according to them) It's in a way ingenious what they are doing. Law making is a form of hacking in my view.\nreply\n\t\n\njonkelly 5 days ago | link\n\nresponding to each point. 1) expecting to \"make that money\" when no other state has is idiocy. 2) the \"stake in the ground\" is just symbolic. Do they really think that helps at all? 3) like I said, I can see both sides of tax debate, I just find it wildly irresponsible to nuke the revenue for a bunch of your constituents because you've lost track of priorities.\nreply\n\t\n\njbooth 5 days ago | link\n\nRE: 1), you'll notice a lot of big market states have all jumped at the same time. That makes the jump look a lot more appealing to me if I'm a state legislator. Eventually Amazon won't want to forfeit affiliate business in every big state economy except Texas and Delaware. Or that's the gamble, at least.\nreply\n\t\n\njonkelly 5 days ago | link\n\nnostromo had the right response to that below. There are plenty of jurisdictions for the larger affiliates to jump over to. I'd be very surprised if Amazon doesn't have a 95/5 ratio like most affiliate programs in terms of revenue produced by largest affiliates. The bigger affiliates will have no problem incorporating in one of the no tax states.\nreply\n\t\n\n18pfsmt 4 days ago | link\n\nJust to illustrate your point, in Colorado the legislature did this same thing (lobbying by small business was used as justification), but the large affiliates I'm familiar with (2, specifically) moved their offices to Laramie, Wyoming (2.5hrs from Denver). Keep in mind, there are ~5M people in our entire state.\nreply\n\t\n\nInclinedPlane 5 days ago | link\n\nThe justification for these actions is simple: every state has spent the last many years spending themselves into a great deal of debt, now they are desperately grasping for revenue and amazon has big pockets.\nreply\n\t\n\njonkelly 5 days ago | link\n\nBut... it hasn't worked at all, producing not even one penny for any of the states that have tried it. Isn't that the proverbial definition of insanity?\nreply\n\t\n\njbooth 5 days ago | link\n\nMost states were actually doing great as of 2000, and have been cutting relative to their cost inflation for the last decade or so.\n10-15% healthcare cost inflation has been spending states into debt.\nreply\n\t\n\nInclinedPlane 5 days ago | link\n\nConnecticut has increased its state spending by about 50-60% over the 2000-2010 period while its economy grew by only 15% in that time.\nreply\n\t\n\njbooth 4 days ago | link\n\nAnd what % of that is healthcare costs? Check it out. I don't know conneticut that well but if you employ a lot of people, and they have healthcare, then it was a huge cost to roll them over these past 11 years.\nreply\n\t\n\npetercooper 4 days ago | link\n\nIt specifically imposes the collection of taxes from consumers on sales by online retailers [..] even if those retailers have no physical presence in the state.\nThe wording of this implies that Amazon now has to charge sales tax to customers in CT. If so, isn't that a massive gain for the state? Or will Amazon just ignore the law?\nreply\n\t\n\nfierarul 5 days ago | link\n\nThink of it as a signaling game: what happens when all the US states have similar laws. Will Amazon just give up on affiliates?\nreply\n\t\n\nnostromo 5 days ago | link\n\nRemember, there are five states that do not charge sales tax, so they have no incentive to fight Amazon. There are another five states where Amazon already collects sales tax. So, that's 10 states that have no reason to pass similar laws.\nAnd of course, affiliates that make a sizable amount of money will probably not move, but will instead simply incorporate in one of those states (one of which happens to be Delaware).\nreply\n\t\n\njonkelly 5 days ago | link\n\nExactly. There are plenty of choices of jurisdiction for the bigger affiliates that matter to Amazon. This is really just a \"nuke the mom and pop blogger\" effect.\nreply\n\t\n\n_delirium 4 days ago | link\n\nCan you really just change the place of incorporation? I thought to avoid a determination (under these laws) that Amazon itself has a physical presence in a state via its affiliates, Amazon was refusing to accept affiliates who had any physical presence in the states in question. As I read it, if your corporation is incorporated in Delaware but has its offices in Connecticut, you're no longer eligible for the affiliate program.\nEnforcement might be another matter; seems like there's a good chance that you won't get flagged if you give Amazon a Delaware mailing address, even if you're (as a company or individual) resident elsewhere. But you don't even need to incorporate to do that.\nreply\n\t\n\ngscott 5 days ago | link\n\nWith the widespread adoption of the Google Content network for advertising I am wondering if affiliate links are needed because they can just show up in the Google ad and the advertiser gets paid for clicks. The previous affiliate can still make a page about a product but let Google figure out the links to show.\nreply\n\t\n\nchopsueyar 5 days ago | link\n\nThink of what happens when all US states legalize medicinal marijuana?\nIt is still illegal at the federal level.\nHow much money can a state afford to spend on federal litigation against a team of Amazon's lawyers?\nreply\n\t\n\nraganwald 5 days ago | link\n\nMy guess would be that the \"income\" from affiliates is immaterial, but the states have a different problem: They do apply the tax to other companies with a \"presence\" in the state, and if they didn't attempt to collect from Amazon, they would face a challenge from companies that maintain a less tenuous presence in the state such as physical stores.\nINAL, but if CT didn't try to collect the tax from Amazon, I would expect Barnes and Noble to argue that bn.com sales should be tax exempt even though they have a brick and mortar store in Glastonbury, CT.\nreply\n\t\n\nchopsueyar 5 days ago | link\n\nExcept the whole argument comes down to physical presence.\nNow, if Barnes and Noble had a caravan of mobile RV bookstores registered in a different state that came into CT to sell books, B&amp;N would have a better argument.\nreply\n\t\n\netherael 4 days ago | link\n\nWe hear you have money, we'd like it.\nreply\n\t\n\nhvs 5 days ago | link\n\nIllinois did something similar with what it called the \"Mainstreet Fairness Act\" in March [1]. Illinois, as you may know, was rated as the worst state in the country for debt last year [2]. These states would rather chase after more revenues (and chase companies out of their states in the process) rather than have to do the hard work of actually cutting spending.\n[1] http://articles.chicagotribune.com/2011-03-10/business/ct-bi...\n[2] http://www.forbes.com/lists/2010/44/debt-10_Global-Debt-Cris...\nreply\n\t\n\nraganwald 5 days ago | link\n\nThere are two orthogonal questions to ask:\n1. How much should states tax their citizens?\nand:\n2. Given a certain amount of tax to collect, what is/are the best mechanism(s)?\nYou seem to be talking to point number one. Eliminating sales tax or collecting it from fewer transactions would accomplish that, but so would applying the law exactly as CT applies it but lowering the percentage collected on each transaction. For that reason, I find it confusing to look at a situation like this and slide into arguing about cutting spending. If they should tax people less, fine, but that's orthogonal to the question of how they tax people and the consequences of the tax mechanisms they choose to employ.\nreply\n\t\n\njwhite 4 days ago | link\n\nThe phrase \"Mainstreet Fairness Act\" sounds like something directly out of Atlas Shrugged.\nreply\n\t\n\nfleitz 4 days ago | link\n\nWhy is cutting spending hard work? You just write a lower number on the budget, voila, less gets spent. I mean the only thing you really have to do to cut spending is for a Governor to refuse to sign the budget, spending gets cut automatically. In my opinion cutting spending is less work.\nreply\n\t\n\n[deleted]\n\t\n\ncheez 5 days ago | link\n\nThink about the children.\nreply\n\t\n\nirons 5 days ago | link\n\nI've heard about Amazon pulling this tactic in NY and other states that forced them to start collecting sales tax, but that is one breathtakingly petulant letter. What exactly is the constitutional issue they're alluding to?\n(I live in Washington state, where Amazon has always collected sales tax, so personally it seems like a non-issue.)\nreply\n\t\n\nhvs 5 days ago | link\n\nThe issue is collecting sales tax for items bought outside of their state. Interstate commerce is a federal jurisdiction. If they collect sales tax in Washington state, it's because they have an office there.\nreply\n\t\n\nlotharbot 4 days ago | link\n\nIn Amazon's case, they have a lot of offices in Washington state, including their headquarters. They have always collected WA sales tax.\nreply\n\t\n\nirons 4 days ago | link\n\nThat's not a constitutional argument for Amazon's position.\nYou're obligated to pay sales tax on items purchased out of state, by annual remittance. Almost nobody does it, but if you're ever audited (by your state), they can ding you if you don't. Amazon is plenty big enough to be specifically called out by cash-strapped state governments for, effectively, encouraging people to evade the law.\nreply\n\t\n\n_delirium 5 days ago | link\n\nA more accurate headline would be: Amazon.com kills affiliate marketing in Connecticut, due to dissatisfaction with new Connecticut law.\nreply\n\t\n\njonknee 5 days ago | link\n\nIf a state enacts a law that will make a business lose money without changing polices, it's not the company's fault if they change policies. Amazon's pullout was a completely known outcome of the law well before it was put into place.\nreply\n\t\n\n_delirium 5 days ago | link\n\nSure, companies can do whatever they wish (within the law) in response to any factors they choose to take into account. But I think it's still Amazon making the decision here; from the headline, I had thought that Connecticut banned affiliate programs or something. I'd similarly say that Boeing's move from Seattle to Chicago, partly due to tax policies, was a decision made by Boeing, not by Seattle.\nreply\n\t\n\njonknee 5 days ago | link\n\nBy that logic, the state can never be to blamed when making decisions that affect businesses. This wasn't an unintended consequence--every time a similar law has passed Amazon has acted identically on the exact day that it begins.\nreply\n\t\n\n_delirium 5 days ago | link\n\nI'm not saying the state can't be blamed, either, just that the headline is misleading. My comment was not a political one, but one about headlines that editorialize at the expense of clarity. If you want to emphasize the state's culpability, that can still be done with somethting like, \"Connecticut tax change causes Amazon to pull affiliate program\".\nI do think Amazon's action was predictable, but it's still Amazon's action. I'm also not quite sure it was mandated by the decision; I suspect Amazon could still turn a profit even by retaining its CT affiliate program, but with the current landscape (% of states that do versus don't have such laws) it was a better business decision to pull out. Amazon is also probably looking at it as a strategic move to put pressure on other states, rather than considering the CT business case in isolation.\nreply\n\t\n\nchopsueyar 5 days ago | link\n\nNo, then Amazon would have set the precedent that it is acceptable for Amazon to collect sales tax for every muncipality it does business with.\nWhy should it be any different than a mail-order catalog?\nCoordinating the payout of sales tax for every state and city in the US? Coding and compliance nightmare.\nIf the Connecticut legislature legalized slavery (in direct conflict with federal law), and businesses were disgusted at the idea and refused to continue to conduct business in Connecticut, would you still argue the same reasoning?\nPS\n\"Connecticut Killed Affiliate Marketing with Amazon.com\" seems clear to most of us.\nreply\n\t\n\n_delirium 5 days ago | link\n\nWhat reasoning am I arguing?\nI'm making two comments in the comment you're responding to:\n1. The article headline is a bit misleading, and should've said something like: Amazon pulls out of CT due to a tax-law change. Even, Terrible CT Tax-Law Change Drives Amazon Out, or something. As it's written, I thought that CT had banned affiliate programs; in a rush to editorialize, the headline author sacrificed clarity about agency. Of course, maybe everyone reading is already following the saga, so I was the only briefly confused person, but nonetheless it seemed like an easy problem to avoid.\n2. Amazon is probably pulling out in part due to, as you say, a feeling that this would set a negative precedent for their business, not solely due to the CT case taken in isolation.\nYour slavery example seems off the mark; Amazon is not taking action due to moral opposition to CT's tax policy, but because it's bad for their business. They happily do business in states with all sorts of unethical laws without complaining about them, as long as those laws don't impact their profits.\nreply\n\t\n\nchopsueyar 5 days ago | link\n\nMy 'slavery example' was actually a question I was asking you, not an example.\nNot sure why you got confused with the headline or this discussion.\nreply\n\t\n\npvodsevhcm 4 days ago | link\n\nYou really don't understand _delirium's point, or are you just playing stupid?\nreply\n\t\n\nchopsueyar 4 days ago | link\n\nPlease enlighten me.\nreply\n\t\n\nInclinedPlane 5 days ago | link\n\nMy reading is that Amazon would have had to charge sales tax on the full price of the item if the buyer and the referer were both in Connecticut. It doesn't take an ecommerce expert to figure out that sort of model is not sustainable.\nEdit: on reread it appears that the law was written such that amazon would have to collect sales tax on all sales to CT buyers because the affiliates counted as a \"local presence\".\nreply\n\t\n\nearl 4 days ago | link\n\nNot sustainable? All sorts of businesses collect sales taxes in CT -- every establishment you enter. And despite the (to you) obvious non sustainability of this, somehow the businesses are still there. Which could lead you to believe that collecting sales tax is, in fact, sustainable.\nreply\n\t\n\nInclinedPlane 4 days ago | link\n\nIf the only reason to pay sales tax is the existence of CT affiliates, then the affiliate system is not sustainable.\nWhy not require all internet companies to pay local sales tax, regardless of where they are located? That seems to be the argument here.\nreply\n\t\n\nrwg 4 days ago | link\n\nSome food for thought regarding Amazon and sales tax:\nAmazon has plans to build and operate a distribution warehouse near Columbia, South Carolina. State and local leaders rolled out the red carpet for Amazon -- a free building site, property tax cuts, employment tax credits, and the repeal of a county law prohibiting Sunday morning sales. But Amazon also wanted a five year exemption on collecting South Carolina sales tax. After all, they would have a physical presence in the state, and there's absolutely no ambiguity about whether or not they'd have to collect sales tax at that point.\nWhen the state legislature voted \"no\" on the sales tax exemption, Amazon immediately stopped construction on the warehouse and took down all of the job listings for that location. There was a huge uproar, with supporters of the exemption accusing legislators of siding with \"special interests.\" (Where \"special interests\" apparently means \"every other business with a physical presence in South Carolina that doesn't get a sales tax exemption.\")\nAmazon won this game of chicken, however. Faced with massive voter backlash, the state legislature flinched and voted 90-14 for a new deal that would exempt Amazon from collecting South Carolina sales tax until January 2016. The bill became law earlier this week.\nhttp://www.therepublic.com/view/story/CPT-AMAZON-SCAROLINA_5...\nhttp://seattletimes.nwsource.com/html/businesstechnology/201...\nreply\n\t\n\nkarlkrantz 5 days ago | link\n\nIt wasn't just Amazon, but also sites owned by Amazon, including Audible. Amazon and Audible were my two sources of revenue for my site thestartupdaily.com\nYesterday both accounts were closed with no advanced notice and my business model is effectively broken. While I support Amazon for taking a stand, I'm angry at Amazon for not giving some sort of warning to affiliates. It also seems like they wasted a good opportunity to get people who are most passionate about the issue to make some noise for them. The could have sent emails to affiliates as the issue was unfolding, and instead of the short and rather unfriendly letter to affiliates saying \"your contract has been terminated\". They should have used that notice to give people more information and phone numbers and other contact details about who is behind this.\nSeems to me like big chain stores buying protectionist legislation and selling it to voters as \"protecting small business\", while in reality they are protecting yesterday's dinosaurs and screwing forward thinking Internet based businesses.\nreply\n\t\n\ndangrossman 5 days ago | link\n\nYou've had years of notice. Amazon did exactly the same thing with Illinois, Colorado, Hawaii, North Carolina... Amazon has publicly said it would do the same thing in Connecticut for months.\nreply\n\t\n\nkarlkrantz 5 days ago | link\n\nOf course I knew it was a possibility, but my point is that \"saying publicly\" in press releases or court rooms is a lot different from having a conversation with your customers or partners.\nTelling people that the affiliate program will be closed to them in 30 days would have been a lot nicer than telling people that their income stops effective immediately.\nreply\n\t\n\ndangrossman 4 days ago | link\n\nThen they'd have to collect sales tax from Connecticut customers for 30 days. They'd piss off a lot more people than their affiliates, including their customers for suddenly collecting taxes they didn't used to collect, to their shareholders for creating mass customer confusion just to be nice to affiliates.\nreply\n\t\n\n18pfsmt 4 days ago | link\n\nCouldn't they simply tell their affiliates they were considering terminating the program as early as possible? I have to believe they were aware of the situation, and monitoring patiently as it unfolded. For example, I knew it was imminent in Colorado. yet I've never collected a penny in affiliate revenue. It was debated quite a bit before it passed.\nEdit: I guess what I'm asking is: am I missing something, or is there some reason why the affiliates couldn't have been alerted to the possibility earlier? Or, does alerting them at all require Amazon to pay sales tax?\nreply\n\t\n\ndangrossman 4 days ago | link\n\nI can't disagree that sending out an e-mail would've been nice, though it'd also not be good to stir up all the affiliates when it wasn't yet known if the bill would be made law. That aside, anyone who made a significant portion of their income as an affiliate should've been aware of the impending bill for months and have been watching whether it would pass at the same time Amazon was watching it. It's not an Amazon bill, it affects every affiliate in the state for all companies... Overstock is another big company that severed its affiliate relationships with everyone in the state when the bill passed.\nreply\n\t\n\nVivtek 5 days ago | link\n\nIs anybody keeping track of which states this applies to? I know Illinois was (one of?) the first, and now obviously CT as well. Which states remain?\nreply\n\t\n\njonknee 5 days ago | link\n\nAmazon is (obviously).\nhttps://affiliate-program.amazon.com/gp/associates/agreement\n&gt; In addition, if at any time following your enrollment in the Program you become a resident of Colorado, Illinois, North Carolina, Rhode Island, or Connecticut, you will become ineligible to participate in the Program, and this Operating Agreement will automatically terminate, on the date you establish residency in that state.\nreply\n\t\n\nunshift 5 days ago | link\n\nThis same thing happened in Colorado a few months back, and I think California has a similar bill on the horizon\nreply\n\t\n\nzaidf 5 days ago | link\n\nNorth Carolina, I think, is one of them.\nreply\n\t\n\ndavidw 5 days ago | link\n\n!Oregon - there is no sales tax there.\nreply\n\t\n\nolefoo 5 days ago | link\n\nSsshh; Portland is already overrun with people who are online marketing experts... don't bring more of them.\nreply\n\t\n\nsixtofour 5 days ago | link\n\nColorado\nreply\n\t\n\nminouye 4 days ago | link\n\nFor anyone affected by this legislation, I'd encourage them to get involved with the Performance Marketing Association. They seem to be the main group (excluding merchants like Amazon) that is actively lobbying against such legislation. In fact, they recently filed suit against the Illinois Dept. of Revenue: http://performancemarketingassociation.com/pma-vs-state-of-i...\nreply\n\t\n\nmikiem 4 days ago | link\n\nRetailers, on-line or otherwise, have to pay sales tax on sales within a state where they have a physical presence. That's the way it is. The problem in this case is that CT wants sales tax on a sale that was referred by an affiliate... Even when the retailer (Amazon) and the customer are both not located in CT. In states that charge sales tax, most (all?) charge \"use tax\" of the same amount as sales tax, on items shipped to them from an out-of-state retailer that did not charge sales tax. Retailer customers almost never pay it, probably don't know it exists. Sates usually don't go after the tax for consumers, but they do go after businesses for it (at least they do in CA) presumably because the amount of tax is potentially much higher.\nIn other words, technically, you have to pay sales tax when you buy something, and if you don't then you're supposed to pay the equivalent amount in \"use tax\". In this case, CT is trying to wedge in there and get tax when neither the buyer or the retailer are in CT.\nIt's not a business killer for Amazon to charge and pay the taxes for every state. Many on-line retailers do. There is software and subscription services to keep billing systems up to date with current tax tables. But, who pays sales/use tax on what when a guy in CA buys something from Amazon that was referred by a CT affiliate? What about when someone in a state where Amazon has no physical presence buys from Amazon through a CT affiliate? My brain hurts and I suspect someone's getting screwed.\nreply\n\t\n\nmtumbrel 4 days ago | link\n\nI think the point is that very few CT residents actually pay the use tax, and the situation they're trying to catch is where the buyer is in CT.\nIf Amazon had a retail presence, it would be collecting that tax and forwarding it to CT, and this law was an attempt to define affiliates as a retail presence.\nI agree that it's not a \"business killer\", but lower prices drive sales, and that's why they've fought so hard to collect nothing.\nFrankly I found the one-sided slant of the article a little nauseating. Couldn't you as easily say that Amazon has been abetting tax evasion for years? Who moved your cheese? Was it the Connecticut legislature or Amazon? Well, yes.\nConnecticut wants that 6%. Not sometimes. Always. So they passed a law. Given a choice between collecting a 6% tax and hanging out their long-time affiliates partners to dry, Amazon picked the latter.\nEven though I live in a high-tax state with a similar use tax requirement, I don't think Amazon's position is reasonable. Use tax compliance rates are generally pitiful. Both state and the federal Internet Tax Freedom Act allow for this kind of use tax. Local sales tax collection is not optional.\nThe goods in question do not magically teleport into the homes of Connecticut residents. They come over state roads, often carried by state residents in big trucks. If they weren't bought over the internet, they'd be bought at stores where residents would be paying 6% or more.\nreply\n\t\n\nandrewpi 5 days ago | link\n\nAnyone know the status of Amazon's litigation with New York State? Amazon still charges sales tax to NY addresses.\nreply\n\t\n\nusername98 4 days ago | link\n\nI don't get why Amazon didn't cut off all its New York affiliates yet.\nreply\n\t\n\nforgetcolor 5 days ago | link\n\nIL also mandated a boycott of Amazon for any state procurement. IOW no state employee can buy goods from Amazon using state purchasing methods (refardless of aource of funds). Of course it's not like any sales taxes were paid by tne state since they're exempt.\nreply\n\t\n\nMediaTrustpete 4 days ago | link\n\nThe PMA Performance Marketing Association just filed the first law suit to fight the Affiliate Nexus Tax. this is the first action of its kind from the internet marketing industry fighting back..\nPerformance Marketing Association Sues State of Illinois over Affiliate Nexus Law Lawsuit Aims to Protect 9,000 Illinois Small Businesses; State Will Lose Estimated $22 Million in Income Taxes if Law Takes Effect http://performancemarketingassociation.com/pma-vs-state-of-i...\nworth while checking out and helping support and spread this info...\nreply\n\t\n\nbwb 4 days ago | link\n\nArkansas did too earlier this week.\nreply",
        "date": "2011-06-16 17:26:54",
        "timestamp": 1308259614
      },
      {
        "id": 178,
        "parent_id": 152,
        "author_ip": "75.67.116.140",
        "author": "Carl Pet",
        "email": "carlpet@comcast.net",
        "content": "Sounds like a plan!",
        "date": "2011-06-19 09:28:15",
        "timestamp": 1308490095
      },
      {
        "id": 183,
        "parent_id": 0,
        "author_ip": "75.17.98.183",
        "author": "Delian Naydenov",
        "email": "delian@didiooo.com",
        "content": "It makes sense as the Amazon.com program should be similar to Google AdSense. When you display an ad you receive revenue not when someone buys the product but when someone views the ad. If you receive a % of the sold items then you are more of a reseller than an advertiser.",
        "date": "2011-06-21 22:36:01",
        "timestamp": 1308710161
      },
      {
        "id": 531,
        "parent_id": 0,
        "author_ip": "12.111.50.7",
        "author": "Cindy L",
        "email": "cindy.lablanc@gmail.com",
        "content": "CT is CRAZY! I can see a good debate on whether someone making a purchase on the internet should be state taxes to their own state as the product is being delivered to them. I DO NOT see how a consumer should have to pay taxes to a state where the product that they ordered neither came from that state, nor do they (the consumer) live in that state. (The only  person involved in the transaction is the affiliate who is living in the state that wants the taxes (CT, in this case). The affiliate does not supply the product but only offers a link telling visitors who visit his/her website about the product as a recommendation. And they don't make a whole lot for that recommendation. (But it is an income source however small that supports people (and these people DO pay taxes already on the revenue that they earn thru their affiliate links). CT is money-hungry and very short-sighted and showing way too much greed.",
        "date": "2011-08-15 12:55:42",
        "timestamp": 1313427342
      },
      {
        "id": 3072,
        "parent_id": 0,
        "author_ip": "99.41.224.80",
        "author": "Johnny B",
        "email": "pondybolk@gmail.com",
        "content": "I am a CT resident UNFORTUNATELY!!  I set up a wishlist from Amazon to put on my site when I come to find out I can't.  Then I got to this website.  SERIOUSLY??  We are looking to get the heck out of this overpriced, big government BS.  Everywhere I turn my hands are tied and my wallet is empty.  CT Sucks!",
        "date": "2012-06-17 17:56:07",
        "timestamp": 1339970167
      },
      {
        "id": 15777,
        "parent_id": 0,
        "author_ip": "127.0.0.1",
        "author": "marycontrary",
        "email": "jqb230@gmail.com",
        "content": "I am curious - is this still in effect? (Dec. 2012?)\nWondering if any legislation since has changed this. \n\nI agree Amazon is big and can throw its weight around but I think it is essential to see the lack of sense in this Connecticut law. \n\nAmazon aside. \n\nThe benefit to local retailers doesn't pan out, if  protecting them was the intent of this law. \n\nConsumers buy online or they don't. Affiliate taxing doesn't affect that behavior at all. \n\nConsumers buying online from Connecticut may pay Connecticut sales taxes, depending. It should stay that way. Consumers from Pennsylvania who arrive at the merchant by clicking on merchant's ad on a Connecticut web page may have to pay  Penn. Sales tax on their purchase. End of story. \n\nWhat Connecticut did was meaningful in one way and one way only, and the meaning is a negative one - there is no upside to  this law -- it hit the little person, the  person with a webpage who is trying to make a little money on the side by hosting an ad for an online merchant, such as Amazon. \n\nAnd what then does Amazon and other merchants do with a customer in Penn, say, and the affiliate in Conn? Pay two taxes? One to Connecticut, one to Penn? \n\nIs this correct? \n\nDo people really feel that advertising should be treated that way? \n\nSay I live in Texas and take a road trip to Connecticut and see a billboard on I-91 for L.L. Bean.  I write down the phone number, which is LL Bean's Rockport, ME phone number. When I get home to Dallas, I call the number and buy a parka, which is shipped to me from Rockport Me. I disclose to LL Bean that I am calling because I saw a billboard in Connecticut. Connecticut, therefore, taxes the sale because the ad was on a billboard in Connecticut? \n\nDo I understand this correctly that this is analogous? \n\nAnd this benefits whom? It benefits nobody. \n\nIt hurts ad hosts and the merchant and nobody else. it doesn't benefit local retailers and doesn't even benefit big box stores directly. \n\nWhat it is is a malicious political hit and nothing else - meant to  harm, only harm, and is not for any particular good. It is a bad law and an essentially corrupt law. \n\nSo, is there anything I am missing? Is there information I don't have that would justify this law?",
        "date": "2012-12-29 17:13:53",
        "timestamp": 1356819233
      },
      {
        "id": 15778,
        "parent_id": 0,
        "author_ip": "127.0.0.1",
        "author": "marycontrary",
        "email": "jqb230@gmail.com",
        "content": "I agree with Cyndy that it mostly comes down to greed. It is tempting to try to see other motives in Connecticut - complicated revenue problems in search of solutions. Enacting this one even though it is probably an error. \n\nBut that is giving Connecticut way to much credit. \n\nConnecticut has problems like everywhere else, but every place is different and Connecticut has flaws that are particularly Connecticut. Among the most gaping flaws about which Connecticut remains in deep denial is its unceasing greed, selfishness and contempt for each other. \n\nBecause it thinks of itself as such a smart and wealthy state, it never notices how stupid and oppressive it is. \n\nIt has a bloated bureaucracy that is really expensive and less effective than in leaner less wealthy states where agencies actually get things done. Connecticut government more than most places i know really does exists to feed itself. \n\nIts bureaucracy is  particularly nasty and neurotic too so that state services often aren't delivered to constituents by the over-funded state agencies mandated to deliver them. If every state in the nation functioned like Connecticut does,  the nation would collapse in a week. \n\nConnecticut is basically a big loser, an example to other states of what not to do and not to be. This reality is disguised by the fact that the state and many of its residents continue to rake in the dough because they are in the North East, near NY, and because of their long history in the Northeast, cradle of US Power and Money. That's it -- there is no other reason. That's all its got and all it does is spend that down, grow desperate, get more abusive, and drive people and businesses away.",
        "date": "2012-12-29 17:28:51",
        "timestamp": 1356820131
      },
      {
        "id": 15954,
        "parent_id": 0,
        "author_ip": "127.0.0.1",
        "author": "Dr. Mike Williams",
        "email": "drmlwilliams@outlook.com",
        "content": "Let's look at this from a technology perspective. When I have Amazon ads on my site in New Mexico and a customer clicks on the ad, they are literally redirected to the Amazon site where ever it is located with a cookie saying where they came from. All my site did was refer them to the Amazon site. The actual transaction takes place on the Amazon site. Hence the sale did not take place on my site.\n\nIf this is taxable, then why can't the state demand I pay sales tax if I verbally tell someone to go to the Amazon Site and place an order? The only difference is that if I refer them electronically, Amazon knows who sent them. If I refer them verbally, then Amazon does not know who referred them unless they call Amazon on the phone and say, \"Hey, Mike sent me.\"\n\nFrom a technology perspective, Amazon is right. These outside states are trying to collect sales taxes from Amazon on sales that are not made in their state.",
        "date": "2013-07-03 12:45:02",
        "timestamp": 1372869902
      }
    ],
    "metadata": {
      "_wp_old_slug": "593",
      "_edit_last": "1"
    }
  },
  "att-uverse-residential-gateway-broadband-led-flashes-red-intermittently": {
    "name": "att-uverse-residential-gateway-broadband-led-flashes-red-intermittently",
    "id": "612",
    "link": "http://russell2.ballestrini.net/att-uverse-residential-gateway-broadband-led-flashes-red-intermittently/",
    "title": "ATT Uverse Residential Gateway Broadband LED flashes red intermittently",
    "content": "<strong>ATT Uverse Residential Gateway Broadband LED flashes red intermittently</strong>\n\nI have struggled with my uverse Internet and TV service randomly and intermittently shutting off for the last 3 months.  When this occurs the Residential Gateway's broadband LED flashes red.  \n\nAlso I get Uverse DSL link errors:\n\n<center><img src=\"/wp-content/uploads/2011/06/uverse-dsl-link-errors.jpg\" alt=\"uverse link errors\" title=\"uverse-dsl-link-errors\" width=\"476\" height=\"438\" class=\"size-full wp-image-619\"/></center>\n\nWhen I 'live support chatted' with the uverse technical support they claimed I had bridge tap on my lines, and that my lines had other 'issues'.\n\nThey had to dispatch 3 technicians before one of them figured out the problem.  It turns out I had a faulty 'voice/data' splitter.\n\nI snapped a picture of the demarcation box before and after he replaced the faulty splitter.\n\n<center><img src=\"/wp-content/uploads/2011/06/old-splitter.jpg\" alt=\"old uverse splitter\" title=\"old-splitter\" width=\"272\" height=\"400\" class=\"size-full wp-image-618\" /><img src=\"/wp-content/uploads/2011/06/new-splitter.jpg\" alt=\"new uverse splitter\" title=\"new-splitter\" width=\"275\" height=\"400\" class=\"size-full wp-image-617\" /></center>\n\n<center><em>Old Uverse filter on the left, New filter on the right</em><center>\n\n<strong>So far I have had no Uverse DSL Link Errors!</strong>",
    "date": "2011-06-18 10:08:17",
    "timestamp": 1308406097,
    "comments": [
      {
        "id": 16100,
        "parent_id": 0,
        "author_ip": "127.0.0.1",
        "author": "Darci D.",
        "email": "darci.demeio89@gmail.com",
        "content": "We have had U Verse for three years, the three services for one. Last week, all three services went down, broadband light intermittently flashed red and green. Tech said an upgrade to services blew out our fibers. He gave us four instead of two fibers. Services went out again two days ago. Tech replaced the RG and iNid downstairs and I think the one outside. Service went out again this morning. It seems to want to hook up, but it just can't. Any ideas??",
        "date": "2013-08-15 21:47:40",
        "timestamp": 1376617660
      }
    ],
    "metadata": {}
  },
  "google-bot-attempts-to-crawl-shortest-urls-first": {
    "name": "google-bot-attempts-to-crawl-shortest-urls-first",
    "id": "673",
    "link": "http://russell2.ballestrini.net/google-bot-attempts-to-crawl-shortest-urls-first/",
    "title": "Google Bot Attempts to Crawl Shortest Urls First",
    "content": "<p>\n<strong>Recently I built <a href=\"http://school.yohdah.com\">http://school.yohdah.com</a> a Python, Pyramid, and mongoDB project during the last couple weekends. </strong>\n</p>\n\n<p>\n<a href=\"http://school.yohdah.com\"><img src=\"/wp-content/uploads/2011/06/us-public-schools1.png\" alt=\"\" title=\"us-public-schools\" width=\"100\" height=\"100\" class=\"alignright size-full wp-image-700\" /></a>\n</p>\n\n<p>\nThe site features a directory style navigation of nearly every public school in the US.  We have 61 state pages, approximately 19,000 city pages, and over 103,000 school pages.\n</p>\n\n<p>\nIt seems the Google Bots have noticed <a href=\"http://school.yohdah.com\">school.yohdah.com</a> and started crawling the site.  Since the initial crawl I started reviewing a sample of the sites apache logs in an attempt to track the bot's activity.  After a few minutes of viewing the logs, I locked onto a pattern; Google Bot's algorithm appears to crawl the short URLs first!\n</p>\n\n<p>\n<strong>PersonalCompute (a user) attached a graph of the fetched URL lengths here:\n</p>\n\n<a href=\"/wp-content/uploads/2011/06/school.yohdah.com_.graph_.png\"><img src=\"/wp-content/uploads/2011/06/school.yohdah.com_.graph_.png\" alt=\"school.yohdah.com.graph\" title=\"graph of google bot crawl url lengths\" width=\"11962\" height=\"400\" class=\"aligncenter size-full wp-image-695\" /></a></strong>\n\n<p>\n<strong>I have attached a zip containing the apache google bot crawl logs here: <a href='/wp-content/uploads/2011/06/access-school.yohdah.log_.zip'>access-school.yohdah.log.zip</a></strong>\n</p>\n\n<p>\nI found the pattern by opening the file in vim and scrolling very quickly down.  You will notice the log lines will grow slowly to the right, as the urls being fetched increase by one character.\n</p>\n\n<p>\n<strong>Why does Google do this?  Does anyone have speculation as to what this means? </strong>\n</p>",
    "date": "2011-06-25 09:37:29",
    "timestamp": 1309009049,
    "comments": [
      {
        "id": 199,
        "parent_id": 0,
        "author_ip": "180.183.184.190",
        "author": "Joe",
        "email": "joe@motionmethod.com",
        "content": "It's interesting - and I'm going through the data. \n\nBUT - does it have any relevance regarding SEO?",
        "date": "2011-06-25 10:27:22",
        "timestamp": 1309012042
      },
      {
        "id": 200,
        "parent_id": 0,
        "author_ip": "184.152.30.165",
        "author": "hojboj",
        "email": "aaadsf@gmail.com",
        "content": "You gonna crawl them randomly? You gonna crawl them by longest url first? Alphabetically? What difference does it make?",
        "date": "2011-06-25 10:36:31",
        "timestamp": 1309012591
      },
      {
        "id": 201,
        "parent_id": 0,
        "author_ip": "24.201.36.64",
        "author": "Guy Gervais",
        "email": "guy.gervais.1@gmail.com",
        "content": "I'd speculate that it's because most site will try to make their most interesting content accessible via short, easy to remember URLs. So by crawling those first, Google gets the best parts rapidly.",
        "date": "2011-06-25 10:38:30",
        "timestamp": 1309012710
      },
      {
        "id": 202,
        "parent_id": 0,
        "author_ip": "84.237.142.129",
        "author": "Tenticle",
        "email": "japan@japan.jp",
        "content": "MongoDB is Web Scale.",
        "date": "2011-06-25 11:12:54",
        "timestamp": 1309014774
      },
      {
        "id": 203,
        "parent_id": 0,
        "author_ip": "88.76.225.96",
        "author": "Dominik",
        "email": "dbelca@gmail.com",
        "content": "I think Google tries to fetch any page starting from the homepage down the tree of pages. But how should Google know how deep an url is in your page tree?\n\nIt seems pretty pragmatic to me to use the length of the url as an indicator. The logic could possibly be that with every added directory /a/b/c in the url you hit a page deeper down the tree. \n\nThis is only a rule of thumb but makes perfectly sense, as google fetches only a limited number of url per domain and the deeper a page lies on yout domain the less important its content should be.\n\nWhat do you think?",
        "date": "2011-06-25 11:17:15",
        "timestamp": 1309015035
      },
      {
        "id": 204,
        "parent_id": 0,
        "author_ip": "190.149.60.151",
        "author": "JK",
        "email": "Jkreimmer@gmail.com",
        "content": "seems pretty obvious because this way let the bot find the entire structure of a site, for ex if crawl domain.com/longestdirectory/dir/dir2 it could miss some \"folder\" before dir2",
        "date": "2011-06-25 11:17:59",
        "timestamp": 1309015079
      },
      {
        "id": 205,
        "parent_id": 0,
        "author_ip": "74.45.35.107",
        "author": "Anonymous",
        "email": "fakeemail@fake.com",
        "content": "Graphed the googlebot requests in the log: http://i.imgur.com/uMoUT.png",
        "date": "2011-06-25 11:41:23",
        "timestamp": 1309016483
      },
      {
        "id": 206,
        "parent_id": 205,
        "author_ip": "192.168.1.21",
        "author": "Russell Ballestrini",
        "email": "russell.ballestrini@gmail.com",
        "content": "Wow! thanks, I'm going to add that to the blog up top!",
        "date": "2011-06-25 11:53:38",
        "timestamp": 1309017218
      },
      {
        "id": 207,
        "parent_id": 0,
        "author_ip": "71.227.254.214",
        "author": "Ben Krull",
        "email": "benkrull3755@gmail.com",
        "content": "In my experience, Google follows this crawling pattern when all links are of equal value and there are too many links on a page to crawl them all (e.g. most of your state pages). I have several local directory style sites so I've seen this pattern many times.\n\nJust curious, where did you get the public school information for this site? I'm always looking for good quality local data sources.",
        "date": "2011-06-25 12:01:50",
        "timestamp": 1309017710
      },
      {
        "id": 208,
        "parent_id": 0,
        "author_ip": "24.185.90.126",
        "author": "Anthony Brunetti",
        "email": "anthonydase@gmail.com",
        "content": "Hi Russell,\n\nInteresting.\n\nI believe It's because of your site's internal links &amp; hierarchy:\n\nhome - links to:\nhome/state - links to:\nhome/state/city - links to:\nhome/state/city/school-name\n\nThe spider will follow all of the links on each level before going deeper, following your hierarchy. I imagine your chart looks flat because all states have about the same URL length. To test this theory, find a city/school URL that is shorter then a /city and see if that gets crawled first. You could also test the internal link theory by linking to a long URL on your homepage, and watch the spider hit that first.\n\nP.S.\n\nI have a client who does lead generation for education, I run their SEO program, and I could make use of this set-up. I'd also be happy to share some SEO advice. If you're interested, get in touch with me: anthony@42metrics.com.\n\nHave a great day.",
        "date": "2011-06-25 14:47:15",
        "timestamp": 1309027635
      },
      {
        "id": 209,
        "parent_id": 0,
        "author_ip": "81.157.253.44",
        "author": "aaaaaaa",
        "email": "a@a.com",
        "content": "Given the graph, it is clear to see it is not getting the entire set of urls first and then sorting them, so it is not actually attempting to crawl shortest urls first. It is just crawling based on the structure of the website, which happens to be that further depth links have a longer url, and so it is a breadth-first search as expected. This type of search is completely usual for a crawler, and how I would write one too. The observation that it somehow is crawling the shortest urls first is mistaken, it is just following the structure of the website as you can see in the graph.",
        "date": "2011-06-25 16:02:03",
        "timestamp": 1309032123
      },
      {
        "id": 210,
        "parent_id": 0,
        "author_ip": "201.201.84.146",
        "author": "Carlos Sol\u00eds",
        "email": "csolisr@gmail.com",
        "content": "If I built a crawler myself, of course I'd start with the shortest URLs first, just to build a simpler iterator (e.g. from \"z.com\" I'd pass to \"aa.com\" and so on).",
        "date": "2011-06-25 17:32:15",
        "timestamp": 1309037535
      },
      {
        "id": 211,
        "parent_id": 0,
        "author_ip": "81.159.96.212",
        "author": "Dave Stopher",
        "email": "dchstopher@googlemail.com",
        "content": "This is some really good info about the google bot.\n\nGoogle has always said that it prefers shorter URLS!!\n\nDave",
        "date": "2011-06-25 17:51:13",
        "timestamp": 1309038673
      },
      {
        "id": 212,
        "parent_id": 0,
        "author_ip": "98.232.62.210",
        "author": "Ian",
        "email": "ian@portent.com",
        "content": "I've suspected for a long time that Google uses URL length as one 'guess' at site structure. So it might assume shorter URLs are at the 'top' of the hierarchy, and therefore more important.\n\nThis is by far the best evidence of this I've seen though. Nice work Russell!!!",
        "date": "2011-06-25 19:05:36",
        "timestamp": 1309043136
      },
      {
        "id": 213,
        "parent_id": 0,
        "author_ip": "66.138.74.160",
        "author": "Keith Brown",
        "email": "kbrown@mavs.uta.edu",
        "content": "Site structure could be one reason Googlebot crawls shorter URL's, another would likely be a measure of quality. Often dynamic URL's from search strings and deep navigation are not as important as cleaner SEO friendly URL. Google's click tracking studies have shown time and time again that CTR drops as URL length increases. Google wants to index the most friendly URL's first obviously.",
        "date": "2011-06-25 22:43:12",
        "timestamp": 1309056192
      },
      {
        "id": 218,
        "parent_id": 0,
        "author_ip": "192.168.1.21",
        "author": "HackerNews",
        "email": "HackerNews@HackerNews.com",
        "content": "JonnieCache 1 day ago | link\n\nThere is probably some highly non-obvious reason that sorting your queue of URLs by length is optimal, which was arrived at after a lot of modelling and testing.\nWe're unlikely to ever know the answer unless someone from google explains it to us.\nreply\n\t\n\nJonnieCache 1 day ago | link\n\nThinking about it more, its probably just a breadth-first search. Duh.\nreply\n\t\n\nfrisco 1 day ago | link\n\nBreadth-first search means crawling all of the links on the page and adding all of the links on the child pages to the queue at once rather than drilling down on one link-path first before moving to the other links on the first page. There's probably some highly non-obvious reason for crawling by url length.\nreply\n\t\n\nxyzzyz 16 hours ago | link\n\nThe thing is, the deeper you are, the longer are the urls, so if you do a breadth first search, you are more likely to visit short urls first.\nreply\n\t\n\nAshleysBrain 1 day ago | link\n\nIt's probably just a short URL is a heuristic for an important site. www.site.com/section is probably more important than www.site.com/section/subsection/detail/page/5/comments. A good move for the crawler - don't get distracted by \"deep\" pages - try and stick to high level ones first.\nEdit: this would also encourage webmasters to use short URLs, which benefits users by being easier to remember, too.\nreply\n\t\n\norijing 20 hours ago | link\n\nHow well-known is this to webmasters? If it's not well-known I cannot see how the second part could be. But the first could very much be true, and is what I would've surmised.\nreply\n\t\n\ncma 1 day ago | link\n\nI don't agree with the last point; seems like it would encourage things like foo.com/1hu83FG2 or lead to excessive abbreviation.\nreply\n\t\n\nDrJokepu 1 day ago | link\n\nIf I remember correctly, your page ranks better in Google if the search terms are in the URL (even more so if they are in the hostname).\nreply\n\t\n\nAshleysBrain 1 day ago | link\n\nWell, it's just a heuristic. Also, would you consider that level of abbreviation for your own site instead of descriptive names? You do want users to be able to remember URLs if at all possible...\nreply\n\t\n\narn 1 day ago | link\n\nI noticed this behavior also when I was following Googlebot's crawl of my old pages after I had done a redesign.\nit's not because of sitemap or because of url structure or because of dynamic content.\nMine were blog articles in the same format. This is how it was crawled:\nsitename.com/year/mo/day/stub\nsitename.com/year/mo/day/stub-one\nsitename.com/year/mo/day/stub-one-two\nsitename.com/year/mo/day/stub-one-two-three\nsitename.com/year/mo/day/stub-one-two-three-four\nreply\n\t\n\norijing 20 hours ago | link\n\nThe question that I have is whether this is a relative behavior (i.e. whether, for a given domain 'domain.com' Google prioritizes domain.com/short-url over domain.com/longer/url.html) or a global one (i.e. prioritizing short.com/url over very-long-domain.com/nested/pages/hierarchy.html, all else equal).\nI can definitely see the local/relative effects being a natural consequence of prioritizing by pagerank, but the global part sounds more like a separate signal.\nDoes anyone have insights?\nreply\n\t\n\nesryl 1 day ago | link\n\nThe site adheres to a strict url structure. /state/city/id/schoolname - entering from the homepage, the only way to crawl the site 1 level at a time would be crawling the shortest urls first. this structure is also emphasised in the breadcrumbs on every page, the shortest urls are also the ones with the most internal links.\nwhy would you crawl the site in any other way?\nreply\n\t\n\nunderdown 1 day ago | link\n\n\"why would you crawl the site any other way?\"\nI could see crawling pages most likely to have changed first as those pages would most likely lead to fresh content.\nreply\n\t\n*\n\n3 points by foxhop 1 day ago | link\n\nIf you look at a particular city page you will notice that the cities are in alphabetical order, however google bot still crawls by length of url...\nreply\n\t\n\npersonalcompute 1 day ago | link\n\nI did a more scientific analysis of the googlebot requests in the provided log (graph! http://i.imgur.com/uMoUT.png) and it definitely looks like it is taking shortest urls first. Anyone else with a large site want to check as well for further data?\nreply\n\t\n*\n\n1 point by foxhop 1 day ago | link\n\nThanks for the graph, I've added it to the blog page\nreply\n\t\n\nmeow 1 day ago | link\n\nThat's probably because short urls usually tend to be static pages while long ones tend to be dynamically generated content.\nreply\n\t\n\npersonalcompute 1 day ago | link\n\nHis entire site that he observed this behavior on is static.\nreply\n\t\n\njules 1 day ago | link\n\nBut Google doesn't know that.\nreply\n\t\n\n_grrr 1 day ago | link\n\nA poor mans PageRank algorithm, assuming nothing else, would assign a higher PageRank to shorter site links on a page. Presumably the crawler visits pages with a higher PR first.\nreply\n\t\n\nTuxPirate 1 day ago | link\n\nPagerank determine crawls rate amongst other things. I find it is also likely that short URLs (especially in the case of a directory-type site) are seen first by the spider and that this order is respected by the crawler (FIFO).\nYou can also ask in #seo on irc.freenode.net I know there are knowledgable SEO people in there who might be able to provide you with a decent answer.\nreply\n\t\n\nignifero 1 day ago | link\n\nMaybe because that's how they are sorted in the hashtable/database they use to queue urls? Or maybe because they want to index the shortest pages first, so that they are processed before any duplicates with longer urls (i.e. get /articles/ before /articles/index.php)\nreply\n\t\n\ngeorgemcbay 1 day ago | link\n\nI suspect you're on the right track with your first guess.\nMost people posting here are looking for some sort of deep meaning in this when IMO it is more likely just due to a localized side-effect of doing something such as storing the urls in a trie-like structure and then iterating over it breadth-first.\nreply\n\t\n\nchristianwilde 1 day ago | link\n\nI imagine that in these specific case is because longer URLs represent deeper pages on the site that are less \"important\" (in terms of internal incoming links and pagerank) than the shorter ones. It doesn't seem logical that google order the URLs by length and then crawl them in that order; probably the URL length can be a factor that the bot takes into account, but not the only one in the manner this article suggest :)\nAnyway, good point, that deserves more testing to extract some conclussions\nreply\n\t\n\nTuxPirate 1 day ago | link\n\nPagerank determine crawls rate amongst other things (http://techpatio.com/2009/search-engines/google/matt-cutts-g...).\nI find it is also likely that short URLs (especially in the case of a directory-type site) are seen first by the spider and that this order is respected by the crawler (FIFO).\nYou can also ask in #seo on irc.freenode.net I know there are knowledgable SEO people in there who might be able to provide you with a decent answer.\nreply\n\t\n\nabrudtkuhl 1 day ago | link\n\nIt starts at the top of your sitemap - which are likely shorter URLs\nreply\n\t\n\nbauchidgw 1 day ago | link\n\ncant confirm this - sitemaps are used for discovery -&gt; the urls listed in the sitemap get pushed into the 'discovered urls queue' then this queue is prioritized for crawling- and - if there are no other factors - the shorter urls get prioritized higher (as there is a bigger chance that a shorter url is a canonical version of a longer url - well, the chance is bigger then the other way round\nreply",
        "date": "2011-06-26 22:36:18",
        "timestamp": 1309142178
      },
      {
        "id": 252,
        "parent_id": 0,
        "author_ip": "12.49.220.49",
        "author": "Michael Martinez",
        "email": "michael.martinez@xenite.org",
        "content": "I downloaded your data and found many examples where long URLs were crawled before short URLs.  I don't know why you would think there was a pattern.  The graphic is cute but doesn't show how the crawlers skipped around your link list rather than follow the pattern on your root URL (which was fetched along with the favicon.ico file many times before anything else was fetched).\n\nYou don't indicate when links started appearing to the site, where they appeared, or provide any helpful information that might pinpoint when and how Google learned about the site.\n\nSo the only explanation that makes sense for what you have perceived is your relative inexperience in analyzing crawl logs and patterns.",
        "date": "2011-07-01 14:24:05",
        "timestamp": 1309544645
      },
      {
        "id": 254,
        "parent_id": 252,
        "author_ip": "99.62.119.200",
        "author": "Russell Ballestrini",
        "email": "russell.ballestrini@gmail.com",
        "content": "All the pages were created at the same time.\n\nThe 'cute' graph (not graphic) displays the length of the resource \"get\" urls in the order that google bot fetched them.  There is obviously a pattern in the graph related to length.\n\nOf course there were some urls fetched first that were larger than others, but that is just some chaotic abnormalities among the data.  The pattern persists regardless.\n\nAttacking my experience does not help your argument.",
        "date": "2011-07-01 18:27:29",
        "timestamp": 1309559249
      }
    ],
    "metadata": {}
  },
  "add-a-breadcrumb-subscriber-to-a-pyramid-project-using-4-simple-steps": {
    "name": "add-a-breadcrumb-subscriber-to-a-pyramid-project-using-4-simple-steps",
    "id": "709",
    "link": "http://russell2.ballestrini.net/add-a-breadcrumb-subscriber-to-a-pyramid-project-using-4-simple-steps/",
    "title": "Add a Breadcrumb Subscriber to a Pyramid project using 4 simple steps",
    "content": "<p>\n<strong>This article will explain how to add a breadcrumb subscriber to a Pyramid project using 4 simple steps.</strong>\n</p>\n\n<p>\nWhile programming [linkpeek-hover uri=\"http://school.yohdah.com/random\" text=\"school.yohdah.com\"] I needed the ability to easily create breadcrumb links from the current url.  You may view the <a href=\"https://bitbucket.org/russellballestrini/bread/src/tip/bread.py\">bread.py source code here</a>.  The following guide describes the process I took to add this functionality.\n</p>\n\n<strong>1.</strong>  Download and include <a href=\"https://bitbucket.org/russellballestrini/bread/raw/tip/bread.py\">bread.py</a> at the top of your Pyramid project's __init__.py file:\n\n<pre>\n    from yourproject.bread import Bread\n    from pyramid.events import subscriber, NewRequest\n</pre>\n\n<strong>2.</strong>  At the bottom of the projects __init__.py file create the following subscriber function as follows:\n\n<pre>\n    def bread_subscriber( event ):\n        \"\"\" Build Bread object and add to request \"\"\"\n        event.request.bread = Bread( event.request.url )\n</pre>\n\n<strong>3.</strong>  Now we can add our new subscriber to our Pyramid config inside main and above the routes:\n\n<pre>\n    config.add_subscriber( base_subscriber, NewRequest )\n</pre>\n\n<strong>4.</strong>  You are done!  Now you should have the ability to use the bread object from your template.  Below I have provided a mako template snippet:\n\n<pre>\n    % for link in request.bread.links:\n      ${ link | n } /\n    % endfor\n</pre>",
    "date": "2011-07-02 17:25:47",
    "timestamp": 1309641947,
    "comments": [
      {
        "id": 4093,
        "parent_id": 4059,
        "author_ip": "127.0.0.1",
        "author": "Russell Ballestrini",
        "email": "russell.ballestrini@gmail.com",
        "content": "@Mark Huang, thank you for your kind comments.  Feel free to show off the twitter bootstrap integration!",
        "date": "2012-07-10 18:34:27",
        "timestamp": 1341959667
      },
      {
        "id": 4059,
        "parent_id": 0,
        "author_ip": "127.0.0.1",
        "author": "Mark Huang",
        "email": "zhenghao1@me.com",
        "content": "Wow, that was dead simple man!  Thanks for the little tutorial!\n\nI was thinking of integrating your solution with Twitter Bootstrap's Bread crumb ui.",
        "date": "2012-07-09 23:57:20",
        "timestamp": 1341892640
      },
      {
        "id": 11847,
        "parent_id": 0,
        "author_ip": "127.0.0.1",
        "author": "Julien Tanay",
        "email": "julien.tanay@gmail.com",
        "content": "Hi Russell, \n\nThanks for your answer !\n\nbtw, you solved another problem I faced with this <code> | n</code> tip :).\n\nJulien",
        "date": "2012-12-10 14:57:47",
        "timestamp": 1355169467
      },
      {
        "id": 11843,
        "parent_id": 0,
        "author_ip": "127.0.0.1",
        "author": "Russell Ballestrini",
        "email": "russell.ballestrini@gmail.com",
        "content": "The problem wasn't with the links method, the problem was with my example mako template code:\n\n  \n  <code>% for link in links:\n    ${link|n} /  \n  % endfor</code>\n\nNewer versions of pyramid escape HTML.  To prevent escaping HTML (to send HTML into your template from your view) use <code> | n </code>\n\nI updated the example above to be more correct.",
        "date": "2012-12-10 13:19:17",
        "timestamp": 1355163557
      },
      {
        "id": 11828,
        "parent_id": 0,
        "author_ip": "127.0.0.1",
        "author": "Julien Tanay",
        "email": "julien.tanay@gmail.com",
        "content": "Hi ! \nYour breadcrumbs solution is the best i've found so far.\nAfter implementing it in my Pyramid application, it happens that link are displayed without being interpreted in HTML: \n\n<code>\"<a href=\"http://localhost:6543\" rel=\"nofollow\">localhost:6543</a> / (...)\"</code>\n\nDid I miss something ?\n\nThanks, \n\nJulien",
        "date": "2012-12-10 09:21:41",
        "timestamp": 1355149301
      },
      {
        "id": 11831,
        "parent_id": 0,
        "author_ip": "127.0.0.1",
        "author": "Julien Tanay",
        "email": "julien.tanay@gmail.com",
        "content": "@update :\n\nI solved my error by tweaking a bit your links function :\n\n<code>def links( self ):\n    links = []\n    for count, crumb in enumerate( self.crumbs, start = 1 ):\n        crumb_uri = self._protocol + '/'.join( self.crumbs[ 0:count ] )\n        links.append({'uri':crumb_uri, 'crumb':crumb})\n    return links</code>\n\nI also embedded it in a Boostrap breadcrumb (as suggested earlier) :\n\n  \n\n<code>  % for link in request.bread.links:\n    \n      <a href=\"${link['uri']}\" rel=\"nofollow\">${link['crumb']}</a> /\n    \n  % endfor</code>\n\n\nThanks again for this useful tool !\n\nJulien",
        "date": "2012-12-10 10:28:53",
        "timestamp": 1355153333
      }
    ],
    "metadata": {
      "_edit_last": "1",
      "_spost_short_title": null
    }
  },
  "a-hack-to-gain-80-percent-efficiency-when-creating-github-projects": {
    "name": "a-hack-to-gain-80-percent-efficiency-when-creating-github-projects",
    "id": "791",
    "link": "http://russell2.ballestrini.net/a-hack-to-gain-80-percent-efficiency-when-creating-github-projects/",
    "title": "a hack to gain 80 percent efficiency when creating github projects",
    "content": "<img src=\"/wp-content/uploads/2011/07/80-percent-github-hack.png\" alt=\"\" title=\"80-percent-github-hack\" width=\"200\" class=\"alignright\" />\n\n<p>\n<strong> Last night I decided to learn how to use git for its popularity and github to code more socially.</strong>\n</p>\n\n<p>\nI have to admit early on that I enjoy hg and bitbucket so it came to a surprise that github would have me jump through hoops to create a new repository...\n</p>\n\n<p>\nBelow I have copied the seemingly bloated github instructions for creating a new project.\n</p>\n\n<p>\n<strong>Github:</strong>\n</p>\n<ol>\n\t<li>create project/repo using the form</li>\n\t<li>mkdir scratch</li>\n\t<li>cd scratch</li>\n\t<li>git init</li>\n\t<li>touch README</li>\n\t<li>git add README</li>\n\t<li>git commit -m 'first commit'</li>\n\t<li>git remote add origin</li>\n\t<li>git@github.com:russellballestrini/scratch.git</li>\n\t<li>git push -u origin master</li>\n</ol>\n\n\n\n<p>\nWow...\n</p>\n<p>\nI like my method better.\n</p>\n<p>\n<strong>My Github clone hack:</strong>\n</p>\n  <ol>\n\t<li>create project/repo using the form</li>\n\t<li>git clone https://russellballestrini@github.com/russellballestrini/scratch.git</li>\n</ol>\n\n\n<p>\nAwesome, 2 steps versus 10.  That hack appears 80% more efficient!  Now if only my name was shorter... LOL\n</p>\n<p>\n<strong>You should follow me on twitter <a href=\"http://twitter.com/russellbal\">here</a>.</strong>\n</p>",
    "date": "2011-07-11 21:47:42",
    "timestamp": 1310435262,
    "comments": [],
    "metadata": {}
  },
  "the-great-gist-heist": {
    "name": "the-great-gist-heist",
    "id": "810",
    "link": "http://russell2.ballestrini.net/the-great-gist-heist/",
    "title": "The Great Gist Heist",
    "content": "<p>\n<strong>The Great Gist Heist</strong>\n</p>\n\n<p>\nI have crawled, downloaded, and archived all of gist.github.com.  Please hear my story before jumping to conclusions.\n</p>\n\n<p>\n<strong>Why?</strong>\n</p>\n\n<p>\nI'm currently building software that requires a large corpus  of source code.  \n</p>\n<p>\nI began to search for a collection of source code documents but my pursuit appeared fruitless.  Feeling displeased I attempted to gather all of my own source code.  My collection lacked fidelity perhaps because of my revere for the python language.\n</p>\n<p>\nRegardless of the reasoning, I needed a higher quantity of samples. I needed unbiased samples from all programming languages. I needed, most importantly, samples in a variation of quality that only the most popular paste sites have... sites like gist.\n</p>\n<p>\n<strong>Why are you sharing it?</strong>\n</p>\n<p>\nI feel a little bad about using Github's bandwidth.\n</p>\n<p>\nSharing this collection should reduce the chances that others will crawl for the same data.  If you need a large collection of source code, <a href=\"http://russell.ballestrini.net/wp-content/uploads/2011/08/the-great-gist-heist.torrent\" title=\"the-great-gist-heist.torrent\">download this torrent</a>.\n</p>\n<p>\n<strong>How did you do it?</strong>\n</p>\n<p>\nI wrote a short, 30 line, python script.  The script is part of the torrent.\n</p>\n<p>\nAt the peak of the scrape I had 14 threads running of the script, using approximately 580Kbps (I used iftop).\n</p>",
    "date": "2011-08-07 02:30:08",
    "timestamp": 1312698608,
    "comments": [
      {
        "id": 13635,
        "parent_id": 0,
        "author_ip": "127.0.0.1",
        "author": "Ben Boyter",
        "email": "bboyte01@gmail.com",
        "content": "Hi Russel,\n\nI found this link through criticue (after they updated to let me see the rest of the feedback). Already downloading this so cheers for providing the feedback. If you want to contact me directly use the email (in this post).\n\nAlways looking for someone to help out.\n\nBen",
        "date": "2012-12-16 22:57:02",
        "timestamp": 1355716622
      }
    ],
    "metadata": {
      "_edit_last": "1",
      "_spost_short_title": null
    }
  },
  "programming-is-like-alchemy": {
    "name": "programming-is-like-alchemy",
    "id": "842",
    "link": "http://russell2.ballestrini.net/programming-is-like-alchemy/",
    "title": "Programming is like Alchemy",
    "content": "[caption id=\"attachment_845\" align=\"alignright\" width=\"240\" caption=\"This image comes from Michael Maier&#039;s 1618 treatise on alchemy. It combines music, image and text to communicate alchemical knowledge to adepts. Note the combination of Pythagorean imagery with alchemical practice.\"]<a href=\"/wp-content/uploads/2011/08/Alchemy_2.jpg\"><img src=\"/wp-content/uploads/2011/08/Alchemy_2.jpg\" alt=\"This image comes from Michael Maier&#039;s 1618 treatise on alchemy. It combines music, image and text to communicate alchemical knowledge to adepts. Note the combination of Pythagorean imagery with alchemical practice. - princeton.edu\" title=\"Alchemy\" width=\"240\" /></a>[/caption]\n\n<p>\n<strong>Programming is like Alchemy</strong> except instead of exchanging matter, we programmers exchange time.  Also depending on the program the exchange of time worked (coding) increases the productivity (time) of its users.\n</p>\n\n<p>\nOn second thought, perhaps programmers correlate less with Alchemists and more with Time Travelers; Or at the very least time manipulators.  For example, on a good day a programmer can easily complete a task that would take one thousand men.  See, time created!  On a bad week we can procrastinate and do nothing at all.  Time lost!   \n</p>\n\n[caption id=\"attachment_857\" align=\"alignleft\" width=\"260\" caption=\"Golem from the Soul Caliber universe\\n\\n\"]<a href=\"/wp-content/uploads/2011/08/Golem-soul-caliber1.jpg\"><img src=\"/wp-content/uploads/2011/08/Golem-soul-caliber1.jpg\" alt=\"Golem-soul-caliber\" title=\"Golem-soul-caliber\" width=\"260\" class=\"size-full wp-image-857\" /></a>[/caption]\n\n<p>\n<strong>Programming embodies other magic like wizardry.</strong>  For example,\nour programs typically live as golems performing one task, repeatedly, over and over.  Golem programs, without a soul, stuck in a loop of servitude.\n</p>\n\n<p>Recently we have started coding creations with artificial intelligence.  These smart programs act like familiar spirits (Wikipedia) and assist their creator in conjuring even more magic.\n</p>\n\n<p> \nSo we settled it! Programmers are like bad ass, time travelling, wizard alchemists!\n</p>\n\n<p>\n<strong>Or maybe not ...</strong>\nIt is more accurate to group programs with technology then magic, but less fun.  Programs are leveraged tools born to save people time and energy. \n</p>\n\n<p>\n<strong>Thanks for reading, you should follow me on twitter <a href=\"http://twitter.com/russellbal\">here</a>.</strong>\n</p>\n\n\n[caption id=\"attachment_867\" width=\"1440\" caption=\"Time Travelling Wizard Alchemists - http://imgur.com/r/pics/qW6Fv\"]<a href=\"/wp-content/uploads/2011/08/time-travelling-wizard-alchemists.jpg\"><img src=\"/wp-content/uploads/2011/08/time-travelling-wizard-alchemists.jpg\" alt=\"time-travelling-wizard-alchemists\" title=\"time-travelling-wizard-alchemists\" width=\"1440\" height=\"900\" class=\"size-full wp-image-867\" /></a>[/caption]",
    "date": "2011-08-08 21:52:21",
    "timestamp": 1312854741,
    "comments": [],
    "metadata": {
      "_edit_last": "1",
      "_spost_short_title": null
    }
  },
  "virt-back-restoring-from-backups": {
    "name": "virt-back-restoring-from-backups",
    "id": "871",
    "link": "http://russell2.ballestrini.net/virt-back-restoring-from-backups/",
    "title": "virt-back: restoring from backups",
    "content": "<p>\n<strong>In a perfect world we should create backups but never need them.</strong>  Although this statement holds truth, creating guest backups provides many more benefits. \n</p>\n<p>\nThe most common reasons system administrators restore from a virt-back guest backup:\n</p>\n  <ul>\n    <li>recovering from data corruption</li>\n    <li>recovering deleted files</li>\n    <li>recovering from a virus infection</li>\n    <li>recovering from a compromised server</li>\n    <li>backing out a failed change</li>\n    <li>rolling back to a previous state</li>\n    <li>testing disaster recovery plans</li>\n    <li>cloning a server</li>\n    <li>building test environments</li>\n  </ul>\n\n<br />\n<p>\nDuring this article we will cover how to restore a system from a virt-back guest backup.  This article will not cover how to restore a VM host server.\n</p>\n<p>\n<strong>Virt-back guest restore procedure</strong>\n</p>\n<p>\nIn this guide our guest mbison has failed with a major corruption and we would like to restore from our backups.  We have our running production guest images in /KVMROOT and our virt-back guest backups in /KVMBACK.  We will be restoring the backup on the same hypervisor.\n</p>\n<p>\n<strong>Overview:</strong>\n</p>\n<ol>\n  <li>Ensure the guest is shut off. </li>\n  <li>move the bad image file out of the way</li>\n  <li>untar the virt-back backup into place</li>\n  <li>power up the guest</li>\n</ol>\n\n<br />\n\n<strong>Detailed Procedure:</strong>\n\n<ol>\n  <li>Verify the guest is shut off by running:\n\n    <pre>virt-back --info-all\n    </pre>\n\n  </li>\n\n  <li>We noticed that mbison was still running so we invoked:\n\n    <pre>virt-back --shutdown mbison\n    </pre>\n\n  </li>\n\n  <li>Move the corrupted image file out of the way:\n\n    <pre>mv /KVMROOT/mbison.img /KVMROOT/mbison.img.NFG\n    </pre>\n\n  </li>\n\n  <li>Unzip and unarchive the backup using the following command: \n\n    <pre>sudo tar -xzvf /KVMBACK/mbison.tar.gz -C /KVMROOT --strip 1\n    </pre>\n\n  </li>\n\n  <li>When the untar completes, start the guest: \n\n    <pre>virt-back --create mbison\n    </pre>\n\n  </li>\n\n  <li>Connect to the guest over SSH and verify that all required services and applications start.  Determine if the restore was successful.\n  </li>\n</ol>\n\n<br/>\n\n<strong>Restore guest backup on new hypervisor:</strong>\n\n<br/>\n<br/>\n\nThe details in this section were adapted from a tutorial given by <a href=\"http://fabianrodriguez.com/\">\nFabian Rodriguez</a>.\n\n<br/>\n<br/>\n\n<ol>\n<li>Re-create any bridge network interfaces on new hypervisor (/etc/network/interfaces for Debian)</li>\n<li>Adjust mbison.xml if needed (for example if you are changing paths)</li>\n</ol>\n\n<pre>\nsudo mkdir /KVMROOT\nsudo tar -xvzf mbison.tar.gz -C /KVMROOT --strip 1\nvirsh create /KVMROOT/mbison.xml\n</pre>\n\n<p>\n<strong>Note:</strong> We use virsh create instead of virt-back create.  While both commands start guest DOMs, virsh create will also register the DOM into the hypervisor.\n</p>\n\n<br/>",
    "date": "2011-08-10 23:20:06",
    "timestamp": 1313032806,
    "comments": [],
    "metadata": {
      "_edit_last": "1",
      "_series_part": "2",
      "_spost_short_title": null
    }
  },
  "security-professionals-yes-we-appear-vulnerable-but-that-attack-vector-will-never-happen": {
    "name": "security-professionals-yes-we-appear-vulnerable-but-that-attack-vector-will-never-happen",
    "id": "919",
    "link": "http://russell2.ballestrini.net/security-professionals-yes-we-appear-vulnerable-but-that-attack-vector-will-never-happen/",
    "title": "Security Professionals: Yes we appear vulnerable but that attack vector will never happen",
    "content": "<p>\n<strong>Security Professionals: Yes, we appear vulnerable but that attack vector will never happen.</strong>\n</p>\n\n<p>\nIn loom of recent internet attacks many institutions have started scrambling in attempt to \"strengthen\" their security stance.  I agree that auditing our systems and networks for potential flaws seems appropriate at this time to prevent getting \"caught with our pants down\".  Incidentally, I have recently witnessed the introduction of silly and at times ineffective security adjustments.  Many of these new procedures, rules, and requirements do not make us more secure and worse instill a false sense of security.  \n</p>\n<p>\nI have previously addressed the fallacy of absolute security.  No system is perfect.  A successful security model accomplishes fortitude by implementing [linkpeek-hover uri=\"http://russell.ballestrini.net/dropbox-encryption-with-truecrypt/\" text=\"layers like an onion\" size=\"336x504\"]. Through the use of security layers we can significantly hamper attack vectors and create a safer complex.\n</p>\n<p>\nWhen analyzing a potential attack vector we must first determine our current location in the security layers.  This step serves two purposes:\n</p>\n<ul>\n\t<li>to prevent wasting time and energy on vulnerabilities that don't matter at that point in our matrix.</li>\n\t<li>to prevent causing outages and unneeded administrator and customer heartache.</li>\n</ul>\n\n<br />\n<p>\nIf a vulnerability requires root or elevated privileges to occur, don't waste your time resolving it. If the attacker already has root, you have bigger problems on your hands. \n</p>\n<p>\n<strong>Some real life examples:</strong>\n</p>\n<ol>\n\t<li>\nFirewall denying a large range of IP addresses (like entire countries).  This truly does not increase security, it just creates headaches for users.  An attacker could just proxy to an open range (like a VPS based in a more trusted zone) and gain access from there.  Also if you decide to ignore this advice and create blanket IP range deny rules, DON'T also block services intended to be internet-facing.  For example, don't block your Internet-facing DNS server if it is authoritative for public domains.  This will cause countless intermittent issues and will be a nightmare to diagnose.  \n        </li>\n\n        <br />\n\n\t<li>\nWeekly Scanning for Windows viruses on network shares or data at rest.  This hammers the servers for no reason.  If all the desktops run antivirus then the file was already scanned when it was downloaded.  That same file will be scanned again when retrieved on the share.  If you want the warm and fuzzys of virus scanning network file shares, do it once a year.  These scans waste time and resources.  I feel even more outraged when asked to virus scan network shares hosted on UNIX servers or NAS.\n        </li>\n</ol>\n\n<br />\n<p>\nI speculate that most of these arbitrary ideas come about because the people in charge make uninformed decisions out of fear without first consulting the appropriate subject matter experts.\n</p>\n<p>\nUnfortunately, once a security mandate occurs it seems difficult to expunge.  People are just not willing to put their neck on the chopping block to banish a legacy or silly mandates; So we end up living with nonsensical rules and procedures.\n</p>\n[caption id=\"attachment_969\" align=\"aligncenter\" width=\"740\" caption=\"http://xkcd.com/936/ \"]<a href=\"/wp-content/uploads/2011/08/password-strength.png\" target=\"_blank\"><img src=\"/wp-content/uploads/2011/08/password-strength.png\" alt=\"http://xkcd.com/936/ \" title=\"password-strength\" width=\"740\" height=\"601\" class=\"size-full wp-image-969\" />[/caption]",
    "date": "2011-08-18 21:36:16",
    "timestamp": 1313717776,
    "comments": [
      {
        "id": 560,
        "parent_id": 0,
        "author_ip": "193.130.120.206",
        "author": "Consultuning",
        "email": "consultuning@gmail.com",
        "content": "While I agree with your general argument, I have a lot of concerns for your second example. Which by the way, was exactly what I was telling people three years ago. Now with experience, I can give you some advice:\n\n- Yes, if you scan shared folders on Unix boxes you're not preventing any infection spreading from one Unix or NAS box to the next, because there are close to none for them. However, what about files stored there infected with Windows malware that are being accessed by Windows boxes? AV scan of NAS/Unix shares can prevent those servers acting as infection vectors.\n\n- Oh, yes, you're redundantly doing the AV on both the servers and the client workstations, so why you cannot remove one of them? This is a good question, but again you have not give it enough thought.\n\nLook, if you can absolutely-positively-without-any-doubt be sure that no one is ever going to plug into your network a machine that does not have AV up to date (or is not sensible to such attacks, such as a Unix box) then you can remove the AV engine on the file server and leave it to the workstation to do the virus scan.\n\nSomehow I think that the technical measures to prevent someone to plug into your network a non sanctioned equipment are going to be way more costly and difficult to implement. And no, just banning that as company policy is not going to work, people regularly bring equipment from home and plug it into the Ethernet port just to see what happens.\n\nIn fact, if I had to remove one of the two AV engines, it would be the one on the workstations. Workstations can be easily replaced and if they go down affect only a single person. Yes, there can be critical data stored on an individual desktop or laptop, but logic says that it should be a minor impact compared to losing a whole file server.\n\nSo keep hammering your server with weekly, no, make it daily, AV scans. File servers are there to be hammered, after all they are designed for... serving files. And consider AV scanning your file server just another of those security layers you correctly mention as the foundation of good security. \n\nHowever, using such a bad example does not invalidate your argument. Yes, there are more than a fair share of completely useless security policies out there, and people prefer to keep them rather than taking the risk of thinking by themselves. I agree fully with that. Just as your bad example shows, if you don't think enough about them you may find yourself in an awkward position if something bad happens.\n\nOr better yet, use a sensible security professional. Which I am not (security professional, I mean)",
        "date": "2011-08-19 04:57:40",
        "timestamp": 1313744260
      },
      {
        "id": 555,
        "parent_id": 0,
        "author_ip": "208.180.38.162",
        "author": "Randall",
        "email": "1337geekguy@gmail.com",
        "content": "A realization must be made: The fact that as security increases, functionality decreases. A piece of dirt has no security flaws.",
        "date": "2011-08-18 22:52:57",
        "timestamp": 1313722377
      },
      {
        "id": 557,
        "parent_id": 0,
        "author_ip": "38.126.103.131",
        "author": "Overand",
        "email": "overand@gmail.com",
        "content": "Yes, but at least things like food can grow in a big collection of Dirt.",
        "date": "2011-08-19 00:59:10",
        "timestamp": 1313729950
      },
      {
        "id": 559,
        "parent_id": 0,
        "author_ip": "72.220.193.245",
        "author": "Weird",
        "email": "russell@ballestrini.net",
        "content": "\"One Thousand Steps Begins With One, but a Jack of All Trades is a Master of None\"\n\nHuh? Did you paste together two random quotes?",
        "date": "2011-08-19 02:58:28",
        "timestamp": 1313737108
      },
      {
        "id": 565,
        "parent_id": 0,
        "author_ip": "69.207.119.1",
        "author": "irv",
        "email": "irvingthemagnificent@yahoo.com",
        "content": "A lot of what you complain about is \"security theater.\" That is, some executive mandates that something be done entirely so they can go to upper management and say that something has been done. \n\nEffectiveness is not even part of the equation.",
        "date": "2011-08-19 14:33:46",
        "timestamp": 1313778826
      },
      {
        "id": 561,
        "parent_id": 0,
        "author_ip": "64.255.164.78",
        "author": "Brian",
        "email": "bdclang@gmail.com",
        "content": "Major reason we often do anything someone thinks up is because no one in security wants to put their neck on the line with senior management by saying something is a waste of time and effort on the remote chance that something actually happens. They know their butt is on the line.",
        "date": "2011-08-19 06:25:05",
        "timestamp": 1313749505
      },
      {
        "id": 554,
        "parent_id": 0,
        "author_ip": "208.180.38.162",
        "author": "Grend",
        "email": "riddle1337@gmail.com",
        "content": "I agree with you and think the problem will worsen, as security is increasingly politicized, politicians who know nothing about the field will begin to use their security ideas as political flare. I believe we are going to enter a new era of computer security hilarity because of this.",
        "date": "2011-08-18 22:30:47",
        "timestamp": 1313721047
      }
    ],
    "metadata": {}
  },
  "python-image-grabber-pig-py": {
    "name": "python-image-grabber-pig-py",
    "id": "974",
    "link": "http://russell2.ballestrini.net/python-image-grabber-pig-py/",
    "title": "Python Image Grabber pig.py",
    "content": "[caption id=\"attachment_1007\" align=\"alignright\" width=\"225\" caption=\"Pigpy the pig.py creative commons mascot!\"]<img src=\"/wp-content/uploads/2011/08/pig1-color1.png\" alt=\"pig.py\" title=\"pig.py\" width=\"220\" />[/caption]\n<p>\n<strong>Python Image Grabber pig.py</strong>\n</p>\n<p>\nPython Image Grabber or pig.py is a <em>very</em> simple python command line tool to download all the images from a given uri.\n</p>\nEnjoy!\n\n<br />\n<p>\n<strong>Download and Installation:</strong>\n</p>\n<a href=\"https://bitbucket.org/russellballestrini/pig/raw/tip/pig.py\">https://bitbucket.org/russellballestrini/pig/raw/tip/pig.py</a>\n\n<p>\n<strong>Usage:</strong>\n</p>\n<code>python pig.py <full uri with protocol to crawl>\n</code>\n<strong>Example:</strong>\n<code>python pig.py http://www.foxhop.net\n</code>\n\n<p>\n<strong>Open Source:</strong>\n</p>\n<a href=\"https://bitbucket.org/russellballestrini/pig/raw/tip/pig.py\">pig.py</a> has been placed in the public domain and its sourcecode may be viewed or branched here here: <a href=\"https://bitbucket.org/russellballestrini/pig\">https://bitbucket.org/russellballestrini/pig</a>\n\n<p>\n<strong>Pigpy Gimp Project</strong>\n</p>\n<a href=\"/wp-content/uploads/2011/08/pigpy.xcf\">pigpy.xcf</a>\n\n<p>\n[gallery]\n</p>",
    "date": "2011-08-22 18:48:57",
    "timestamp": 1314053337,
    "comments": [
      {
        "id": 647,
        "parent_id": 638,
        "author_ip": "20.132.64.141",
        "author": "Russell Ballestrini",
        "email": "russell.ballestrini@gmail.com",
        "content": "In python when you string split() a list (sometimes referred to as array in other languages) is returned.  From that list I'm asking for the last index.  \n\nExample:\n\n# a string\nimg_uri = \"www.foxhop.net/attachments/pic1.jpg\"\n\n# a list of strings\nimg_uri_parts = img_uri.split('/')\n\nprint img_uri_parts\n# >>> ['www.foxhop.net', 'attachments', 'pic1.jpg']\n\n# I want the filename, last part of the uri\n# In this case img_uri[2]\n# This will not always be true.  \n# I always need the last element of the list.\n\nprint img_uri_parts[-1]\n# >>> 'pic1.jpg'\n\nFor more information take a look at this page:\n\nhttp://docs.python.org/tutorial/datastructures.html",
        "date": "2011-08-23 12:15:59",
        "timestamp": 1314116159
      },
      {
        "id": 638,
        "parent_id": 0,
        "author_ip": "65.172.152.16",
        "author": "JasonG",
        "email": "jgurtz@gmail.com",
        "content": "You're a python ninja. Here's my favorite line:\n\n    filename = img_uri.split('/')[-1]\n\nIt intrigues me. With the perl background I'm familiar with the whole split routine...but the [-1]... -1 from what? It's a nice magic :)",
        "date": "2011-08-22 22:51:47",
        "timestamp": 1314067907
      }
    ],
    "metadata": {}
  },
  "deliberating-the-viewers-vs-doers-concept": {
    "name": "deliberating-the-viewers-vs-doers-concept",
    "id": "1111",
    "link": "http://russell2.ballestrini.net/deliberating-the-viewers-vs-doers-concept/",
    "title": "Deliberating the Viewers vs. Doers concept",
    "content": "<p>\n<img src=\"/wp-content/uploads/2011/09/lecture-viewers-vs-doers.jpg\" alt=\"Viewers vs Doers Lecture\" title=\"Viewers vs Doers Lecture\" width=\"640\" height=\"427\" class=\"size-full wp-image-1117\" />\n</p>\n<p>\n<strong><a href=\"http://artofmanliness.com/2011/08/28/viewers-vs-doers-the-rise-of-spectatoritis/\">Brett & Kate McKay from artofmanliness.com</a></strong> recently subscribed to an idea that America has succumbed to \"spectatoritis, a blanket description to cover all kinds of passive amusement, an entering into the handiest activity merely to escape boredom.\" -Jay B. Nash, 1938\n</p>\n<p>\nThe basic concept discussed how people can fall into one of two groups when  interacting in life, the viewers and the doers.  Viewers or spectators watch an activity while doers perform the said activity.  For example:\n</p>\n\n<strong>Viewers</strong>\n<ul>\n\t<li>Sports Fans</li>\n\t<li>Audience</li>\n\t<li>Couch Potatoes</li>\n</ul>\n\n<strong>Doers</strong>\n<ul>\n\t<li>Athletes</li>\n\t<li>Musicians</li>\n\t<li>Actors</li>\n</ul>\n\n<p>\nAlthough the above patterns persists in the physical world, intellectual situations operate differently.  For instance on the surface reading seems like a viewer or spectator task and writing appears to behave like a doer task.  But what happens when a reader learns or responds to a blog post?  We seem to need a stronger definition what constitutes doing a task.\n</p>\n\n<p>\n<blockquote>We should not regard spectating as bad, however we should try to stay lucid of our current rolls when engaging in activities.\n</blockquote>\n</p>\n\n<p>\nSpectating has some important purposes in our society and culture.  Viewing a performance grants power to the actor, whether that person plays football or plays guitar.  For this power and influence to occur a viewer must witness and pay attention to the event.  What if the next great political leader took the podium and no one bothered to view his speech?\n</p>\n\n<p>\nThe truth is our American society has conditioned us to spend much of our lives spectating.  During school a good student will view and listen to the lecture.  While driving to work one radio DJ broadcasts his thoughts to many.  \n</p>\n\n<p>\nSpectating also plays and integral roll in learning.  Human infants learn by first watching and then imitating.  Experts also agree that inspiration for a creative works often occur after observation of prior productions.\n</p>\n\n<p>\n<strong>tl;dr</strong> We should not battle the viewers vs the doers because both hold importance and need each other.\n</p>\n\n<p>\n<strong>If you liked this article, you should follow me <a href=\"https://plus.google.com/101342467879466559261/posts\">here</a>.</strong>\n</p>",
    "date": "2011-09-06 10:20:02",
    "timestamp": 1315318802,
    "comments": [],
    "metadata": {}
  },
  "a-system-administrators-guide-to-installing-and-maintaining-multiple-python-environments": {
    "name": "a-system-administrators-guide-to-installing-and-maintaining-multiple-python-environments",
    "id": "1177",
    "link": "http://russell2.ballestrini.net/a-system-administrators-guide-to-installing-and-maintaining-multiple-python-environments/",
    "title": "A system administrators guide to installing and maintaining multiple python environments",
    "content": "<p>\nSome operating systems depend on a specific version of python to function properly.  For example, Yum on Redhat Enterprise Linux 5 (RHEL5) depends on python 2.4.3.  This version of python lacks support from many utilities and 3rd party libraries.  This guide will cover installing an alternative python instance while leaving the system's python alone.\n</p>\n<p>\n<i>This guide supports the following operating systems: Redhat, CentOS, and Fedora.  As of this publication the latest Python version was 2.7.2; You might want to determine if a newer version exists.</i>\n</p>\n<ol>\n\n<li>Gather the dependencies:</li>\n  <pre>\n  yum install gcc zlib-devel python-setuptools readline-devel\n  </pre>\n\n    <strong>gcc</strong> <em>is a compiler used to build python</em> <br/>\n    <strong>zlib-devel</strong> <em>allows the python zlib module to be built</em><br/>\n    <strong>python-setuptools</strong> <em>provides the easy_install application</em><br/>\n    <strong>readline-devel</strong> <em>arrows readline and history handling in python shell</em><br/>\n\n<br/>\n\n<li>Download and untar the python sourcecode:</li>\n  <pre>\n  wget http://www.python.org/ftp/python/2.7.2/Python-2.7.2.tgz\n  tar -xzvf Python-2.7.2.tgz\n  cd Python-2.7.2\n  </pre>\n\n<li>Compile the sourcecode:</li>\n \n  <pre>\n  ./configure\n  make altinstall\n  </pre>\n\n<li>Test new alternative python:</li>\n\n<pre>\npython2.7</pre>\n\n<li>Now we can install third party libraries into our alternative python.</li>\n\n<pre>\npython2.7 -m easy_install <package name or egg path>\n</pre>\n\n</ol>\n\n<br/>\n<strong>virtualenv</strong> \n<p>\nOptionally we can create a virtualenv (for development) based on the python 2.7 install.  Virtual environments appear useful for testing packages and libraries without installing them to the system owned python site-packages directory.\n</p>\n<ol>\n\t<li>Install virtualenv using easy_install:</li>\n<pre>\neasy_install virtualenv\n</pre>\n\n\t<li>Create a new virtual python environment named virtpy:</li>\n<pre>\nvirtualenv --no-site-packages -p /usr/local/bin/python2.7 virtpy\n</pre>\n<p>\nThis will create a virtual python 2.7.2 environment named virtpy in your present working directory.\n</p>\n<p>\nTo invoke this environment run <code>source virtpy/bin/activate</code> and your prompt should change to reflect the active virtualenv.\n</p>\n<p>\nNow you can run <code>easy_install</code> to install packages into virtpy/lib/python2.7/site-packages.\n</p>\n</ol>\n<p>\n<strong>Thanks for reading, that's all for now.\n</strong>\n</p>",
    "date": "2011-09-08 19:28:01",
    "timestamp": 1315524481,
    "comments": [],
    "metadata": {
      "_edit_last": "1",
      "_spost_short_title": null
    }
  },
  "hacker-olive-oil-lamp-crafted-from-home-materials": {
    "name": "hacker-olive-oil-lamp-crafted-from-home-materials",
    "id": "1220",
    "link": "http://russell2.ballestrini.net/hacker-olive-oil-lamp-crafted-from-home-materials/",
    "title": "Hacker Olive Oil Lamp Crafted From Home Materials",
    "content": "[caption id=\"attachment_1221\" align=\"alignright\" width=\"320\" caption=\"Hacker Olive Oil Lamp\"]<img src=\"/wp-content/uploads/2011/09/hacker-olive-oil-lamp.jpg\" alt=\"\" title=\"hacker-olive-oil-lamp\" width=\"320\" class=\"size-full wp-image-1221\" />[/caption]\n\n<p>\nIn loom of the recent and persisting hurricane season I present a <strong>\"Hacker's Olive Oil Lamp\"!</strong>\n</p>\n<p>\n<strong>YES, this lamp is ...</strong>\n</p>\n<ul>\n\t<li><strong>Fun and simple to build</strong></li>\n \t<li><strong>Fun and simple to use</strong></li>\n\t<li><strong>Easy to Cleanup</strong></li>\n\t<li><strong>Gentle on environment</strong></li>\n\t<li><strong>Energy efficient</strong> (1 cup ~= 16 hours)</li>\n\t<li><strong>Safe</strong> (flame smothers if lamp tips over)</li>\n</ul>\n\n<strong>NO</strong>\n<ul>\n\t<li><strong>Odor</strong></li>\n\t<li><strong>Smoke</strong></li>\n\t<li><strong>Heat near base</strong></li>\n\t<li><strong>Cost</strong> (household materials)</li>\n\t<li><strong>Waste</strong> (Store and seal oil in lamp with jar top)</li>\n</ul>\n\n<strong>Materials</strong>\n<ul>\n\t<li>1x Pickle jar with lid</li>\n\t<li>1x Washcloth 100% cotton</li>\n\t<li>1x Steel wire (salvaged from paint can handle)</li>\n\t<li>2x Cups of olive oil</li>\n</ul>\n\n<strong>Tools</strong>\n<ul>\n\t<li>Needle nose plyers</li>\n\t<li>Scissors</li>\n</ul>\n\n\n\n\n\n\n\n<center><iframe width=\"420\" height=\"345\" src=\"http://www.youtube.com/embed/3I1W2ddJaAs\" frameborder=\"0\" allowfullscreen></iframe></center>\n\n[gallery link=\"file\"]",
    "date": "2011-09-09 19:01:32",
    "timestamp": 1315609292,
    "comments": [
      {
        "id": 776,
        "parent_id": 0,
        "author_ip": "192.168.1.23",
        "author": "Russell Ballestrini",
        "email": "russell.ballestrini@gmail.com",
        "content": "This very old tradition was explained to me by a co-worker during coffee break.  At first I was skeptical but I did some research and this was what I came up with!",
        "date": "2011-09-09 21:20:40",
        "timestamp": 1315617640
      }
    ],
    "metadata": {}
  },
  "new-baby-homecoming": {
    "name": "new-baby-homecoming",
    "id": "1288",
    "link": "http://russell2.ballestrini.net/new-baby-homecoming/",
    "title": "New Baby Homecoming",
    "content": "<code>\nfamily += 1\n\n</code>\n\n\n<img src=\"/wp-content/uploads/2011/09/RJ-and-Carter.jpg\" alt=\"\" title=\"RJ-and-Carter\" width=\"600\" height=\"800\" class=\"aligncenter size-full wp-image-1292\" />",
    "date": "2011-09-27 16:53:51",
    "timestamp": 1317156831,
    "comments": [],
    "metadata": {}
  },
  "how-do-i-calculate-the-m-in-my-mvp": {
    "name": "how-do-i-calculate-the-m-in-my-mvp",
    "id": "1305",
    "link": "http://russell2.ballestrini.net/how-do-i-calculate-the-m-in-my-mvp/",
    "title": "How do I calculate the M in my MVP?",
    "content": "<p>\n[linkpeek-link-image uri=\"http://russell.ballestrini.net\" size=\"140x100\" style=\"margin-right: 25px; float: left;\"]\n</p>\n\n<p>\n<strong>Recently I have contemplated the idea of building a screen capture service for websites.</strong>  I anticipate the service would provide functionality similar to Google's \"Instant Preview\".  I also pondered the idea of archiving the screen captures and providing a history similar to the internet Wayback Machine.\n</p>\n<p>\n[linkpeek-link-image uri=\"http://four2go.gumyum.com\" size=\"140x100\" style=\"margin-right: 25px; float: left;\"]\n</p>\n<p>\nAfter floundering on my first web application, <a href=\"http://four2go.gumyum.com\">http://four2go.gumyum.com</a>, I'm afraid to spend lots of time and energy on another project if I cannot earn users or customers.\n<br/>\n</p>\n<p>\nI feel the market segment for this type of service would include website directories, live portfolio generation for website designers, and websites in general for people who wish to keep track of the overall look of their site over time.\n</p>\n<p>\nAs an MVP I plan to build a free live web capture application to basically provide a \"Gravatar\" for website addresses.  \n</p>\n<p>\n[linkpeek-link-image uri=\"http://news.ycombinator.com\" size=\"140x100\" style=\"margin-right: 25px; float: left;\"]\n</p>\n<p>\n<strong>Do you think anyone would use my MVP or service?\n</strong>\n</p>\n<p>\n<strong>Have I missed any other market segments?\n</strong>\n</p>\n<br/>\n<br/>\n<strong>UPDATE: <a href=\"http://linkpeek.com/\" target=\"_blank\">LinkPeek.com</a> API has an <del datetime=\"2011-11-10T23:36:16+00:00\">alpha</del> beta release!</strong>\n<br/>\n<p>\n[linkpeek-link-image uri=\"http://linkpeek.com\" size=\"140x100\" style=\"margin-right: 25px; float: left;\"]\n\nLinkPeek provides an easy to use API which enables anyone to instantly create live web page thumbnails.  We provide the API free of charge, think of LinkPeek as Gravatar for web addresses.\n</p>\n\n<br/>",
    "date": "2011-10-01 15:32:01",
    "timestamp": 1317497521,
    "comments": [
      {
        "id": 881,
        "parent_id": 0,
        "author_ip": "109.231.193.164",
        "author": "Gerhard",
        "email": "gerhard@lazu.co.uk",
        "content": "Russell, this is amazing stuff. I was just looking for something like this for our back-end at gosquared.com. Any chance of providing dimensions as params for the thumbs?",
        "date": "2011-10-05 11:03:59",
        "timestamp": 1317827039
      },
      {
        "id": 883,
        "parent_id": 0,
        "author_ip": "192.168.1.21",
        "author": "Russell Ballestrini",
        "email": "russell.ballestrini@gmail.com",
        "content": "Gerhard,\n\nThank you for the support and kind words.\n\nIf people enjoy our service we plan to release a \"professional\" version of the API that would enable subscribers to request ANY thumbnail dimension.",
        "date": "2011-10-05 15:54:54",
        "timestamp": 1317844494
      }
    ],
    "metadata": {}
  },
  "linkpeek-com-web-address-thumbnail-api-alpha-release": {
    "name": "linkpeek-com-web-address-thumbnail-api-alpha-release",
    "id": "1362",
    "link": "http://russell2.ballestrini.net/linkpeek-com-web-address-thumbnail-api-alpha-release/",
    "title": "LinkPeek.com web address thumbnail api alpha release",
    "content": "<br>\n<br>\n\n<a href=\"http://linkpeek.com\" target=\"_blank\">\n[linkpeek-image uri=\"http://linkpeek.com\" apikey=\"9fhvyH9KP\" secret=\"S4EZ6wePOv\" size=\"140x100\" style=\"margin-right: 25px; float: left;\"]\n</a>\n\n<strong><a href=\"http://linkpeek.com/\" target=\"_blank\">LinkPeek.com</a> API has an alpha release!</strong>\n\n<br><br>\n\nLinkPeek is a website screenshot service. Convert any webpage to an image. No software, no downloading and absolutely no waiting!  Unlimited free 140 pixel thumbnails! \n\n\n<br/>\n<br/>\n</p>\n<p>\n<strong>You should dress up your naked links with [linkpeek-hover uri=\"http://linkpeek.com\" apikey=\"9fhvyH9KP\" secret=\"S4EZ6wePOv\" size=\"140x100\" text=\"LinkPeek.com\"]\n image previews.</strong>\n</p>",
    "date": "2011-10-05 20:58:34",
    "timestamp": 1317862714,
    "comments": [],
    "metadata": {
      "_edit_last": "1",
      "_spost_short_title": null
    }
  },
  "css-frameworks-not-rendering-properly-on-all-browsers": {
    "name": "css-frameworks-not-rendering-properly-on-all-browsers",
    "id": "1380",
    "link": "http://russell2.ballestrini.net/css-frameworks-not-rendering-properly-on-all-browsers/",
    "title": "CSS frameworks not rendering properly on all browsers",
    "content": "<p>I ran into an issue when testing skeleton css framework and twitter bootstrap where some browsers were not rendering the pages properly.  After some research I determined that I forgot the DOCTYPE declaration tag.\n</p>\n<p>\nThe DOCTYPE tag tells the browser what \"rules\" or standards to use when rendering markup.  \n</p>\n<p>\nThe following code block displays the minimal approach to setting the document type:\n</p>\n<p>\n<code><!DOCTYPE html>\n<HTML>\n</code>\n</p>\n<p>\nAfter placing the doctype above the <HTML> tag the pages render properly because we forced the browser into standards mode.\n</p>\n<p>\nThanks!\n</p>",
    "date": "2011-10-11 00:07:33",
    "timestamp": 1318306053,
    "comments": [],
    "metadata": {}
  },
  "a-better-way-to-show-website-backlinks": {
    "name": "a-better-way-to-show-website-backlinks",
    "id": "1386",
    "link": "http://russell2.ballestrini.net/a-better-way-to-show-website-backlinks/",
    "title": "A better way to show website backlinks",
    "content": "<p>\nEarly web pilgrims of the Internet fashioned search queries like<br/> <code>link:russell.ballestrini.net</code> to gather backlinks for a domain.  \n</p>\n<p>\nI however advocate a revolutionary search pattern like -\n<pre>\"russell.ballestrini.net\" -site:russell.ballestrini.net</pre> to gather an improved representation of backlinks.\n</p>\n<p>\n<a href=\"http://www.google.com/#sclient=psy-ab&hl=en&source=hp&q=%22school.yohdah.com%22+-site:school.yohdah.com&pbx=1&oq=%22school.yohdah.com%22+-site:school.yohdah.com&aq=f&aqi=&aql=1&gs_sm=e&gs_upl=1836l15311l0l17007l43l35l0l0l0l8l274l5507l4.23.8l35l0&bav=on.2,or.r_gc.r_pw.r_cp.,cf.osb&fp=18aa9f7b4fee5b6d&biw=1485&bih=912\">Click here to try now!</a>\n</p>",
    "date": "2011-10-12 20:05:52",
    "timestamp": 1318464352,
    "comments": [],
    "metadata": {
      "_edit_last": "1",
      "_spost_short_title": null
    }
  },
  "i-cancelled-my-xbox-live-automatic-renewal": {
    "name": "i-cancelled-my-xbox-live-automatic-renewal",
    "id": "1407",
    "link": "http://russell2.ballestrini.net/i-cancelled-my-xbox-live-automatic-renewal/",
    "title": "I cancelled my xbox live automatic renewal",
    "content": "<p>\n<strong>I cancelled my xbox live automatic renewal today because I no longer use the service.  </strong>\n</p>\n<p>\nI find humor in Microsoft's list of reasons to keep Xbox live:\n</p>\n <img src=\"/wp-content/uploads/2011/10/microsoft-resells-free-internet-services.png\" alt=\"\" title=\"microsoft-resells-free-internet-services\" width=\"625\" height=\"420\" class=\"aligncenter size-full wp-image-1442\" />\n\n<p>\n<strong>6 out of 8 services above are FREE Internet services!</strong>\n</p>\n<p>\nIf I want to use those free Internet services on my TV, I'd rather use my media center PC anyways. I have a \n[linkpeek-hover uri=\"http://www.foxhop.net/nT330i\" size=\"220x400\" text=\"very tiny ubuntu media center pc\"]\n\nwhich is amazing and near silent. \n</p>\n<p>\nMicrosoft prevents subscribers from cancelling their service over the web, and thus they forced me to call their service center.  After waiting 5 minutes I finally spoke to a sales representative who seemed friendly and understanding.  \n</p>\n<p>\nShe asked me why I wanted to cancel automatic payments.  I explained \"I want to pay month to month\".  After listening to my position the sales representative offered <strong>1 month of Xbox Live for $1.00.</strong>  So I'll have Xbox live until December 2011 after all.\n</p>\n<p>\nMicrosoft, give your users a web browser on the Xbox and stop trying to sell subscriptions to Free Internet services.\n</p>\n<p>\n<strong><em>PlayStation 3 launched with a web browser and a FREE network.</em></strong>\n</p>\n\n<p><strong>UPDATE:</strong> If you opt into the $1 for one month deal, you are also opting back into the automatic renewal program...  Even if the person on the phone tells you otherwise.\n\nCall us directly by dialing:\n\n<pre>Toll free: \n(800) 4MY-XBOX or (800) 469-9269\nHearing impaired (TDD device): \n(866) 740-9269 or (425) 635-7102\n9 am to 1 am Eastern Time \n6 am to 10 pm Pacific Time</pre>",
    "date": "2011-10-15 19:26:33",
    "timestamp": 1318721193,
    "comments": [],
    "metadata": {
      "_edit_last": "1",
      "_spost_short_title": null
    }
  },
  "r8168-driver-issues-after-ubuntu-11-10-upgrade-kernel-linux-3-0": {
    "name": "r8168-driver-issues-after-ubuntu-11-10-upgrade-kernel-linux-3-0",
    "id": "1414",
    "link": "http://russell2.ballestrini.net/r8168-driver-issues-after-ubuntu-11-10-upgrade-kernel-linux-3-0/",
    "title": "r8168 driver issues after Ubuntu 11.10 upgrade kernel linux 3.0",
    "content": "<p>\n<strong>I had network issues after upgrading to Ubuntu 11.10 which has the linux 3.0 kernel.  \n</strong>\n</p>\n<p>\n\n</p>\n\n<p>\nI had to compile the r8168 from source following \n[linkpeek-hover uri=\"http://www.foxhop.net/realtek-dropping-packets-on-linux-ubuntu-and-fedora\" size=\"504x504\" text=\"this guide\"] - (linkpeek.com instant preview service!)\n</p>\n<p>\nhowever I needed to alter the Makefile to include support for linux 3.0 kernel.\n</p>\n<br/>\n<strong>edit src/Makefile:</strong>\n<pre>\n#KEXT  := $(shell echo $(KVER) | sed -ne 's/^2\\.[567]\\..*/k/p')o\nKEXT := $(shell echo $(KVER) | sed -ne 's/^[23]\\.[1-9]\\..*/k/p')o-\n</pre>\n<p>\n<strong>Now you should follow the rest of the guide.\n</strong>\n </p>",
    "date": "2011-10-15 11:15:52",
    "timestamp": 1318691752,
    "comments": [],
    "metadata": {
      "_edit_last": "1",
      "_spost_short_title": null
    }
  },
  "cell-shading-practice-a-sketched-skull-and-wind-mill-photograph": {
    "name": "cell-shading-practice-a-sketched-skull-and-wind-mill-photograph",
    "id": "1478",
    "link": "http://russell2.ballestrini.net/cell-shading-practice-a-sketched-skull-and-wind-mill-photograph/",
    "title": "Cell shading practice, A sketched skull, and an old wind mill photograph",
    "content": "<p>\n<strong>Some of my resent works using my Ubuntu + Wacom Bamboo + MyPaint + Gimp Workflow!\n</strong>\n</p>\n<p>\nFirst some cell shading practice.  Inside MyPaint I only used two colors in my palate but to gain shadow and depth I used variations of opacity.  This technique produces a neat cartoon effect.\n</p>\n\n<img src=\"/wp-content/uploads/2011/10/smile-guy.jpg\" alt=\"\" title=\"smile-guy\" width=\"682\" height=\"817\" class=\"aligncenter\" style=\"box-shadow: none; -webkit-box-shadow: none; -moz-box-shadow: none;\" />\n\n<p>\nNext I sketched a skull.  I attempted to keep all my strokes in the same direction.  This piece is monotone.  I used only one color and deviated each layer with different opacity settings.\n</p>\n\n<img src=\"/wp-content/uploads/2011/10/skull.jpg\" alt=\"\" title=\"skull\" width=\"420\" height=\"664\" class=\"aligncenter\"  style=\"box-shadow: none; -webkit-box-shadow: none; -moz-box-shadow: none;\" />\n\n<p>\nLast is a photograph I snapped during a family outing.  I had to lower the quality of this image to prepare for use on the web, but I still adore the colors spectrum of this shot.  The white birch tree frames the right side while the grass and brush frame the bottom.  The wind mill was intentionally placed in the center with the apex extending upward.  The ivy creeps up the structure and the cloud filled sky appears to stretch forever.  The foliage emits a euphoric glow under the sun saturated rays.\n</p>\n\n<img src=\"/wp-content/uploads/2011/10/wind.jpg\" title=\"wind\" width=\"750\" height=\"1000\" class=\"aligncenter\" />\n\n<strong>Come back soon!</strong>",
    "date": "2011-10-16 12:56:49",
    "timestamp": 1318784209,
    "comments": [],
    "metadata": {
      "_edit_last": "1",
      "_wp_old_slug": "mypaint-wacom-sketched-skull-and-cell-shading-practice",
      "_spost_short_title": null
    }
  },
  "adding-inline-image-support-to-a-gmail-messages": {
    "name": "adding-inline-image-support-to-a-gmail-messages",
    "id": "1538",
    "link": "http://russell2.ballestrini.net/adding-inline-image-support-to-a-gmail-messages/",
    "title": "Adding inline image support to a gmail messages",
    "content": "<br>\n\nEnable ability to insert images into a message body. You can upload and insert image files in your computer, or insert images by URLs. This lab will not work if you have offline enabled.\n\n<ol>\n\t<li>\nGo to google labs: <a href=\"https://mail.google.com/mail/#settings/labs\" target=\"_blank\">https://mail.google.com/mail/#settings/labs</a>\n</li>\n\t<li>\nSearch \"images\".\n</li>\n\t<li>\n<em>Enable</em> \"Inserting images - by Kent T\"\n</li>\n</ol>\n\n<strong>Welcome!\n</strong>",
    "date": "2011-10-20 17:46:15",
    "timestamp": 1319147175,
    "comments": [],
    "metadata": {
      "_edit_last": "1",
      "_wp_old_slug": "adding-inline-images-support-to-a-gmail-message"
    }
  },
  "occupy-wall-street-stack-vs-queue": {
    "name": "occupy-wall-street-stack-vs-queue",
    "id": "1545",
    "link": "http://russell2.ballestrini.net/occupy-wall-street-stack-vs-queue/",
    "title": "Occupy Wall Street Stack vs Queue",
    "content": "<p>\n<strong>Occupy Wall Street contributors claim to use a \"stack\" to determine speaking arrangements.</strong>\n</p>\n<p>\nI plan to explain how the term \"stack\" used in this scenario does not align itself with the mathematical or computer science definition.\n</p>\n<p>\nThe term stack means First In Last Out or \"FILO\".  For example: a person placed on the stack in the morning would be the last to speak at the end of the day.  This isn't happening like that...\n</p>\n<p>\nOccupy Wall Street compatriots really use a technique called \"queue\".  Mathematicians and Computer Scientists define a queue as First In First Out or \"FIFO\".  A real world example of a queue would be a line at the grocery store.  The first person in line is the first person to leave the line.  \n</p>\n<ul>\n\t<li>An object enters a stack from the rear and exits the rear.</li>\n\t<li>An object enters a queue from the rear and exits from the front.</li>\n</ul>\n<br/>\n<img src=\"/wp-content/uploads/2011/10/stack-vs-queue.png\" alt=\"\" title=\"stack-vs-queue\" width=\"640\" height=\"400\" class=\"aligncenter size-full wp-image-1559\" />",
    "date": "2011-10-21 13:31:50",
    "timestamp": 1319218310,
    "comments": [],
    "metadata": {}
  },
  "webmaster-tools-alerted-issue-turned-out-pylon-session-files-flooded-inodes": {
    "name": "webmaster-tools-alerted-issue-turned-out-pylon-session-files-flooded-inodes",
    "id": "1590",
    "link": "http://russell2.ballestrini.net/webmaster-tools-alerted-issue-turned-out-pylon-session-files-flooded-inodes/",
    "title": "Webmaster tools alerted issue turned out Pylon session files flooded inodes",
    "content": "<strong>This graph could happen to you if you ever forget to configure munin email alerting: </strong>\n\n<img src=\"/wp-content/uploads/2011/10/df_inode-year.png\" alt=\"\" title=\"df_inode-year\" width=\"497\" height=\"347\" class=\"aligncenter size-full wp-image-1591\" />\n\nIt only took approximately 1 hour to diagnoses and resolve this issue however most of my web applications hosted on this server were down for about 11 hours.  I was lucky that this outage fell on a weekend otherwise I would not have known about the problem till around 6:30pm!\n\n<img src=\"/wp-content/uploads/2011/10/df_inode-day.png\" alt=\"\" title=\"df_inode-day\" width=\"497\" height=\"347\" class=\"aligncenter size-full wp-image-1592\" />\n\n<strong>Diagnosis:</strong>\n<p>\nTwo of my pylons apps had session files that slowly ran away on me.  The session files don't consume much capacity however the shear quantity of them caused my inode usage to hit 100%.\n</p>\n<p>\nHad I properly configured Munin's email alerting this issue would have been identified well before it was a problem.\n</p>\n<p>\nWant to know what alerted me to the problem?  G Webmaster's tools claimed it could not read my robots.txt on a few of my sites... After investigating I learned the site was down.  Checking the Apache error logs pointed me to disk space issues.  <code>df -ha</code> reported everything was fine, however <code>df -hi</code> reported 100% inode usage!  At this point I started looking to cache and log locations to find lots of files, which lead me to my pylons web applications data/sessions directories.\n</p>\n<p>\n<strong>Resolution:</strong>\n</p>\n<p>\nDelete the session cache tree directories and allow the applications to rebuild them.\n</p>\n<p>\nTodo: move /www off the root disk partition.  This issue could have been much worse if I was unable to boot or login to remedy.  Moving /www off root should prevent the web server from effecting the systems ability to boot.\n</p>",
    "date": "2011-10-29 15:30:36",
    "timestamp": 1319916636,
    "comments": [],
    "metadata": {}
  },
  "im-petrified-of-launching-my-web-application": {
    "name": "im-petrified-of-launching-my-web-application",
    "id": "1606",
    "link": "http://russell2.ballestrini.net/im-petrified-of-launching-my-web-application/",
    "title": "I'm petrified of launching my web application",
    "content": "<br/>\n<strong>I'm petrified of launching my web application because I'm fearful that I won't ...</strong>\n\n<ul>\n\t<li>acquire users</li>\n\t<li>support my users well</li>\n\t<li>scale in a timely manner</li>\n\t<li>react quickly to feedback</li>\n\t<li>monetize the application</li>\n</ul>\n\nBut most of all I'm scared that nobody will like me.  I'm scared of failure.\n\n<p>\n\n<strong>\nNow that I got that out of my system please check out \n[linkpeek-hover uri=\"http://linkpeek.com\"].\n</strong>",
    "date": "2011-11-03 21:35:07",
    "timestamp": 1320370507,
    "comments": [],
    "metadata": {}
  },
  "career-development-is-a-game-of-chutes-and-ladders": {
    "name": "career-development-is-a-game-of-chutes-and-ladders",
    "id": "1755",
    "link": "http://russell2.ballestrini.net/career-development-is-a-game-of-chutes-and-ladders/",
    "title": "Career development is a game of chutes and ladders",
    "content": "<p>\nIf career development was a game of chutes and ladders, job networking would be the ladder.  They provide a shortcut to the top, a direct route to win your dream job.\n</p>\n\n<img src=\"/wp-content/uploads/2011/11/job-networking-chutes-and-ladders.gif\" alt=\"\" title=\"job-networking-chutes-and-ladders\" width=\"200\" height=\"203\" class=\"alignright size-full wp-image-1764\" />\n\n<p>\nAt work today, a colleague was reviewing resumes for an open requisition within the unix group.  I decided later that night to clean up <a href=\"http://russell.ballestrini.net/wp-content/uploads/2011/10/Russell-Ballestrini-Resume.pdf\">my resume</a> to make it more relevant.  I felt like I did something positive for my career.  After coming down from the high of resume writing I began to question rational.  I then came to the contradictory conclusion that a great resume holds less importance than a mediocre recommendation.\n</p>\n\n<p>It is better for an employer to learn about you from a recommendation then your resume.  Why?  Hiring new people holds risk.  A hiring manager will reduce risk by promoting from within or using personal recommendations.  In both cases the resume becomes a document of formality instead of a document of credentials. \"Its not what you know, its who you know.\"  \n</p>\n\n<p>\nWhat can we assume about the position they are extending to the public?  We can safely assume that the manager has already hired somebody for the high risk, enjoyable position and is now looking for a bottom feeder with a resume to fill the newly vacant position.  Keep this in mind the next time a head hunter sends you a job proposition email.\n</p>\n\n<p>\nNormally taking shortcuts goes against conventional wisdom for success.  Job networking however, allows candidates to skip ahead and arrive at their dream occupation more directly.  Meeting new people seems like the single best way to land a job doing what you love.  \n</p>\n\n<p>\n<strong>Get out there and meet people with similar interests!\n</strong></p>",
    "date": "2011-11-29 20:42:46",
    "timestamp": 1322617366,
    "comments": [],
    "metadata": {}
  },
  "how-to-incorporate-custom-configuration-in-a-pyramid-application": {
    "name": "how-to-incorporate-custom-configuration-in-a-pyramid-application",
    "id": "1788",
    "link": "http://russell2.ballestrini.net/how-to-incorporate-custom-configuration-in-a-pyramid-application/",
    "title": "How to Incorporate Custom Configuration in a Pyramid Application",
    "content": "<p>\nImagine that you have just built a wiki, blog, or cms web application that will be deployed multiple times by different people.  You would like to provide the ability to configure some aspects of the program without the end user altering your python or template code.  This guide explains how to incorporate custom configuration from the project's .ini file with the rest of the application. \n</p>\n\n<p>\nTo explain this process we will add a customizable Google Analytics key to our project.\n</p>\n\n<ol>\n\t<li>Make a configuration key/value pair for Google Analytics</li>\n\t<li>Add the Google Analytics javascript code to the template</li>\n</ol>\n\n<p>\n<strong>Make a configuration key/value pair for Google Analytics</strong>\n</p>\n\n<p>\nInside production.ini place the following in the <code>[app:main]</code> section:\n</p>\n\n<pre>#google_analytics_key = UA-55555555-1\ngoogle_analytics_key =\n</pre>\n\n\n<strong>Add the Google Analytics javascript code to the template</strong>\n</p>\n\n<p>\nIn this case I will show an example in mako.  Other template solutions should look similar.\n</p>\n\n<xmp>\n<%def name=\"google_analytics()\">\n    % if request.registry.settings['google_analytics_key']:\n      <script type=\"text/javascript\">\n\n          var _gaq = _gaq || [];\n          _gaq.push(['_setAccount', \"${request.registry.settings['google_analytics_key']}\"]);\n          _gaq.push(['_trackPageview']);\n\n          (function() {\n          var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;\n          ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';\n          var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);\n          })();\n\n      </script>\n    % endif\n</%def>\n</xmp>\n\n<p>\nThen in the head section call the function:\n</p>\n\n<pre>${ google_analytics() }\n</pre>\n\n<p>\nThat was easy because the configuration string just needed to be substituted into the JavaScript in the template.  What if you needed to do something with the provided key/value before using it?  Next I will show you a method for building renderer globals again showing a different way to configure Google Analytics:\n</p>\n\n<p>\n<strong>Make an inject_renderer_globals subscriber</strong>\n</p>\n\n<p>\nDefine <code>inject_renderer_globals(event)</code> function in the project's <code>__init__.py</code> file.\n</p>\n\n<p>\nI normally place it at the bottom of the file and it looks like this:\n</p>\n\n<pre>\ndef inject_renderer_globals(event):\n    \"\"\"Inject some renderer globals before passing to template\"\"\"\n\n    request = event['request']\n   \n    # Build ${google_analytics_key} from the configuration file  \n    event['google_analytics_key'] = request.registry.settings[ 'google_analytics_key' ]\n</pre>\n\n<p>\nImport BeforeRender at the top of the <code>__init__.py</code>:\n</p>\n\n<pre>from pyramid.events import BeforeRender</pre>\n\n<p>\nIn the main function, add the <code>inject_renderer_globals</code> to the subscribers:\n</p>\n\n<pre>config.add_subscriber(inject_renderer_globals, BeforeRender)</pre>\n\n<p>\nNow you can use <code>${google_analytics_key}</code> anywhere in your template.\n</p>\n\n<strong>Thank you for reading, and feel free to leave comments</strong>",
    "date": "2011-11-30 22:47:51",
    "timestamp": 1322711271,
    "comments": [
      {
        "id": 139207,
        "parent_id": 0,
        "author_ip": "127.0.0.1",
        "author": "sbivol",
        "email": "sergiu@cip.md",
        "content": "Thank you very much for this example!\nAfter implementing your solution, I finally started to understand the Pyramid documentation on this subject.",
        "date": "2015-04-07 03:34:44",
        "timestamp": 1428392084
      }
    ],
    "metadata": {
      "_edit_last": "1",
      "_wp_old_slug": "incorporate-google-analytics-into-a-pyramid-application",
      "_spost_short_title": null,
      "_stcr@_sergiu@cip.md": "2015-04-07 03:34:44|Y"
    }
  },
  "linkpeek-com-number-one-on-hacker-news": {
    "name": "linkpeek-com-number-one-on-hacker-news",
    "id": "1817",
    "link": "http://russell2.ballestrini.net/linkpeek-com-number-one-on-hacker-news/",
    "title": "LinkPeek.com Number One on Hacker News",
    "content": "<br/>\n<strong>We made the number one spot on Hacker News.\n</strong>\n<br/><br/>\nLinkPeek was used to take a snapshot of the event:\n<br/><br/>\n<img src=\"/wp-content/uploads/2011/12/linkpeek-number-1-on-hacker-news.png\" alt=\"\" title=\"linkpeek-number-1-on-hacker-news\" width=\"800\" height=\"1110\" class=\"aligncenter size-full wp-image-1818\" />",
    "date": "2011-12-05 00:39:00",
    "timestamp": 1323063540,
    "comments": [],
    "metadata": {}
  },
  "flash-mob-office-meeting-definition": {
    "name": "flash-mob-office-meeting-definition",
    "id": "1824",
    "link": "http://russell2.ballestrini.net/flash-mob-office-meeting-definition/",
    "title": "flash mob office meeting definition",
    "content": "<p>\n<strong>Flash Meeting</strong>\n</p>\n\n<p>\n  In a office or cubicle environment a group of uninvited people gather and hover around your desk to talk to you.  A meeting forms in immaculate conception as you sit bewildered at your desk.\n</p>\n\n<p>\n<strong>Other names:</strong> <em>Flash Meeting</em>, <em>Flash Mob Meeting</em>, <em>Flash Office Meeting</em>\n</p>",
    "date": "2011-12-12 15:34:48",
    "timestamp": 1323722088,
    "comments": [],
    "metadata": {}
  },
  "linkpeek-com-webpage-to-image-was-a-by-product": {
    "name": "linkpeek-com-webpage-to-image-was-a-by-product",
    "id": "1839",
    "link": "http://russell2.ballestrini.net/linkpeek-com-webpage-to-image-was-a-by-product/",
    "title": "LinkPeek.com, webpage to image, was a by-product",
    "content": "<p>\n<strong>tldr; </strong>When faced with pivoting or killing a project, take a good look at all possible by-products.  Don't miss the hidden gem in a project's slag!\n</p>\n\n<p>\nLast year I built yoursitemakesmebarf.com, a novelty web application which allowed anonymous link submission.  The software would automatically take <a href=\"http://russell.ballestrini.net/linkpeek-com-web-address-thumbnail-api-alpha-release/\" title=\"LinkPeek.com web address thumbnail api alpha release\">screenshots</a> of submitted links and curate a blog.  I enjoyed building the site and the project served as my first Pyramid application.\n</p>\n\n<p>\nThe project's original intent was to jokingly poke fun at ugly design.  The idea never caught on.  Instead the application angered website owners and attracted undesirable people.  Eventually, I decided to take it down  and come up with less combative idea.\n</p>\n\n\n<p>\nAfter witnessing the Goog release of \"instant previews\",  I knew there was a market for a fast and reliable web screen shot service.\n</p>\n\n[linkpeek-image uri=\"http://linkpeek.com\" style=\"float: left; margin-right: 25px;\" size=\"200x180\"]\n\n<p>\nSo I decided to bring instant previews to anyone who needed them.  I wanted to build a fast, flexible, and easy to use screenshot API.  After a few months I had a working prototype.  I named the product LinkPeek because it described what the service was, and the domain was available.  \n</p>\n\n<p>\nNext I built a website thumbnail generator to show off the software.  The generator application helped reinforce the simplicity of the underlying LinkPeek API.  It didn't require any downloading, installation, or waiting. \n</p>\n\n<p>\nAbout a week later on a whim I posted the generator to hacker news.<br/>  I gave an honest title: [linkpeek-hover uri=\"http://linkpeek.com/website-thumbnail-generator\" text=\"Convert Any Webpage to an Image\"] and within about about 30 minutes LinkPeek.com was placed on the front page in the number 1 spot.\n</p>\n\n<strong>Remember, when faced with pivoting or killing a project, take a good look at all possible by-products.   Don't miss the hidden gem in a project's slag!</strong>",
    "date": "2011-12-19 00:23:08",
    "timestamp": 1324272188,
    "comments": [],
    "metadata": {
      "_edit_last": "1",
      "_spost_short_title": null
    }
  },
  "today-i-lost-a-customer": {
    "name": "today-i-lost-a-customer",
    "id": "2076",
    "link": "http://russell2.ballestrini.net/today-i-lost-a-customer/",
    "title": "Today I lost a customer",
    "content": "<p>\nToday I lost a customer.  I added some new code to [linkpeek-hover uri=\"http://linkpeek.com/website-thumbnail-generator\" text=\"LinkPeek.com\"] to accept coupons and I didn't think of an edge case.  This ended up creating an uncaught exception in my server side code which ultimatly served the newly subscribing customer an HTTP 500 error page.  The damage was done.\n</p>\n\n<p>\nThis error was catastrophic and ultimately killed the conversion.  Here is an excerpt of how the user felt after the experience:\n</p>\n<p>\n<blockquote>At this point the website has failed spectacularly enough that I can no longer trust you with my business. Please void the charge on my American Express card before it's processed. I need to hear from you ASAP.</blockquote>\n</p>\n\n<p>\nCustomers and prospects are forgiving for normal bugs in software.  However, customers are intolerant to bugs in the sign up or payment flow.  An error in payment flow will cause friction and friction will kill the sale.\n</p>\n\n<p>\nRunning a start up is hard work, and negative (but justified) feedback hurts more then I thought it would.  This email made me feel awful.\n</p>\n\n<p>\nFortunately I learned from this mistake and the user's card was never charged.  \n</p>\n\n<p>\nAlways make sure to extensively test payment and sign up code.  Try all the edge cases.  Try to make your software break.  Don't inflict friction on your potential customers.\n</p>",
    "date": "2012-01-18 23:30:55",
    "timestamp": 1326947455,
    "comments": [],
    "metadata": {}
  },
  "what-are-the-differences-between-message-confidentiality-and-message-integrity": {
    "name": "what-are-the-differences-between-message-confidentiality-and-message-integrity",
    "id": "2090",
    "link": "http://russell2.ballestrini.net/what-are-the-differences-between-message-confidentiality-and-message-integrity/",
    "title": "What are the differences between message confidentiality and message integrity",
    "content": "<BR/>\n\n<strong>\nWhat are the differences between message confidentiality and message integrity?  Can you have confidentiality without integrity? Can you have integrity without confidentiality?\n</strong>\n\n<BR/>\n<BR/>\n\n<DL>\n  <DT>message confidentiality</DT>\n  <DD>Two or more hosts communicate securely, typically using encryption.  The communication cannot be monitored (sniffed) by untrusted hosts.  The communication between trusted parties is confidential.</DD>\n\n  <DT>message integrity</DT>\n  <DD>The message transported has not been tampered with or altered.  A message has integrity when the payload sent is the same as the payload received.</DD>\n</DL>\n\n<p>\nSending a message confidentially does not guarantee data integrity.  Even when two nodes have authenticated each other, the integrity of a message could be compromised during the transmission of a message.\n</p>\n\n<p>\nYes, you can have integrity of a message without confidentiality.  One can take a hash or sum of the message on both sides to compare.  Often we share downloadable files and provide data integrity using md5 hash sums.\n</p>",
    "date": "2012-02-08 17:47:48",
    "timestamp": 1328741268,
    "comments": [
      {
        "id": 15775,
        "parent_id": 0,
        "author_ip": "127.0.0.1",
        "author": "tharibdudoo",
        "email": "thdoo@yahoo.com",
        "content": "This article is really helped me to understand the different between integrity and confidentiality",
        "date": "2012-12-29 05:22:08",
        "timestamp": 1356776528
      }
    ],
    "metadata": {
      "_series_part": "1",
      "_edit_last": "1",
      "_spost_short_title": null
    }
  },
  "reasons-why-some-internet-entities-might-want-secure-communication": {
    "name": "reasons-why-some-internet-entities-might-want-secure-communication",
    "id": "2102",
    "link": "http://russell2.ballestrini.net/reasons-why-some-internet-entities-might-want-secure-communication/",
    "title": "Reasons why some Internet entities might want secure communication",
    "content": "<BR/>\n<strong>Internet entities often need to communicate securely.</strong>\n<BR/>\n<BR/>\n\n\nHere are some reasons why some Internet entities might want secure communication:\n\n<ol>\n\t<li><strong>Web Servers:</strong> Communication on the Internet, or any network for that matter, should be encrypted before transmitting sensitive data.  This will help prevent snooping from unauthorized parties.  Most often SSL or HTTPS may be used to create a secure communication \u201ctunnel\u201d between a web server and a web client (browser).   </li>\n\n\t<li> <strong> Server Administration:</strong>   </li> A secure protocal should always be used when administrating a server or remote computer.  Typically SSH (Secure Shell) is used.</li>\n\n<li><strong>DNS Servers:</strong> Using DNSSEC could help prevent DNS poisoning and certifies DNS data. DNS was first conceived as a distributed and highly scalable address lookup system.  Security was not its top priority.  Since then we have new DNSSEC extensions which allow for origin authentication of DNS data, authenticated denial of existence, and data integrity.  DNSSEC does not attempt to solve availability and confidentiality. </li>\n\n\n<ol>",
    "date": "2012-02-08 18:23:07",
    "timestamp": 1328743387,
    "comments": [],
    "metadata": {
      "_edit_last": "1",
      "_series_part": "2"
    }
  },
  "attributes-of-an-8-block-cipher": {
    "name": "attributes-of-an-8-block-cipher",
    "id": "2119",
    "link": "http://russell2.ballestrini.net/attributes-of-an-8-block-cipher/",
    "title": "Attributes of an 8-block cipher",
    "content": "<strong>Consider an 8-block cipher and answer the following:\n</strong><p>\nHow many possible input blocks does this cipher have? How many possible mappings are there? If we view each mapping as a key, then how many possible keys does this cipher have?\n<p/>\n\n<p>\nTo find the input blocks of this cipher we raise 2 to the 8th power.  2^8 = 256 possible inputs.\n<p/>\n\n<p>\nTo find the number of possible mappings we take the 256 input blocks and find it's factorial.  There are 256! possible mappings.\n<p/>\n\n<p>\nWe can view each of these mappings as a key, so this cipher has 256! keys.\n<p/>",
    "date": "2012-02-13 00:50:05",
    "timestamp": 1329112205,
    "comments": [],
    "metadata": {
      "_edit_last": "1",
      "_series_part": "3"
    }
  },
  "symmetric-encryption-vs-public-key-encryption": {
    "name": "symmetric-encryption-vs-public-key-encryption",
    "id": "2126",
    "link": "http://russell2.ballestrini.net/symmetric-encryption-vs-public-key-encryption/",
    "title": "Symmetric Encryption vs Public Key Encryption",
    "content": "<strong>How many keys are involved for symmetric key encryption? How about public key encryption?</strong>\n\n<p>\nSuppose you have N people who want to communicate with each other using symmetric keys.  All communication between any two people, i and j, is visible to group N.  Only person i and person j can decrypt each others messages.  \n</p>\n\n<p>\n<strong>How many keys would Symmetric Encryption require to protect group N?</strong>\n</p>\n\n<p>\nI solved this with the following python function:\n\n<pre>\ndef count_symmetric_keys( N=2 ):\n    \"\"\"Provide the number of entities in group N.\n    return the number of symmetric keys needed for this group\"\"\"\n    keys = 0\n    for i in range( 0, N ): keys += i\n    return keys\n</pre> \n\nA reader suggested the following optimized formula:\n<pre>\ndef calc_symmetric_keys( N=2 ):\n    \"\"\"Provide the number of entities in group N.\n    return the number of symmetric keys needed for this group\"\"\"\n    return N*(N-1)/2\n</pre> \n\n</p>\n\n<p>\nIf group N had 10 members, it would need to generate and maintain 45 Symmetric Keys.\n</p>\n<p>\nIf group N had 50 members, it would need to generate and maintain 1225 Symmetric Keys.\n</p>\n<p>\nSymmetric keys are also susceptible to man-in-the-middle attacks.  This attack occurs when an entity poses as a trusted entity.  Let i and j be trusted entities.  Let k be an untrusted attacker.  If k determined the Symmetric key it could send or receive messages posing as i or j.\n</p>    \n<p>\n<strong>How many keys would Public-key Encryption require to protect group N?</strong>\n</p>\n<p>\nPublic Key Encryption requires 2n keys or two keys per person in group N.  Public key encryption also does not require 'pre sharing' the secret key before communication may start.  Each member would need 1 public key and 1 private key.\n</p>\n<p>\nIf group N had 10 members, it would need to generate and maintain 20 Public/Private Keys.\n</p>\n<p>\nIf group N had 50 members, it would need to generate and maintain 100 Public/Private Keys.\n</p>",
    "date": "2012-02-13 17:25:44",
    "timestamp": 1329171944,
    "comments": [
      {
        "id": 1304,
        "parent_id": 0,
        "author_ip": "125.17.11.118",
        "author": "james",
        "email": "james_9912345@yahoo.com",
        "content": "Good to know about the Symmetric Encryption vs Public Key Encryption",
        "date": "2012-02-14 06:05:45",
        "timestamp": 1329217545
      },
      {
        "id": 15910,
        "parent_id": 15908,
        "author_ip": "127.0.0.1",
        "author": "Russell Ballestrini",
        "email": "russell.ballestrini@gmail.com",
        "content": "Yeah, it is a typo, the function I provided works.  However the formula you give is a better solution because it O(1) instead of O(n) so I'm going to update the blog post with your corrections.",
        "date": "2013-06-20 22:58:24",
        "timestamp": 1371783504
      },
      {
        "id": 15908,
        "parent_id": 0,
        "author_ip": "127.0.0.1",
        "author": "NetEng",
        "email": "vasser@desert.com",
        "content": "Looks like you made a typo or Python calculated it wrong:\n\nn(n-1)/2= keys needed\n\n50(50-1)/2= 1,225 symmetric keys needed.\n\nI thought I didn't have the formula correct when I first saw your article, but I double checked it. Thanks for helping me learn that formula even better!  ;)",
        "date": "2013-06-20 12:18:40",
        "timestamp": 1371745120
      }
    ],
    "metadata": {
      "_series_part": "4",
      "_edit_last": "1",
      "_spost_short_title": null
    }
  },
  "why-does-a-hash-provide-better-message-integrity-then-an-internet-checksum": {
    "name": "why-does-a-hash-provide-better-message-integrity-then-an-internet-checksum",
    "id": "2142",
    "link": "http://russell2.ballestrini.net/why-does-a-hash-provide-better-message-integrity-then-an-internet-checksum/",
    "title": "Why does a Hash provide better message integrity then an Internet checksum?",
    "content": "<p>\n<strong>Why does a Hash provide better message integrity then an Internet checksum?</strong>\n</p>\n\n<p>\nHash function and checksum function both return a value which cannot be reversed.\n</p>\n\n<p>\nAn Internet checksum (TCP checksum or IP checksum) is designed to detect common errors quickly and efficiently.  An Internet checksum does not attempt to prevent collisions.\n<em>Man cksum for more info.</em>\n</p>\n\n<p>\nA Hash provides better message integrity because it has less collisions then an Internet checksum. A collision means there is more then one way to produce the same sum.  A great hash function aims to reduce the occurrence of collisions. <em>Man md5 and sha for more info.</em>\n</p>\n\n<strong>What is a collision</strong>\n<p>\nLet H() be a hash function.  Let x and y be two differing messages.\n<em>H(x) = H(y)</em> would be a collision.\n</p>\n\n<p>\n<strong>I like to use python to show examples of hash functions</strong>\n</p>\n\n<p>\nIn this example I pass a message into the MD5 hash function to produce a resulting hash of the message.  You can think of this hashed output as a finger print of the message.\n</p>\n\n<p>\n<pre>\nimport hashlib as h\nmessage = \"This message will be placed into an MD5 hash function to authenticate its integrity.\"\nprint h.md5(message).hexdigest()\n</pre>\n</p>\n\n<p>\n<em>Hash Output:</em><BR/>\n<pre>18f189f94b245ad8566206c199b4f60a</pre>\n</p>\n\n<p>\nNow If I passed that message to you along with its MD5 hash hex representation, you could put the message into your own MD5 hash function and compare the resulting hash.  This method is used to validate the message or verify data integrity.</p>\n\n<p>\n<strong>Can you \"decrypt\" a hash of a message to get the original message</strong>\n\n<p>No! A hash may <em>not</em> be reversed, which means it cannot be decrypted.</p>\n\n</p>\n<p>\nBy design a hash algorithm has no inverse, there is no way to get the original message from the hash.  This is good news, turns out we have some really great applications for this type of function.  We can validate messages, we can securely store passwords, and we can quickly determine if a message or file has been tampered with.\n</p>\n\n<p>When using a publicly known hash function for storing password hashes, make sure to always use a salt or shared secret.  Failure to do so will make your storage scheme susceptible to a rainbow table attack.  A rainbow table allows a cracker to quickly match a list of hashes with a table of previously computed hash values and correlated passwords.</p>\n\n<p>\n<strong>What is salt or a shared secret?</strong>\n</p>\n\n<p>You can use salt or a shared secret to add extra data to a message before hashing with a publicly known algorithm. Below I will document how to properly add salt to a message before generating a SHA256 hash.</p>\n\n<p>\n<pre>\nimport hashlib as h\nmessage = \"This message and some salt will be hashed with SHA 256.\"\nsalt = \"This is some secret salt data\"\nprint h.sha256(message+salt).hexdigest()\n</pre>\n</p>\n\n<p>\n<em>Hash Output:</em><BR/>\n<pre>5e8d86bab9604620f19cfbc5f836f47feb9e8c9e74264fff1f4938bdaab1eeaa</pre>\n</p>\n\n<p>Adding a salt to the message allows us to use a publicly know algorithm in a more protected manner.</p>\n\n<p>\n<strong>Can you spot the error in the python code below?</strong>\n</p>\n\n<p>\n<pre>\nimport hashlib as h\nmessage = \"This message and some salt will be hashed with SHA 256.\"\nsalt = \"This is some secret salt data\"\nprint h.sha256(message).hexdigest()+salt\n</pre>\n</p>\n\n<p>\nIf you guessed that the message and salt BOTH need to be hashed together then you are correct!\n</p>\n<p>\nThe above code would have produced the following invalid hash:\n</p>\n\n<p>\n<em>Hash Output:</em><BR/>\n<pre>79cd4bfa1bcb71a7a1b5bfd5e8cfc8368a6cc6cb836d24bf04f2ef2bd0e81261This is some secret salt data</pre>\n</p>\n\n<p>\n<strong>You should follow me on twitter <a href=\"https://twitter.com/#!/RussellBal\">here</a>.</strong>\n</p>",
    "date": "2012-02-13 19:19:08",
    "timestamp": 1329178748,
    "comments": [
      {
        "id": 15867,
        "parent_id": 0,
        "author_ip": "127.0.0.1",
        "author": "homa",
        "email": "homa60ir@yahoo.com",
        "content": "hi thank u for your info, i have question what s difference between hash, checksum and also parity?\nthank u",
        "date": "2013-05-15 08:42:02",
        "timestamp": 1368621722
      }
    ],
    "metadata": {
      "_series_part": "5",
      "_edit_last": "1",
      "_spost_short_title": null
    }
  },
  "monoalphabetic-cipher-and-inverse-written-in-python": {
    "name": "monoalphabetic-cipher-and-inverse-written-in-python",
    "id": "2180",
    "link": "http://russell2.ballestrini.net/monoalphabetic-cipher-and-inverse-written-in-python/",
    "title": "Monoalphabetic Cipher and Inverse Written in Python",
    "content": "<BR/>\n<strong>Here is my implementation of a Monoalphabetic Cipher written with a python dictionary:</strong>\n\n<pre>\nmonoalpha = {\n    'a': 'm',\n    'b': 'n',\n    'c': 'b',\n    'd': 'v',\n    'e': 'c',\n    'f': 'x',\n    'g': 'z',\n    'h': 'a',\n    'i': 's',\n    'j': 'd',\n    'k': 'f',\n    'l': 'g',\n    'm': 'h',\n    'n': 'j',\n    'o': 'k',\n    'p': 'l',\n    'q': 'p',\n    'r': 'o',\n    's': 'i',\n    't': 'u',\n    'u': 'y',\n    'v': 't',\n    'w': 'r',\n    'x': 'e',\n    'y': 'w',\n    'z': 'q',\n    ' ': ' ',\n}\n\ninverse_monoalpha = {}\nfor key, value in monoalpha.iteritems():\n    inverse_monoalpha[value] = key\n\nmessage = \"This is an easy problem\"\nencrypted_message = []\nfor letter in message:\n    encrypted_message.append( monoalpha[letter.lower()] )\n\nprint ''.join( encrypted_message )\n</pre>\n\n<strong>The encrypted output:\n</strong> <code>uasi si mj cmiw lokngch</code>\n\n<p>\n<strong>Now we may use the inverse cipher to decrypt a message, \"rmij'u uamu xyj\"</strong>\n</p>\n\n<pre>\nencrypted_message = \"rmij'u uamu xyj\"\ndecrypted_message = []\nfor letter in encrypted_message:\n    try:\n        decrypted_message.append( inverse_monoalpha[letter] )\n    except KeyError:\n        decrypted_message.append( letter )\n\nprint ''.join( decrypted_message )\n</pre>\n\n<strong>Decrypted message: </strong><code>wasn't that fun</code>",
    "date": "2012-02-13 22:39:39",
    "timestamp": 1329190779,
    "comments": [
      {
        "id": 174434,
        "parent_id": 0,
        "author_ip": "127.0.0.1",
        "author": "Russell Ballestrini",
        "email": "russell.ballestrini@gmail.com",
        "content": "This is a way to randomly generate a new mono alphabetic cipher\n\n<pre>\nfrom string import letters\nkeys = values = list(letters)\nshuffle(values)\nmonoalpha = dict(zip(keys, letters))\nmonoalpha[' '] = ' '\n</pre>",
        "date": "2015-10-29 22:25:43",
        "timestamp": 1446171943
      }
    ],
    "metadata": {
      "_wp_old_slug": "my-python-monoalphabetic-cipher-and-the-inverse",
      "_series_part": "6",
      "_edit_last": "1",
      "_spost_short_title": null
    }
  },
  "block-cipher-lab": {
    "name": "block-cipher-lab",
    "id": "2203",
    "link": "http://russell2.ballestrini.net/block-cipher-lab/",
    "title": "Block cipher lab",
    "content": "<BR/>\n<strong>Consider the following block cipher.</strong>  Suppose that each block cipher T simply reverses the order of the eight input bits (so that, for example 11110000 becomes 00001111).  \n<BR/><BR/>\nFurther suppose that the 64-bit scrambler does not modify any bits.  With n = 3 iterations and the original 64-bit input equal to 10100000 repeated eight times, what is the value of the output? \n<BR/><BR/>\nNow change the last bit of the original 64-bit input from 0 to a 1. Now suppose that the 64-bit scrambler inverses the order of the 64 bits.\n<BR/><BR/>\n<strong>Solution in python:</strong>\n <pre>def chunks( l, n ):\n    \"\"\"accept a list and chuck size, return chunks\"\"\"\n    return [ l[ i:i+n ] for i in range( 0, len(l), n ) ]\n\n\ndef T( blocks ):\n    \"\"\"for each block, reverse block, return blocks\"\"\"\n    result = []\n    for block in blocks:\n        result.append( ''.join( [bit for bit in reversed( block )] ) )\n\n    return result\n\ndef scrambler( input ):\n    \"\"\"inverse the order of input\"\"\"\n    return ''.join( [i for i in reversed( input ) ] )\n\n\ndef cipher1( input, n = 3, chunk_length = 8 ):\n    \"\"\"make chucks out of input, reverse each chunk return result\"\"\"\n    blocks = chunks( input, chunk_length )\n    for i in range( 0, n ): blocks = T( blocks )\n    return ''.join( blocks )\n\n\ndef cipher2( input, n = 3, chunk_length = 8 ):\n    \"\"\"same as cipher1 but with scrambler\"\"\"\n    blocks = chunks( input, chunk_length )\n    for i in range( 0, n ):\n        blocks = T( blocks )\n        blocks = chunks( scrambler( ''.join( blocks ) ), chunk_length )\n    return ''.join( blocks )\n\n\nif __name__ == \"__main__\":\n\n    input = \"1010000010100000101000001010000010100000101000001010000010100000\"\n    print cipher1( input )\n    # output: 0000010100000101000001010000010100000101000001010000010100000101\n\n    input = \"1010000010100000101000001010000010100000101000001010000010100001\"\n    print cipher1( input )\n    # output: 0000010100000101000001010000010100000101000001010000010110000101\n\n    input = \"1010000010100000101000001010000010100000101000001010000010100000\"\n    print cipher2( input )\n    # output: 1010000010100000101000001010000010100000101000001010000010100000\n\n    input = \"1010000010100000101000001010000010100000101000001010000010100001\"\n    print cipher2( input )\n    # output: 1010000110100000101000001010000010100000101000001010000010100000\n</pre>",
    "date": "2012-02-14 01:36:37",
    "timestamp": 1329201397,
    "comments": [
      {
        "id": 146864,
        "parent_id": 0,
        "author_ip": "127.0.0.1",
        "author": "Josh",
        "email": "mayes-j1@email.ulster.ac.uk",
        "content": "Hi there, I have a question in an exam paper which is pretty much this, any help on a plaintext answer? I wouldn't have time in the exam to write the python solution. Thanks",
        "date": "2015-05-10 16:43:35",
        "timestamp": 1431290615
      }
    ],
    "metadata": {
      "_series_part": "7",
      "_edit_last": "1",
      "_spost_short_title": null,
      "_stcr@_mayes-j1@email.ulster.ac.uk": "2015-05-10 16:43:35|Y"
    }
  },
  "my-4-month-olds-15-minutes-of-fame": {
    "name": "my-4-month-olds-15-minutes-of-fame",
    "id": "2258",
    "link": "http://russell2.ballestrini.net/my-4-month-olds-15-minutes-of-fame/",
    "title": "My 4 month old's 15 minutes of fame",
    "content": "<a href=\"http://russell.ballestrini.net/my-4-month-olds-15-minutes-of-fame/cutest-redsox-fan-ever/\" rel=\"attachment wp-att-2285\"><img src=\"/wp-content/uploads/2012/02/cutest-redsox-fan-ever.jpg\" alt=\"\" title=\"cutest-redsox-fan-ever\" width=\"612\" height=\"612\" class=\"aligncenter size-full wp-image-2285\" /></a><p>\nMy sister [linkpeek-hover uri='https://twitter.com/#!/VeronicaBal' text='@VeronicaBal'] was watching my son ([linkpeek-hover uri='https://twitter.com/#!/RussellBal' text='@RussellBal'] ) today and tweeted a picture of him.  \n</p>\n<p>\nThe official [linkpeek-hover uri='https://twitter.com/#!/RedSox' text='@RedSox'] re-tweeted the image to all 197,035 followers!\n</p>\n<p>\n<strong>I'm a proud daddy!</strong>\n</p>\n<a href=\"http://russell.ballestrini.net/my-4-month-olds-15-minutes-of-fame/screenshot-at-2012-02-16-172630/\" rel=\"attachment wp-att-2259\"><img src=\"/wp-content/uploads/2012/02/Screenshot-at-2012-02-16-172630.png\" alt=\"\" title=\"Screenshot at 2012-02-16 17:26:30\" width=\"909\" height=\"989\" class=\"aligncenter size-full wp-image-2259\" /></a>",
    "date": "2012-02-16 18:42:50",
    "timestamp": 1329435770,
    "comments": [],
    "metadata": {}
  },
  "jake": {
    "name": "jake",
    "id": "2296",
    "link": "http://russell2.ballestrini.net/jake/",
    "title": "Jake, Finn, and Ice King",
    "content": "<p>\n<strong>Jake from Adventure Time, painted with MyPaint and wacom tablet.</strong>\n</p>\n<p>\nI did this because my 4 month old \"cracks-up\" laughing whenever this character is on screen.  I think the combination of colors and Jake's voice, played by John William DiMaggio, get him going.\n</p>\n\n<img src=\"/wp-content/uploads/2012/02/mypaint-jake-adventure-time.png\" alt=\"\" title=\"mypaint-jake-adventure-time\" width=\"726\" height=\"728\" class=\"aligncenter size-full wp-image-2297\" />\n\n\n<p>\n<strong>Finn from Adventure Time.</strong>\n</p>\n<p>\nTook a shot at painting Finn.\n</p>\n\n<a href=\"http://russell.ballestrini.net/jake/mypaint-finn-adventure-time/\" rel=\"attachment wp-att-2311\"><img src=\"/wp-content/uploads/2012/02/mypaint-finn-adventure-time.png\" alt=\"\" title=\"mypaint-finn-adventure-time\" width=\"480\" height=\"482\" class=\"aligncenter size-full wp-image-2311\" /></a>\n\n<p>\nI'm on a roll!  Here is Ice King.\n</p>\n\n<a href=\"http://russell.ballestrini.net/jake/ice-king-adventure-time/\" rel=\"attachment wp-att-2314\"><img src=\"/wp-content/uploads/2012/02/ice-king-adventure-time.png\" alt=\"\" title=\"ice-king-adventure-time\" width=\"801\" height=\"942\" class=\"aligncenter size-full wp-image-2314\" /></a>\n\n<p>\nJake standing up, painted this this morning\n</p>\n\n<a href=\"http://russell.ballestrini.net/jake/jake-standing/\" rel=\"attachment wp-att-2321\"><img src=\"/wp-content/uploads/2012/02/jake-standing.png\" alt=\"\" title=\"jake-standing\" width=\"803\" height=\"1140\" class=\"aligncenter size-full wp-image-2321\" /></a>\n\n<p>\nLumpy princess\n</p>\n\n<a href=\"http://russell.ballestrini.net/jake/lump-princess/\" rel=\"attachment wp-att-2430\"><img src=\"/wp-content/uploads/2012/02/lump-princess.png\" alt=\"\" title=\"lump-princess\" width=\"746\" height=\"844\" class=\"aligncenter size-full wp-image-2430\" /></a>\n\n<p>\nI'm getting faster, this was completed in 20 minutes\n</p>\n\n<a href=\"http://russell.ballestrini.net/jake/sad-jake/\" rel=\"attachment wp-att-2431\"><img src=\"/wp-content/uploads/2012/02/sad-jake.png\" alt=\"\" title=\"sad-jake\" width=\"846\" height=\"1110\" class=\"aligncenter size-full wp-image-2431\" /></a>",
    "date": "2012-02-18 13:34:37",
    "timestamp": 1329590077,
    "comments": [],
    "metadata": {
      "_edit_last": "1",
      "_wp_old_slug": "2296"
    }
  },
  "how-to-save-hundreds-of-dollars-on-groceries-without-clipping-coupons": {
    "name": "how-to-save-hundreds-of-dollars-on-groceries-without-clipping-coupons",
    "id": "1730",
    "link": "http://russell2.ballestrini.net/how-to-save-hundreds-of-dollars-on-groceries-without-clipping-coupons/",
    "title": "How to save hundreds of dollars on groceries without clipping coupons",
    "content": "<p>\n<strong>This is the first time I've had a guest blogger on my site.  It may sound campy but my wife Jenn wrote this article after explaining how our grocery bill decreased so drastically.</strong>\n</p>\n\n<a href=\"http://russell.ballestrini.net/how-to-save-hundreds-of-dollars-on-groceries-without-clipping-coupons/3247325203_6108897833_o/\" rel=\"attachment wp-att-2401\"><img src=\"/wp-content/uploads/2012/03/3247325203_6108897833_o.jpg\" alt=\"\" title=\"3247325203_6108897833_o\" width=\"300\" class=\"alignright wp-image-2401\" /></a>\n\n<p>\n<strong>How to save hundreds of dollars on groceries without clipping coupons</strong>\n<p>\n\n<p>\nI was recently re-working the cabinets of the kitchen, anticipating a new and organized year. With a growing family and limited space, I decided to do some consolidating.\n</p>\n<p>\nI allocated a whole section of cabinet space to arts and craft supplies for the kids (play-doh, paints, etc), and another to bottles and formula. I suddenly realized that my food products remained scattered across the counter and there was only one lonely cabinet left to fill.\n</p>  \n\n<a href=\"http://russell.ballestrini.net/how-to-save-hundreds-of-dollars-on-groceries-without-clipping-coupons/6876071849_b82bca1076_o/\" rel=\"attachment wp-att-2404\"><img src=\"/wp-content/uploads/2012/03/6876071849_b82bca1076_o.jpg\" alt=\"\" title=\"6876071849_b82bca1076_o\" width=\"300\" class=\"alignright wp-image-2404\" /></a>\n\n<p>Among the food items on the counter, many were far expired, forgotten in the dusty back corners of the cabinets. I thought to myself \u201cHey! I worked hard to find  coupons and sales for all these things.\u201d and I regretfully filled a garbage bag with old food. \n\n<p>\nI had been clipping coupons and striving to match the savings of super couponers. These couponers, featured on popular reality shows, often recommend stock-piling when food items go on sale and you have coupons to purchase them.\n</p>\n\n<a href=\"http://russell.ballestrini.net/how-to-save-hundreds-of-dollars-on-groceries-without-clipping-coupons/6876069657_5a2f4ae487_o/\" rel=\"attachment wp-att-2403\"><img src=\"/wp-content/uploads/2012/03/6876069657_5a2f4ae487_o.jpg\" alt=\"\" title=\"6876069657_5a2f4ae487_o\" width=\"300\" class=\"alignright size-full wp-image-2403\" /></a>\n\nI opened the refrigerator to find a similar and familiar situation: a head of lettuce that had seen better days, a tomato that had lost it's freshness, and other food items that were past the date of recommended consumption.\n</p>\n\n<p>Looking into the garbage bag, I realized that... every week I was wastefully throwing food and money away. I decided to make a commitment that has cut my grocery bill in half! Do I still use coupons? YES, if they fit into my weekly plan. Do I still look at the sale flyers? ABSOLUTELY! But... \n\n\n</p>\n<p><strong>These two strategies have saved me much more money:</strong>\n</p>\n\n<p>\n<strong>1. Cut down food storage spaces</strong><br/><br/>\nIf you currently use 4 food cabinets try designating only 2.  In my case I consolidated my food items to 1 cabinet.  This consolidation saved money in two distinct ways:\n<ul>\n\t<li><em>First,</em> I am easily able to assess which food items I have and when they expire. This helps prevent re-buying a product we already have at home (Remember the time you were at the grocery store buying all the ingredients for those chocolate chip cookies and couldn't remember if you had any brown sugar? Undoubtebly you re-bought, probably to discover you had a brand new, unopened package in the cabinet). </li>\n<br/>\n\t<li><em>Secondly,</em> it prevents stock piling. I can buy only what I need for the week ahead, after all, it is all I have room for. If you generate the sense that \u201cmy kitchen is full of food\u201d, it eliminates the need to buy for the sake of filling the cabinets.</li>\n</ul>\n</p>\n\n<p>\n<strong>2. Let your menu dictate your shopping list</strong><br/><br/> My son complains \u201cThere are no more crackers.\" My husband sighs, \u201cWe're all out of bananas.\"  My answer used to be probably a lot like yours \u201cOK, I'll put it on my list.\" I had the perception that I constantly needed to replenish. It is unnecessary and wasteful to have three different brands of crackers, five types of cereal, and every kind of fruit known to the produce section.  Without fail, I was throwing away  money each week.\n</p>\n<p> Instead, I sit down each week with two sheets of paper.  One serves as dinner menu for the week and the other as my shopping list.  When building my menu, I consider the sales and coupons I have on hand. If I notice a great deal on pasta sauce, we might have spaghetti one night and meatball subs another. I immediately begin a shopping list writing ONLY items used for the companion menu.  Additionally, I add two breakfast choices and two lunch choices for the week if needed (some breakfast items such as pancake batter or a family size box of cereal often last longer).  I also include 2 choices of fresh fruit or vegetable.  To reduce waste and cost I ONLY add the staples (eggs, milk, and bread) when it is on the menu. If none of my breakfast, lunch, or dinner choices call for bread, I skip that aisle for this trip.\n</p>\n\n<p>\n<strong> It took me only a few short weeks to notice that by cutting my food storage space in half, and adopting a disciplined approach to menu and list making, I had saved hundreds of dollars. I challenge you to cut down your food storage spaces and let your menu dictate your shopping list!</strong>\n</p>\n\n<a href=\"http://russell.ballestrini.net/how-to-save-hundreds-of-dollars-on-groceries-without-clipping-coupons/6876068965_db8a5d12b2_o/\" rel=\"attachment wp-att-2402\"><img src=\"/wp-content/uploads/2012/03/6876068965_db8a5d12b2_o.jpg\" alt=\"\" title=\"6876068965_db8a5d12b2_o\" width=\"600\" height=\"480\" class=\"aligncenter size-full wp-image-2402\" /></a>",
    "date": "2012-03-03 19:54:45",
    "timestamp": 1330822485,
    "comments": [],
    "metadata": {
      "_edit_last": "1",
      "_spost_short_title": null
    }
  },
  "how-to-capture-https-ssl-tls-packets-with-wireshark": {
    "name": "how-to-capture-https-ssl-tls-packets-with-wireshark",
    "id": "2221",
    "link": "http://russell2.ballestrini.net/how-to-capture-https-ssl-tls-packets-with-wireshark/",
    "title": "How to capture HTTPS SSL TLS packets with wireshark",
    "content": "<p>\nThis article will explain how to use wireshark to capture TCP/IP packets.  Specifically I will show how to capture encrypted (HTTPS) packets and attempt to document the \"dance\" a client and server do to build an SSL tunnel.\n</p>\n\n<p>\n<strong>What is Wireshark?</strong>\n</p>\n\n<p>Wireshark is a network protocol analyzer for Windows, OSX, and Linux. It lets you capture and interactively browse the traffic running on a computer network. Similar software includes tcpdump on Linux.</p>\n\n<strong>Install Wireshark</strong>\n<p>First step, acquire Wireshark for your operating system.  </p>\n\n<p>\n<em>Ubuntu Linux:</em> <code>sudo apt-get install wireshark</code>\n</p>\n<p>\n<em>Windows or Mac OSX:</em> search for wireshark and download the binary.\n</p>\n\n<strong>How to capture packets</strong>\n\n<p>\nThis is Wireshark's main menu:\n</p>\n\n\n<img src=\"/wp-content/uploads/2012/02/wireshark.png\" alt=\"\" title=\"wireshark\" width=\"594\" height=\"61\" class=\"aligncenter size-full wp-image-2226\" />\n\n<p>\nTo start a capture, click the following icon:\n</p>\n\n<img src=\"/wp-content/uploads/2012/02/wireshark-start-capture.png\" alt=\"\" title=\"wireshark-start-capture\" width=\"595\" height=\"61\" class=\"aligncenter size-full wp-image-2229\" />\n\n<p>\nA new dialog box should have appeared.  Click start on your preferred interface:\n</p>\n\n<img src=\"/wp-content/uploads/2012/02/wireshark-sniff.png\" alt=\"\" title=\"wireshark-sniff\" width=\"680\" height=\"90\" class=\"aligncenter size-full wp-image-2238\" />\n\n<p>\nYou are now capturing packets.  The packet information is displayed in the table below the main menu:\n</p>\n\n<img src=\"/wp-content/uploads/2012/02/wireshark-packets.png\" alt=\"\" title=\"wireshark-packets\" width=\"607\" height=\"609\" class=\"aligncenter size-full wp-image-2242\" />\n\n<p>\nNow browse to an HTTPS website with your browser.  I went to <a href=\"https://linkpeek.com\">https://linkpeek.com</a> and after the page completely loaded, I stopped the Wireshark capture:</a>\n</p>\n\n<img src=\"/wp-content/uploads/2012/02/wireshark-stop-capture.png\" alt=\"\" title=\"wireshark-stop-capture\" width=\"597\" height=\"61\" class=\"aligncenter size-full wp-image-2245\" />\n\n<p>\nDepending on your network, you could have just captured MANY packets.  To limit our view to only interesting packets you may apply a filter.  Filter the captured packets by ssl and hit Apply:\n</p>\n\n<img src=\"/wp-content/uploads/2012/02/wireshark-filter.png\" alt=\"\" title=\"wireshark-filter\" width=\"619\" height=\"29\" class=\"aligncenter size-full wp-image-2246\" />\n\n<p>Now we should be only looking at SSL packets.</p>\n\n<strong>Next we will analyze the SSL packets and answer a few questions</strong>\n\n<p>\n<b>1.</b> For each of the first 8 Ethernet frames, specify the source of the frame (client or server), determine the number of SSL records that are included in the frame, and list the SSL record types that are included in the frame. Draw a timing diagram between client and server, with one arrow for each SSL record.\n</p>\n\n<p>\nFrame 1 client | 1 record | Arrival Time: Feb 15, 2012 15:38:55.601588000 <BR/>\nFrame 2 server | 1 record | Arrival Time: Feb 15, 2012 15:38:55.688170000 <BR/>\nFrame 3 server | 2 record | Arrival Time: Feb 15, 2012 15:38:55.688628000 <BR/>\nFrame 4 client | 3 record | Arrival Time: Feb 15, 2012 15:38:55.697705000 <BR/>\nframe 5 server | 2 record | Arrival Time: Feb 15, 2012 15:38:55.713139000 <BR/>\nframe 6 client | 1 record | Arrival Time: Feb 15, 2012 15:38:55.713347000 <BR/>\nframe 7 server | 0 record | Arrival Time: Feb 15, 2012 15:38:55.713753000 <BR/>\nframe 8 server | 1 record | Arrival Time: Feb 15, 2012 15:38:55.715003000 <BR/>\n</p>\n\n<p>\n<b>2.</b> Each of the SSL records begins with the same three fields (with possibly different values). One of these fields is \u201ccontent type\u201d and has length of one byte. List all three fields and their lengths.</p>\n\n<p>\nEach hexadecimal digit (also called a \"nibble\") represents four binary digits (bits) so each pair of hexadecimal digits equals 1 byte.<BR/>\n\na. Destination mac address | 6 btyes | 00 21 9b 31 99 51 <BR/>\nb. Source mac address | 6 bytes | 00 10 db ff 20  <BR/>\nc. Type: IP | 2 byte | 08 00 <BR/>\n</p>\n\n<strong>ClientHello Records</strong>\n\n<p>\n<b>3.</b>Expand the ClientHello record. (If your trace contains multiple ClientHello\nrecords, expand the frame that contains the first one.) What is the value of the\ncontent type?\n<br/><br/>hex: 16 (16+6=22) Handshake\n</p>\n\n<p>\n<b>4.</b> Does the ClientHello record advertise the cipher suites it supports? If so, in the first listed suite, what are the public-key algorithm, the symmetric-key algorithm, and the hash algorithm?\n<br><br>\nMD5, SHA, RSA, DSS, DES, AES\n</p>\n\n<strong>ServertHello Records</strong>\n\n<p>\n<b>5.</b> Look to the ServerHello packet.  What cipher suite does it choose?\n<br><br>\nCipher Suite: TLS_RSA_WITH_AES_128_CBC_SHA (0x002f)\n</p>\n\n<p>\n<b>6.</b> Does this record include a nonce? If so, how long is it? What is the purpose of the\nclient and server nonces in SSL?<br><br>\nYes, 28 bytes.  The ClientHello packet also generated a nonces.  They are used to make the session communication between the two nodes unique.  It \"salts\" the communication to prevent replay attacks. A replay attack happens when data from old communications is used to \"crack\" a current communication.</p>\n\n<p>\n<b>7.</b>Does this record include a session ID? What is the purpose of the session ID?\n<br><br>\nYes, This is to make things efficient, in case the client has any plans of closing the current connection and reconnect in the near future. \n</p>\n\n<p>\n<b>8.</b>How many frames does the SSL certificate take to send?<br><br>\nIn this case it took 4 frames\n</p>",
    "date": "2012-02-29 19:14:54",
    "timestamp": 1330560894,
    "comments": [
      {
        "id": 9403,
        "parent_id": 0,
        "author_ip": "127.0.0.1",
        "author": "jmohl",
        "email": "jmomlhm@yahoo.com",
        "content": "Hi,\nI am a newbie learning wireshark. I have a doubt - Is the \"client hello\" message under the info field specific to SSL protocol only?\nThanks,\njmohl",
        "date": "2012-10-03 14:28:58",
        "timestamp": 1349288938
      },
      {
        "id": 9405,
        "parent_id": 0,
        "author_ip": "127.0.0.1",
        "author": "jmohl",
        "email": "jmomlhm@yahoo.com",
        "content": "Hi,\nIs there any user manual for wireshark corresponding to each protocol?\nThanks,\njmohl",
        "date": "2012-10-03 14:30:00",
        "timestamp": 1349289000
      },
      {
        "id": 6432,
        "parent_id": 0,
        "author_ip": "127.0.0.1",
        "author": "SAURABH RANA",
        "email": "saurav.rana@gmail.com",
        "content": "nicely explained about packet capturing in SSL .i want to know about the software we can use to show the demonstration.",
        "date": "2012-08-07 01:25:25",
        "timestamp": 1344317125
      },
      {
        "id": 7091,
        "parent_id": 6432,
        "author_ip": "127.0.0.1",
        "author": "Russell Ballestrini",
        "email": "russell.ballestrini@gmail.com",
        "content": "wireshark or tcpdump",
        "date": "2012-08-19 21:14:18",
        "timestamp": 1345425258
      },
      {
        "id": 136845,
        "parent_id": 0,
        "author_ip": "127.0.0.1",
        "author": "Shoaib Moona",
        "email": "shoaibmoona@yahoo.com",
        "content": "Can Someone Please Tell Me How To Do It? Or Can Someone Just Make A Video On it.. Or Just Me Any Link To The pre-Existing Video?? Please Help :)",
        "date": "2015-03-23 12:01:27",
        "timestamp": 1427126487
      }
    ],
    "metadata": {
      "_edit_last": "1",
      "_stcr@_shoaibmoona@yahoo.com": "2015-03-23 12:01:27|Y",
      "_spost_short_title": null
    }
  },
  "zenoss-or-nagios-monitoring-of-https-using-client-certificate-authentication": {
    "name": "zenoss-or-nagios-monitoring-of-https-using-client-certificate-authentication",
    "id": "2324",
    "link": "http://russell2.ballestrini.net/zenoss-or-nagios-monitoring-of-https-using-client-certificate-authentication/",
    "title": "Zenoss or Nagios monitoring of HTTPS using client certificate authentication",
    "content": "<p>I recently needed to monitor an HTTPS API for response time and availability.  At first I planned to just use the Nagios check_http command.  \n</p>\n\n<p>\nAfter gathering more requirements I learned that the API was protected by client certificate authentication.  After some research I quickly found that no solution existed to monitor HTTP protected by client certs.  I needed to write my own plugin.</p>\n\n<p>\nThis is the python plugin I came up with: <strong>check_http_client_cert.py</strong>\n</p>\n\n<pre lang=\"pythonh\">#!/usr/bin/python\n\n\"\"\"Nagios/Zenoss client cert https checker\"\"\"\n\nimport httplib\nfrom optparse import OptionParser\nfrom time import time\nfrom sys import exit\n\ndef request( hostname, port, cert_file, path ):\n    \"\"\"request a resource and return response object\"\"\"\n    try:\n        c = httplib.HTTPSConnection( hostname, port, cert_file=cert_file )\n        c.request( \"GET\", path )\n        return c.getresponse()\n    except:\n        return False\n\nif __name__ == '__main__':\n    parser = OptionParser()\n    parser.add_option('-H', '--hostname', dest='hostname')\n    parser.add_option('-p', '--port', dest='port')\n    parser.add_option('-c', '--cert_file', dest='cert_file')\n    parser.add_option('-P', '--path', dest='path',\n    help=\"Path relative to root, like /image/search\")\n\n    o, args = parser.parse_args()\n    #print o\n   \n    start = time() \n    r = request( o.hostname, o.port, o.cert_file, o.path )\n    elapse = time() - start\n\n    if r:\n        if r.status >= 200 and r.status < 400:\n            print \"HTTP OK:\", r.status, r.reason, \"|time=\" + str(elapse) + \"s;;;\"\n            exit( 0 )\n        print \"HTTP Critical:\", r.status, r.reason\n \n    exit( 2 )\n</pre>",
    "date": "2012-02-26 19:21:43",
    "timestamp": 1330302103,
    "comments": [],
    "metadata": {
      "_spost_short_title": null,
      "_edit_last": "1"
    }
  },
  "what-do-you-name-your-python-virtualenv": {
    "name": "what-do-you-name-your-python-virtualenv",
    "id": "2434",
    "link": "http://russell2.ballestrini.net/what-do-you-name-your-python-virtualenv/",
    "title": "What do you name your python virtualenv?",
    "content": "<BR/>\n<strong>What do you name your python virtualenv?</strong>\n<p>\nI name my virtualenv 'virtpy'.  Is there a standard name being used out there?\n</p>\n\n<p>\nMaybe we can come to a consensus as a standard name?  Please feel free to post your virtualenv names here as a sort of poll.\n</p>",
    "date": "2012-03-07 21:42:30",
    "timestamp": 1331174550,
    "comments": [
      {
        "id": 1510,
        "parent_id": 0,
        "author_ip": "149.152.132.30",
        "author": "Russell Ballestrini",
        "email": "russell.ballestrini@gmail.com",
        "content": "virtpy",
        "date": "2012-03-07 21:43:01",
        "timestamp": 1331174581
      },
      {
        "id": 1512,
        "parent_id": 0,
        "author_ip": "192.168.1.23",
        "author": "Russell Ballestrini",
        "email": "russell.ballestrini@gmail.com",
        "content": "some responses from IRC:\n\nevn\n\nlocalenv\n\nand\n\nnaming the virtualenv after the project/application it supports.",
        "date": "2012-03-08 09:13:23",
        "timestamp": 1331216003
      },
      {
        "id": 1707,
        "parent_id": 0,
        "author_ip": "24.16.134.217",
        "author": "Doug",
        "email": "dougwt@gmail.com",
        "content": "I only recently started using virtualenv, but so far I have been creating a new one for each project or django site.",
        "date": "2012-03-26 20:23:48",
        "timestamp": 1332807828
      },
      {
        "id": 1708,
        "parent_id": 1707,
        "author_ip": "192.168.1.23",
        "author": "Russell Ballestrini",
        "email": "russell.ballestrini@gmail.com",
        "content": "@doug, so you name them after your project?",
        "date": "2012-03-26 20:45:00",
        "timestamp": 1332809100
      },
      {
        "id": 67922,
        "parent_id": 0,
        "author_ip": "127.0.0.1",
        "author": "Gpzim98",
        "email": "gpzim98@gmail.com",
        "content": "Before I put names like: MyVirtualEnv, or put domain name myclientedomain.com.br.\nBut now I've tried find a simple conventions. So, I'm going to use like you right now, myvirtualevn.",
        "date": "2014-09-28 13:21:56",
        "timestamp": 1411924916
      }
    ],
    "metadata": {
      "_stcr@_gpzim98@gmail.com": "2014-09-28 13:21:56|Y",
      "_edit_last": "1"
    }
  },
  "i-see-said-the-blind-man-to-the-deaf-dog-as-he-walked-off-the-cliff": {
    "name": "i-see-said-the-blind-man-to-the-deaf-dog-as-he-walked-off-the-cliff",
    "id": "2451",
    "link": "http://russell2.ballestrini.net/i-see-said-the-blind-man-to-the-deaf-dog-as-he-walked-off-the-cliff/",
    "title": "\"I see\" said the blind man, to the deaf dog, as he walked off the cliff.",
    "content": "<br/>\n\n<strong>\"I see\" said the blind man, to the deaf dog, as he walked off the cliff.</strong>\n\n<br/><br/>\n\n<p>\nAs far as I can tell, I am the originator of this version of this quote.\n</p>\n\n<p>\n<strong>EDIT:</strong> changed \"originator of the quote\" to \"originator of this version of this quote\".\n</p>",
    "date": "2012-03-23 22:04:54",
    "timestamp": 1332554694,
    "comments": [
      {
        "id": 172116,
        "parent_id": 0,
        "author_ip": "127.0.0.1",
        "author": "Jon",
        "email": "jonwilliamwilde@sky.com",
        "content": "\u201cI see\u201d said the blind man, to the deaf dog, as he walked off the cliff. \n\nI first heard the above expression in 1973. A man named Alfred Stubbins said it to me. I was sitting in a park at the time, minding my own business. He just came out with it. Just like that.",
        "date": "2015-10-11 15:08:18",
        "timestamp": 1444590498
      }
    ],
    "metadata": {
      "_edit_last": "1",
      "_spost_short_title": null,
      "_stcr@_jonwilliamwilde@sky.com": "2015-10-11 15:08:18|Y"
    }
  },
  "nosslsearch-cname-is-a-bad-idea-and-solution": {
    "name": "nosslsearch-cname-is-a-bad-idea-and-solution",
    "id": "2459",
    "link": "http://russell2.ballestrini.net/nosslsearch-cname-is-a-bad-idea-and-solution/",
    "title": "nosslsearch cname is a bad idea and solution",
    "content": "<br/>\t\n<strong>Google SafeSearch and SSL Search for Schools suggests implementing the following changes to the network:</strong>\n\n<blockquote>To utilize the no SSL option for your network, configure the DNS entry for www.google.com to be a CNAME for nosslsearch.google.com. </blockquote>\n\n<br/>\n<strong>\nHere are the reasons why this is a bad idea and solution:</strong>\n\n<ul>\n\t<li>In order to create a CNAME record for www.google.com we need to become an authoritative master of that zone.</li>\n\t<li>If you become an authoritative master you need to host all of Google's DNS resource records for the domain.</li>\n\t<li>Google is asking us to DNS poison it's flag ship product on our networks.</li>\n\t<li>If other companies follow suit the internet will quickly become unmanageable.  DNS was not ment to work this way.</li>\n\t<li>Not all networks have a local DNS server</li>\n</ul>\n\n<br/>\nThis is a bad idea.  Please change your stance on this matter.\n\n<br/>\n<br/>\nReference: <a href=\"http://support.google.com/websearch/bin/answer.py?hl=en&answer=186669\">http://support.google.com/websearch/bin/answer.py?hl=en&answer=186669</a>",
    "date": "2012-03-26 11:51:20",
    "timestamp": 1332777080,
    "comments": [
      {
        "id": 18524,
        "parent_id": 0,
        "author_ip": "127.0.0.1",
        "author": "Mateusz",
        "email": "mat.jonczyk@o2.pl",
        "content": "A better way is to redirect traffic using iptables or something similar.",
        "date": "2013-12-20 10:06:43",
        "timestamp": 1387552003
      }
    ],
    "metadata": {
      "_edit_last": "1",
      "_stcr@_mat.jonczyk@o2.pl": "2013-12-20 10:06:43|Y"
    }
  },
  "trouble-mounting-filesystem-on-kvm-guest-after-reboot": {
    "name": "trouble-mounting-filesystem-on-kvm-guest-after-reboot",
    "id": "2471",
    "link": "http://russell2.ballestrini.net/trouble-mounting-filesystem-on-kvm-guest-after-reboot/",
    "title": "Trouble mounting filesystem on KVM guest after reboot",
    "content": "<p>\n<strong>Just found this out the hard way...</strong>\n</p>\n\n<p>\nIt looks like the attachment of <code>/KVMROOT/guest-dev-app.img</code> on guest-dev did not persist when the KVM host rebooted for patching.\n</p>\n\n<p>\nAs it appears the <code>virsh attach-disk</code> command works a lot like the <code>mount</code> command.\n</p>\n\n<p>\nIn order to have a disk attachment persist after a reboot, I think we still need to do a <code>virsh edit <dom></code>.\n</p>\n\n<p>\nthe <code>virsh attach-disk</code> command is useful because it allows us to attach disk images to guests without restarting.\n</p>\n\n<p>\n<strong>tldr;</strong>\n</p>\n\n<p>\n<code>virsh attach-disk</code> is to <code>mount</code> as \n<code>virsh edit</code> is to <code>vim /etc/fstab</code>\n</p>",
    "date": "2012-03-27 12:04:58",
    "timestamp": 1332864298,
    "comments": [],
    "metadata": {
      "_edit_last": "1",
      "_wp_old_slug": "trouble-mounting-filesystem-on-guest-after-reboot"
    }
  },
  "i-just-purchased-instagram-for-1b-and-all-i-got-was-this-lousy-image-filter": {
    "name": "i-just-purchased-instagram-for-1b-and-all-i-got-was-this-lousy-image-filter",
    "id": "2476",
    "link": "http://russell2.ballestrini.net/i-just-purchased-instagram-for-1b-and-all-i-got-was-this-lousy-image-filter/",
    "title": "I just purchased Instagram for 1B and all I got was this lousy image filter",
    "content": "<p><strong>Facebook purchased Instagram on Monday, April 9th 2012 for $1,000,000,000 US.</strong>  Instagram, a free to use image sharing iPhone application, has absolutely NO REVENUE STREAM.  The application is free to download, free to use, and has no monetization support from advertisements.  \n</p>\n\n<blockquote>$1000000000.00 / 30000000 users = <strong>33.33 per user</strong></blockquote>\n\n<p>At the time of the sale Instagram had 30 million users in total (30,000,000).  That means Facebook just paid heavy <strong>$33.33</strong> per user... This is obviously a bad deal for Facebook.\n\n<p>\nInstagram did not have a method to monetize its application.  I hedge my bets that Facebook will never recoup the one billion dollars spent on this deal.\n</p>\n\n<p>Mark Zuckerburg, I just thought up the perfect T-Shirt for you:  </p>\n\n\n\n<blockquote><strong>I just purchased Instagram for 1B and all I got was this lousy image filter.</strong></blockquote>",
    "date": "2012-04-09 17:47:28",
    "timestamp": 1334008048,
    "comments": [],
    "metadata": {
      "_edit_last": "1",
      "_wp_old_slug": "how-much-did-facebook-pay-for-instagram"
    }
  },
  "the-most-valuable-registration-field-how-did-you-hear-about-us": {
    "name": "the-most-valuable-registration-field-how-did-you-hear-about-us",
    "id": "2515",
    "link": "http://russell2.ballestrini.net/the-most-valuable-registration-field-how-did-you-hear-about-us/",
    "title": "The most valuable registration field: How did you hear about us?",
    "content": "<BR/>\n\n<strong>How did you hear about us?</strong>  \n\n<BR/> \n<BR/>\n\n\nI first answered this question when joining [linkpeek-hover uri=\"http://www.linode.com/?r=fbe7966884fa1a4b6ce046a5a36e072127c6f5f7\" text=\"Linode\"].  I remember thinking \"Wow, this is a great time to ask me!\" because the real answer was still in my short term memory.\n\n<BR/>\n<BR/>\n\nWhen I launched [linkpeek-hover uri=\"https://linkpeek.com/signup?plan=better\" text=\"LinkPeek\"] I decided to apply this technique.  After an amazing launch (thank you colleagues from HackerNews) the true significance of this field was exposed.  I would argue that the answer to this simple question holds more value than collecting a username. \n\n<BR/>\n<BR/>\n\n<strong>Here is why:</strong>\n\n<BR/>\n<BR/>\n\nIt may seem obvious after reading this post, but this is what I learned and I am sharing to help other startups.\n\n<BR/>\n<BR/>\n\nAsking \"How did you hear about us?\" will help you determine:\n<ul>\n  <li>how your customers found you</li>\n  <li>which of your marketing efforts are working</li>\n  <li>where to spend money or time marketing (and where to stop)</li>\n  <li>your true target market</li>\n  <li>how your customers will use your product</li>\n</ul>\n\nHere are some other benefits to this technique.  It opens a dialog or conversation with the customer, which in return should help lower friction during the sale.  You already have the the customer engaged during the signup process.  I am firmly against the traditional method of surveying customers.  I believe the same data can be collected without being abrasive or wasting the customers time.\n\n<blockquote>\"How did you hear about us?\" is the gift that keeps on giving.</blockquote>\n\nIt is commonly accepted that shorter registration forms lead to better conversions.  I totally agree, but in this case the sacrifice is worth learning more about my target market and customer needs.\n\n<BR/>\n<BR/>\n\n<strong>How to ask this question properly:</strong>\n<ol>\n  <li><strong>Just ask!</strong> I second guessed my decision to add this field right before launch but that anxiety promptly faded after reading the first answer.\n  </li>\n  <li><strong>Don't use pre-filled answers!</strong> Having pre-filled answers is counter productive because you will not learn anything new.  You are forcing the user into choosing one of your answers...\n  </li>\n  <li>\n  <strong>Allow the user to type as much as they like!</strong>  A few of my customers nearly wrote a book in the text field.\n  </li>\n</ol>\n\n<strong>If you liked this post, you should follow me on twitter <a href=\"https://twitter.com/RussellBal\">here</a>.</strong>",
    "date": "2012-04-26 22:21:54",
    "timestamp": 1335493314,
    "comments": [],
    "metadata": {}
  },
  "how-to-rescue-logs-and-config-from-a-failed-citrix-netscaler-app-gateway": {
    "name": "how-to-rescue-logs-and-config-from-a-failed-citrix-netscaler-app-gateway",
    "id": "2570",
    "link": "http://russell2.ballestrini.net/how-to-rescue-logs-and-config-from-a-failed-citrix-netscaler-app-gateway/",
    "title": "How to rescue logs and config from a failed Citrix NetScaler App Gateway",
    "content": "<p>\nToday our production Citrix NetScaler broke.  The box wouldn't boot and our only backup copy of the config was on the NetScaler itself.  \n</p>\n\n<p>\nBeing the only Unix guy around I attempted to help out the admins working the outage.  I SSH'd into the development NetScaler and noticed it runs on FreeBSD.\n</p>\n\n<p>\nI suggested fetching the Hard drive and mounting it on a Linux computer.  The NetScaler has one SATA (not SAS) disk so my desktop was compatible.\n</p>\n\n<p>\nI installed the disk in the Linux tower and mounted the filesystem using the following command:\n</p>\n\n<p>\n<code>mount --read-only --type=ufs  --test-opts ufstype=44bsd /dev/sda5 /mnt\n</code>\n</p>\n\n<p>\nOnce mounted I was able to SCP interesting files to a safe location.\n</p>\n\n<p>\nWarning, this procedure might void your warranty.  If in doubt, call support first.  \n</p>",
    "date": "2012-05-11 00:05:55",
    "timestamp": 1336709155,
    "comments": [],
    "metadata": {}
  },
  "high-load-on-web-server-after-updating-from-ubuntu-10-04-to-ubuntu-12-04-lts": {
    "name": "high-load-on-web-server-after-updating-from-ubuntu-10-04-to-ubuntu-12-04-lts",
    "id": "2586",
    "link": "http://russell2.ballestrini.net/high-load-on-web-server-after-updating-from-ubuntu-10-04-to-ubuntu-12-04-lts/",
    "title": "High load on web server after updating from Ubuntu 10.04 to Ubuntu 12.04 LTS",
    "content": "<BR/>\n<strong>High load on web server after updating from Ubuntu 10.04 to Ubuntu 12.04 LTS</strong>\n<BR/><BR/>\nCheck out charts which lineup to when I upgraded:\n<BR/>\n\n<a href=\"http://russell.ballestrini.net/high-load-on-web-server-after-updating-from-ubuntu-10-04-to-ubuntu-12-04-lts/high-load-after-updating-ubuntu-from-10-04-lts-to-12-04-lts/\" rel=\"attachment wp-att-2587\"><img src=\"/wp-content/uploads/2012/05/high-load-after-updating-ubuntu-from-10.04-LTS-to-12.04-LTS.png\" alt=\"\" title=\"high-load-after-updating-ubuntu-from-10.04-LTS-to-12.04-LTS\" width=\"528\" height=\"1193\" class=\"aligncenter size-full wp-image-2587\" /></a>\n<br/><br/>\nI couldn't determine the cause of the load average increase...\n\n<p>\n<strong>Update:</strong> The issue might be memory bound.  Check out this graph that show much higher swap.\n</p>\n\n<a href=\"http://russell.ballestrini.net/high-load-on-web-server-after-updating-from-ubuntu-10-04-to-ubuntu-12-04-lts/ubuntu-12-04-swap-year/\" rel=\"attachment wp-att-2644\"><img src=\"/wp-content/uploads/2012/05/ubuntu.12.04.swap_.year_.png\" alt=\"\" title=\"ubuntu.12.04.swap.year\" width=\"497\" height=\"280\" class=\"aligncenter size-full wp-image-2644\" /></a>\n\n<p><strong>After much research this appears to be a load calculation and display problem with the newer Linux kernels. The community has found Commit-ID: c308b56b5398779cd3da0f62ab26b0453494c3d4 to be the problem.  The commit causes incorrect high reported load averages can be reported under conditions of light load and high enter/exit idle frequency conditions (greater then 25 hertz).</strong></p>\n\nA nice fellow at http://www.smythies.com/~doug/network/load_average/new.html researched the topic between tick and tickless linux kernels and the effect they had on load averages.  You should check it out.",
    "date": "2012-05-19 18:40:41",
    "timestamp": 1337467241,
    "comments": [],
    "metadata": {}
  },
  "always-attempt-to-scale-vertically-first": {
    "name": "always-attempt-to-scale-vertically-first",
    "id": "2604",
    "link": "http://russell2.ballestrini.net/always-attempt-to-scale-vertically-first/",
    "title": "Always attempt to scale vertically first",
    "content": "<p>\nI spent the weekend fretting because one of my servers was basically being DOS'd by paying customers.  During the outage I started thinking about the best way to scale and how I could make the code-base more efficient.\n</p>\n<p>\nLinux top reported high load, in the 20's.  Eventually I figured out that the server was having IO performance issues.  \n</p>\n<p>\nI wasted a bunch of time attempting to fight fires.  After about an hour of that I decided to scale my VPS vertically by giving it an extra 256mb of memory and a larger swap file (256mb to 1024mb).\n</p>\n<p>\nThese two changes were surprisingly effective and the IO issues resolved.  Apparently the server was starving for memory which caused the host to swap which brought things to a crawl waiting for IO.\n</p>\n<p>\nCrisis averted for the moment.  Now I am free to think clearly and engineer a proper solution instead of attempting to put out fires.\n</p>\n<p>\nIf you ever encounter a similar situation, attempt the simplest fix.  There is no shame in throwing more money at a problem if it will buy you time.  In this case, an extra $10.00 a month relieved the performance issues and bought myself some time, for the moment.\n</p>\n\n<p>\n<a href=\"http://russell.ballestrini.net/always-attempt-to-scale-vertically-first/vertical-scale-marked/\" rel=\"attachment wp-att-2606\"><img src=\"/wp-content/uploads/2012/06/vertical-scale-marked.png\" alt=\"\" title=\"vertical-scale-marked\" width=\"697\" height=\"302\" class=\"aligncenter size-full wp-image-2606\" /></a>",
    "date": "2012-06-10 21:52:35",
    "timestamp": 1339379555,
    "comments": [],
    "metadata": {
      "_edit_last": "1",
      "_wp_old_slug": "it-is-a-no-brainer-always-try-to-scale-vertically-first",
      "_spost_short_title": null
    }
  },
  "prevent-a-certain-program-from-running-to-long-in-bash": {
    "name": "prevent-a-certain-program-from-running-to-long-in-bash",
    "id": "2625",
    "link": "http://russell2.ballestrini.net/prevent-a-certain-program-from-running-to-long-in-bash/",
    "title": "Prevent a certain program from running too long in bash",
    "content": "<p>I came up this this script to kill certain programs after they run for too long.  This works like similar to a timeout.  Warning this script is pretty harsh and kills the program.  \n</p>\n<pre lang=\"bash\">#!/bin/bash\nPROGRAM=replace-with-program-name\nPIDSFILE=/tmp/kill-these.pids\n\nfor pid in `pidof $PROGRAM`\n  do\n    if grep -q $pid $PIDSFILE\n      then\n        kill $pid\n    fi\n  done\n\n> $PIDSFILE\n\nfor pid in `pidof $PROGRAM`\n  do\n    echo $pid >> $PIDSFILE\n  done\n</pre>\n\n<p>Then I wrote a cronjob to kill hung programs: </p>\n\n<pre lang=\"bash\">* * * * * /usr/local/sbin/killprogs.sh</pre>",
    "date": "2012-06-13 09:45:34",
    "timestamp": 1339595134,
    "comments": [],
    "metadata": {
      "_edit_last": "1",
      "_spost_short_title": null
    }
  },
  "my-top-five-suggestions-for-an-independent-developer-creating-a-new-product-or-service": {
    "name": "my-top-five-suggestions-for-an-independent-developer-creating-a-new-product-or-service",
    "id": "2649",
    "link": "http://russell2.ballestrini.net/my-top-five-suggestions-for-an-independent-developer-creating-a-new-product-or-service/",
    "title": "My top five suggestions for an independent developer creating a new product or service",
    "content": "<p>\n<strong>1. Write everyday.</strong>  Build a blog for the project and write about milestones, progress, and hurdles.  Also keep a personal blog and write about hobbies.  Read some theory about \"copy writing\" and search engine optimization.  Write personalized email responses to customers.  Great communication skills will have the most impact on the success of your company.  The best way to increase communication skills is to do it.\n</p>\n\n<p>\n<strong>2. Spend at least 15 minutes on the company everyday.</strong>  Even small or petty tasks will lift your company to the next goal.  Over time the company will grow strong and people will give you attention.  Building a company seems similar to building a character in an RPG.  Instead however you will progress your company to its next level.  Also, always look ahead when working, don't look back at prior achievements.  Keep your eyes on the next milestone and keep moving forward.  This type of behavior will promote growth and prevent getting stuck in the daily grind.\n</p>\n\n<p>\n<strong>3.  Do not fear manual processes. </strong> In the early stages of a company you should only build automation which will enhance customer experience.  For everything else, build a repeatable manual process.  Coding automation will typically cost more time to build than it will save.  For example on [linkpeek-hover uri=\"http://linkpeek.com\" text=\"LinkPeek\"] I have not automated customer account de-activation because fortunately I don't have to do many of them.  This manual process only takes a couple minutes to complete and gives me a chance to write a \"thank you, and sorry to see you go\" email to the fleeting customer.  Manually de-activating (loosing) a customer is also a humbling experience.  Keep a list of nice-to-have automations and review them in the far future.  How far in the future?  Far, like when your idea is validated and your company appears successful.  Don't waste valuable time building automation for a service nobody will pay for or use.  Time is your most scarce resource.\n</p>\n\n<p>\n<strong>4. Always attempt to increase your luck.</strong>  This might sound funny but the the following suggestions should reduce risk and also increase luck: \n<ul>\n\t<li>Meet fellow lucky people</li>\n\n\t<li>Write on a blog</li>\n\t<li>Advertise your company on side-projects that lack monetization</li>\n\t<li>Think before wasting money using adsense or similar networks</li>\n\t<li>Do market research before committing to an idea</li>\n\t<li>Figure out how to make money before building product</li>\n\t<li>Build the smallest version of the product that could still be sold</li>\n\n\t<li>Try to stay focused on the things your customers will see</li>\n\n\t<li>Don't build an admin dashboard until successful (profitable?)</li>\n\n\t<li>Choose tasks that require the least effort but have the biggest impact</li>\n\n\t<li>Don't be afraid to do things in the early stages that won't scale in the long run (example: personal email responses)</li>\n\n\t<li>Listen to how early adopters describe your product; then on your website, marketing and emails reuse their words.</li>\n\n\t<li>Attempt to keep marketing, newsletter, and support emails only few sentences long.</li>\n\n\t<li>Start maintaining an opt-in email list</li>\n</ul>\n\n\nYou might have the best mouse-trap but without luck it will never gain traction. \n</p>\n\n<p>\n<strong>5. Have an unhealthy passion to support your customers and build your company.</strong> Six days ago I suffered a major chemical burn to my right eye.  With my wife's help I continued to respond to support emails.  Since I launched [linkpeek-hover uri=\"http://linkpeek.com\" text=\"LinkPeek\"] my focus and attention to my customers and company have never faltered.  Dedication plays a key role to success.\n</p>\n\n<a href=\"/wp-content/uploads/2012/06/dedication-eye-chemical-burn.jpg\"><img src=\"/wp-content/uploads/2012/06/dedication-eye-chemical-burn.jpg\" alt=\"\" title=\"dedication-eye-chemical-burn\" width=\"800\" height=\"696\" class=\"aligncenter size-full wp-image-2686\" /></a>",
    "date": "2012-06-25 19:50:41",
    "timestamp": 1340668241,
    "comments": [],
    "metadata": {
      "_edit_last": "1",
      "_wp_old_slug": "my-top-five-suggestions-for-an-independent-developer-creating-a-new-service",
      "_spost_short_title": null
    }
  },
  "miniuri-parser-and-ago-human-timedelta": {
    "name": "miniuri-parser-and-ago-human-timedelta",
    "id": "2690",
    "link": "http://russell2.ballestrini.net/miniuri-parser-and-ago-human-timedelta/",
    "title": "miniuri parser and ago human timedelta",
    "content": "<p>\nI finally got around to packaging and publishing a couple of my python modules.\n</p>\n<hr/>\n<p>\n<strong>miniuri:</strong> The Universal URI parser.\n<br/>\n<strong>Install:</strong> <code>easy_install miniuri</code>\n<br/>\n<br/>\n<strong>Pypi</strong>\n<br/>\n&nbsp; [linkpeek-hover uri=\"http://pypi.python.org/pypi/miniuri\"]\n<br/>\n<strong>Public Revision Control</strong>\n<br/>\n &nbsp; [linkpeek-hover uri=\"https://bitbucket.org/russellballestrini/miniuri\"]\n</p>\n<hr/>\n<p>\n<strong>ago:</strong> Human readable timedeltas\n<br/>\n<strong>Install:</strong> <code>easy_install ago</code>\n<br/>\n<br/>\n<strong>Pypi</strong> \n<br/>\n&nbsp; [linkpeek-hover uri=\"http://pypi.python.org/pypi/ago\"]\n<br/>\n<strong>Public Revision Control </strong> \n<br/>\n&nbsp; [linkpeek-hover uri=\"https://bitbucket.org/russellballestrini/ago\"]\n</p>\n<hr/>\nI hope you enjoy them.",
    "date": "2012-06-29 21:30:23",
    "timestamp": 1341019823,
    "comments": [],
    "metadata": {}
  },
  "the-pyramid-community-taught-me-the-importance-of-test-driven-development": {
    "name": "the-pyramid-community-taught-me-the-importance-of-test-driven-development",
    "id": "2766",
    "link": "http://russell2.ballestrini.net/the-pyramid-community-taught-me-the-importance-of-test-driven-development/",
    "title": "The Pyramid community taught me the importance of test driven development",
    "content": "<p>\n<strong>Sontek's patch:</strong> [linkpeek-hover uri=\"https://github.com/Pylons/pyramid/commit/72561a213ccc456738582551e85fab0f0c8d09ab\" text=\"72561a213ccc456738582551e85fab0f0c8d09ab\" size=\"400x400\"]\n</p>\n<p>\nI greeted the UPS man in the middle of the street to sign for my new Lenovo ThinkPad T430.  Because this was My first <em>brand-new</em> laptop purchase I rationalized the time I spent tracking the package from the factory in China to my hands in Connecticut.  Once inside, I opened the box and started installing Fedora 17. I couldn't help but to take in the new-electronics smell.\n</p>\n<p>\nI've been without a laptop for more than a month so I was eager to get my development environment configured.  Most of my tools ship with vanilla Linux, vim, python, hg mercurial, chromium browser, etc.  My first goal was to get a development copy of linkpeek.com running locally.  This took about 5 minutes and it seemed to be working fine until I noticed a few pages had errors.  The errors seemed to be caused by a difference between Pyramid 1.3 and 1.4a1.  But what was failing?\n</p>\n<p>\nI posted a short message in #pyramid about the bug and minutes later I had multiple developers prodding for hints.  \"Could you post the whole traceback?\", \"What does your view look like?\".  I answered quickly and attempted to explain what I thought was going on.  Turns out I was close but before I could finish explaining the problem, sontek had a working one-character-fix and was in the process finishing the tests to prove the patch.  He also explained what I should do in the interim to patch locally.  \n</p>\n<p>\nIn the next 4 hours a pull request was submitted to the upstream master and the patch was peer reviewed, accepted, and integrate by mcdonc.  That impressed me, a lot.  All Pylon Projects have strict policies about test coverage and now I understand why.  Tests not only help produce better bug-free software but also act as a powerful tool when proving the validity of a patch.  I plan to devote the next couple weeks to making test-driven-development a habit.\n</p>",
    "date": "2012-09-22 14:03:08",
    "timestamp": 1348336988,
    "comments": [],
    "metadata": {}
  },
  "explaining-cache-with-python": {
    "name": "explaining-cache-with-python",
    "id": "2805",
    "link": "http://russell2.ballestrini.net/explaining-cache-with-python/",
    "title": "Explaining cache with python",
    "content": "<p>\n<strong>What is cache?</strong> I define cache as \"a saved answer to a question\".  Caching can speed up an application if a computationally complex question is asked frequently.  Instead of the computing the answer over and over, we can use the previously cached answer.  This post will present one method of adding cache to a python program.  Specifically we will write a program that computes prime numbers and saves the answers into cache for quick retrieval. \n</p>\n\n<p>\n<strong>EDIT:</strong> The kind people of the Internet have expressed concern with my loose use of the term cache; the techniques that follow are most accurately described as memoization. \n</p>\n\n<p>\n<strong>One algorithm for determining if a number is prime follows:</strong>\n<pre lang=\"python\">\nprime_flag = True # default state\nx = 5 # number to test\nif x == 1:\n    prime_flag = False\nelse:\n    for i in range( 2, x ):\n        if x % i == 0:\n            prime_flag = False\n            break\nprint prime_flag\n</pre>\n\n<em>Create a prime_flag variable to hold the answer and default it to true.  Let x be the number being tested and if x is equal to 1, the x is not prime.  Otherwise iterate over each number in the range of 2 to x.  Let i be the current number to be tested.  if x is divided by i without any remainder, x is not prime.  Set the prime_flag to False and break out of the loop.  Print the result.  </em>\n\n</p>\n\n\n<p>\n<strong>Next we will move the algorithm into a function which will allow for code reuse:\n</strong>\n\n<pre lang=\"python\">\ndef is_prime( x ):\n    \"\"\"Determine if a number is prime, return Boolean\"\"\"\n    prime_flag = True\n    if x == 1:\n        prime_flag = False\n    else:\n        for i in range( 2, x ):\n            if x % i == 0:\n                prime_flag = False\n                break\n    return prime_flag\n\n# invoke function:\nprint is_prime( 5 ) # True\nprint is_prime( 4 ) # False\n</pre>\n\nThis function saves us a lot of typing and enables the ability to quickly determine if a given number is prime. \n</p>\n\n<p>\n<strong>Next we will use a python dictionary to implement a result cache.</strong>  Also by circumstance we introduce objects and classes. \n<pre lang=\"python\">\nclass Primer( object ):\n    def __init__( self ):\n        \"\"\"create a cache dictionary\"\"\"\n        self.cache = {}\n\n    def is_prime( self, x ):\n        \"\"\"Determine if x is prime, cache and return result\"\"\"\n        if x in self.cache:\n            return self.cache[x] # lookup result\n\n        prime_flag = True\n\n        if x == 1:\n            prime_flag = False\n        else:\n            for i in range( 2, x ):\n                if x % i == 0:\n                    prime_flag = False\n                    break\n\n        self.cache[x] = prime_flag # cache result\n        return prime_flag\n\np = Primer() # create a new primer object\np.is_prime( 5 ) # True\np.is_prime( 4 ) # False\np.is_prime( 5 ) # True and fetched from cache\n</pre>\n\nWhat is great about this solution is that we can avoid looping and computation if an answer is already in cache.  Looking up a cached result is much more efficient and will ultimately make a program feel more responsive.  \n</p>\n\n<p>\n<strong> Determining if 97352613 is prime takes my laptop nearly 18 seconds.</strong>  Fetching the cached result seems to happen instantly.\n\n<pre lang=\"python\">\n>>> s1 = time();p.is_prime( 97352613 );e1 = time()\nFalse # not prime\n>>> s2 = time();p.is_prime( 97352613 );e2 = time()\nFalse # not prime from cache\n>>> e1 - s1\n17.970067977905273 # seconds\n>>> e2 - s2\n2.5987625122070312e-05 # or approx .000026 seconds\n</pre>\n\nA look-up will always beat a computation.  Anything that can be cached, should be cached.  I hope this helps clear things up.\n\n</p>\n\n<img src=\"/wp-content/uploads/2013/03/2013-03-03-explaining-cache-scaled.png\" alt=\"2013-03-03-explaining-cache-scaled\" class=\"aligncenter\" />",
    "date": "2012-10-02 15:22:05",
    "timestamp": 1349205725,
    "comments": [],
    "metadata": {
      "_edit_last": "1",
      "_spost_short_title": null
    }
  },
  "how-to-drain-an-iphone-battery-without-needing-passcode": {
    "name": "how-to-drain-an-iphone-battery-without-needing-passcode",
    "id": "2862",
    "link": "http://russell2.ballestrini.net/how-to-drain-an-iphone-battery-without-needing-passcode/",
    "title": "how to drain an iPhone battery without needing passcode",
    "content": "<ol>\n\t<li>Press home button [ ]</li>\n\t<li>Slide camera button up</li>\n\t<li>Slide mode to video</li>\n\t<li>Turn on flash</li>\n\t<li>Put iPhone on table with light pointed down</li>\n\t<li>Walk away inconspicuously</li>\n</ol>\n\nExtra points if you set the camera to record (just don't record yourself or sounds) which will fill up the iPhone capacity.",
    "date": "2012-11-03 19:27:42",
    "timestamp": 1351985262,
    "comments": [],
    "metadata": {}
  },
  "tips-for-getting-pull-requests-approved": {
    "name": "tips-for-getting-pull-requests-approved",
    "id": "2911",
    "link": "http://russell2.ballestrini.net/tips-for-getting-pull-requests-approved/",
    "title": "Tips for getting pull requests approved",
    "content": "<p>\n<strong>Pull rejection sucks!</strong>\n</p>\n\n<p>\n\nYou have just coded, implemented, and submitted a pull request.  A short while later the request is declined by an upstream maintainer and you feel crushed.  We have all been there.  Today I'm going to show you a better way.  This article will teach you how to create pull requests that get approved.\n\n<p>\n<strong>Start small</strong>\n</p>\n\n<p>\nYou need to earn trust with the maintainers.  Your first commit should be a small change which they <em>cannot</em> reject.  Try to write a missing test or re-factor duplicate code.  Correct a comment's accuracy or rename a variable to better reflect its purpose.  Your first pull request should not alter how the program works. \n</p>\n\n<a href=\"/wp-content/uploads/2012/12/start-small.xcf_.png\"><img src=\"/wp-content/uploads/2012/12/start-small.xcf_.png\" alt=\"start-small.xcf\" class=\"aligncenter\" /></a>\n\n<p>\n<strong>Don't add leading or trailing white space</strong>\n</p>\n<p>\nAdditional whitespace will alter the diff output.  This causes version control systems to flag lines as changed which is irritating and sometimes misleading.  \n</p>\n\n<a href=\"/wp-content/uploads/2012/12/spot-the-diff.xcf_.png\"><img src=\"/wp-content/uploads/2012/12/spot-the-diff.xcf_.png\" alt=\"spot-the-diff.xcf\" class=\"aligncenter\" /></a>\n\n\n<p>\n<strong>Less is more</strong>\n</p>\n<p>\nMinimize the number of changes to accomplish your goal.  People are busy and at times lazy.  Reduce the work the maintainers must do to perform a merge.  Lowering the amount of lines to review should increase the chance of approval.\n</p>\n\n<a href=\"/wp-content/uploads/2012/12/less-is-more.xcf_.png\"><img src=\"/wp-content/uploads/2012/12/less-is-more.xcf_.png\" alt=\"less-is-more.xcf\" class=\"aligncenter\" /></a>\n\n<p>\n<strong>Only commit working code</strong>\n</p>\n<p>\nDo not break the program, only commit working code.  If the project has tests make sure they work before you commit.\n</p>\n\n<a href=\"/wp-content/uploads/2012/12/commit-working-code.xcf_.png\"><img src=\"/wp-content/uploads/2012/12/commit-working-code.xcf_.png\" alt=\"commit-working-code.xcf\" class=\"aligncenter\" /></a>\n\n<p>\n<strong>Follow the leader</strong>\n</p>\n<p>\nTry to mimic the maintainers.  Follow the coding style and project layout even if it seems wrong.  This is not your playground yet.  Before you commit, review the VCS logs to learn how verbose or terse your commit messages should be.  When in Rome do as the romans do. This silly game of follow the leader reduces friction of an outsider committing to the project.  \n</p>\n\n<a href=\"/wp-content/uploads/2012/12/follow-the-leader.xcf_.png\"><img src=\"/wp-content/uploads/2012/12/follow-the-leader.xcf_.png\" alt=\"follow-the-leader.xcf\" class=\"aligncenter\" /></a>\n\n<p>\n<strong>Comments, docs, and tests oh my!</strong>\n</p>\n<p>\nYour first pull request should clarify or add to the existing documentation.  Fix the README, adjust a comment, or write a test.  These tasks might appear small but they serve to prove that you possess comprehension of the source code.  They also do not alter the program's functionality.  Having a few of these pull requests under-your-belt will earn you trust which will eventuality translate to more responsibly in the future.  You will also differentiate yourself from the rest because most people do not enjoy working on documentation.\n</p>\n\n<a href=\"/wp-content/uploads/2012/12/comments-docs-tests-oh-my.xcf_.png\"><img src=\"/wp-content/uploads/2012/12/comments-docs-tests-oh-my.xcf_.png\" alt=\"comments-docs-tests-oh-my.xcf\" class=\"aligncenter\" /></a>\n\n<p>\n<strong>Blog about the change</strong>\n</p>\n<p>\nWrite about the change, give reasons and examples.  Include a link to your blog post in the pull request.  \n</p>\n\n<a href=\"/wp-content/uploads/2012/12/ideas-blog-get-heard.xcf_.png\"><img src=\"/wp-content/uploads/2012/12/ideas-blog-get-heard.xcf_.png\" alt=\"ideas-blog-get-heard.xcf\" class=\"aligncenter\" /></a>\n\n<p>\n<strong>Pull requests are like paragraphs</strong>\n</p>\n<p>\nIf you were writing an essay, you would split up your ideas into separate paragraphs.  A pull request has many qualities similar to a paragraph.  Each commit should be related to the pull request's main objective.  Commits of a pull request should stay focused and on topic.\n</p>\n<blockquote>A pull request is to a program as a paragraph is to an essay.\n<br/>  A commit is to a pull request as a sentence is to a paragraph.</blockquote>\n<p>\nIn an essay, if you have more then one topic you should have more then one paragraph.  Likewise when coding, only one idea or change per pull request.\n</p>\n<p>Separating your ideas into different pull requests will grant the maintainers greater flexibility when they begin to integrate.  They will have the ability to pick-and-choose which requests to merge and everybody wins!\n</p>\n\n<a href=\"/wp-content/uploads/2012/12/pull-request-stay-focused.xcf_.png\"><img src=\"/wp-content/uploads/2012/12/pull-request-stay-focused.xcf_.png\" alt=\"pull-request-stay-focused.xcf\" class=\"aligncenter\" /></a>",
    "date": "2012-12-12 12:50:51",
    "timestamp": 1355334651,
    "comments": [],
    "metadata": {
      "_spost_short_title": null,
      "_wp_old_slug": "tips-and-tricks-to-have-all-your-pull-requests-approved-and-merged-upstream",
      "_edit_last": "1"
    }
  },
  "ago-py-human-readable-timedelta0-0-4-release": {
    "name": "ago-py-human-readable-timedelta0-0-4-release",
    "id": "2993",
    "link": "http://russell2.ballestrini.net/ago-py-human-readable-timedelta0-0-4-release/",
    "title": "ago.py human readable timedelta 0.0.4 release",
    "content": "<br/>\n<strong>We have released ago.py 0.0.4</strong>\n\n<p>\npypi: <a href=\"http://pypi.python.org/pypi/ago\">http://pypi.python.org/pypi/ago</a><br/>\nrepo: <a href=\"https://bitbucket.org/russellballestrini/ago\">https://bitbucket.org/russellballestrini/ago</a>\n</p>\n\n<p>\nSpecial thanks to David Beitey for supplying <a href=\"http://davidjb.com/\">ideas and python code</a> for this update!\n</p>\n\n<p>\nAll changes are backward compatible.\n</p>\n\nChange log:\n<ul>\n <li>added support for future dates</li>\n <li>added optional past_tense keyword argument string for custom output</li>\n <li>added optional future_tense keyword argument string for custom output</li>\n <li>added 13 tests to help prevent regressions</li>\n <li>Updated the README.rst documentation for better coverage</li>\n</ul>\n\n<p>We were just shy of 800 downloads on pypi for ago-0.0.3, I hope ago-0.0.4 will perform even better!</p>",
    "date": "2013-01-09 00:04:30",
    "timestamp": 1357707870,
    "comments": [],
    "metadata": {
      "_spost_short_title": null,
      "_edit_last": "1"
    }
  },
  "how-to-overload-default-function-arguments-in-python-using-lambda": {
    "name": "how-to-overload-default-function-arguments-in-python-using-lambda",
    "id": "3046",
    "link": "http://russell2.ballestrini.net/how-to-overload-default-function-arguments-in-python-using-lambda/",
    "title": "How to overload default function arguments in python using lambda",
    "content": "<p>\nPython Lambda functions are very powerful but I often forget how they work or the fun things they do.  This post will document how to use a lambda to provide different default arguments to a function.\n</p>\n\n<p>\nWe will use the <a href=\"https://bitbucket.org/russellballestrini/ago/overview\">human function found in ago.py</a> as an example - because I'm the module author and I really like it.  Lets use the interactive python interpreter to run help on the human function. \n</p>\n\n<p>\n<pre>\n>>> from ago import human\n>>> help( human )\n\nHelp on function human in module ago:\n\nhuman(dt, precision=2, past_tense='{} ago', future_tense='in {}')\n    Accept a datetime or timedelta, return a human readable delta string\n\n</pre>\n</p>\n\n<p>\nShown above the human function accepts 1 argument and 3 named keyword arguments.  The dt argument must be a datetime or timedelta object, the precision must be an integer, and the other two must be strings.  If we didn't like the default arguments, we would need to specify (or pass in) new values each time we invoked the function.  \n</p>\n<p>Example: <code>human(dt, 3, 'this happened {} ago!', 'in {} from now!')</code>.  If we know we will always want different default arguments we can create a lambda function to shorten the invocation length.\n</p>\n\n<pre>\n>>> h = lambda dt : human(dt, 3, 'this happened {} ago!', 'in {} from now!')\n>>> print h( dt ) # h is much shorter then human, and still reusable!\n</pre>\n\n<p>\nAbove creates a new function h who only accepts one argument dt.  This function calls human with our default arguments.\nThis lambda is equivalent to this regular python function:\n</p>\n<pre>\n>>> def h( dt ):\n...    return human(d, 3, 'this happened {} ago!', 'in {} from now!')\n>>> print h( dt ) # h is much shorter then human, and still reusable!\n</pre>\n\n<p>\nHere is a working example to show the new lambda function h in action:\n\n<pre>\n>>> from datetime import datetime\n>>> from datetime import timedelta\n>>> from ago import human\n>>> \n>>> h = lambda dt : human(dt, 3, 'this happened {} ago!', 'in {} from now!')\n>>> \n>>> present = datetime.now()\n>>> \n>>> for i in range( 1, 15 ):\n...     if i % 2 == 0:\n...         new_date = present - timedelta( i, i * i, i * i * i )\n...     else:\n...         new_date = present + timedelta( i, i * i, i * i * i )\n...     h( new_date )\n... \n'in 22 hours, 9 minutes, 30 seconds from now!'\n'this happened 2 days, 1 hour, 50 minutes ago!'\n'in 2 days, 22 hours, 9 minutes from now!'\n'this happened 4 days, 1 hour, 50 minutes ago!'\n'in 4 days, 22 hours, 9 minutes from now!'\n'this happened 6 days, 1 hour, 51 minutes ago!'\n'in 6 days, 22 hours, 10 minutes from now!'\n'this happened 8 days, 1 hour, 51 minutes ago!'\n'in 8 days, 22 hours, 10 minutes from now!'\n'this happened 10 days, 1 hour, 52 minutes ago!'\n'in 10 days, 22 hours, 11 minutes from now!'\n'this happened 12 days, 1 hour, 52 minutes ago!'\n'in 12 days, 22 hours, 12 minutes from now!'\n'this happened 14 days, 1 hour, 53 minutes ago!'\n\n</pre>",
    "date": "2013-01-12 11:22:28",
    "timestamp": 1358007748,
    "comments": [],
    "metadata": {
      "_edit_last": "1",
      "_spost_short_title": null
    }
  },
  "virt-backs-domfetcher-class-returns-doms-from-libvirt-api": {
    "name": "virt-backs-domfetcher-class-returns-doms-from-libvirt-api",
    "id": "3089",
    "link": "http://russell2.ballestrini.net/virt-backs-domfetcher-class-returns-doms-from-libvirt-api/",
    "title": "virt-back's Domfetcher class returns doms from libvirt API",
    "content": "<a href=\"/wp-content/uploads/2013/03/2013-03-02-scale-virt-back-domfetcher-scaled.png\"><img src=\"/wp-content/uploads/2013/03/2013-03-02-scale-virt-back-domfetcher-scaled.png\" alt=\"2013-03-02-scale-virt-back-domfetcher-scaled\" width=\"450\" class=\"alignright\" /></a>\n\n<p>\n<strong>I'm hooked...</strong>\n</p>\n\n<p>I attended SCaLE 11x, my first technical conference, and had an amazing time.  My favorite talk was Michael Day's \"<a href=\"http://code.ncultra.org/2013/02/scale-11x-open-virtualization/\">Advancements with Open Virtualization & KVM</a>\" (link to slides).  Michael's presentation inspired me to continue my work on virt-back.  \n</p>\n\n<p>\nDuring my trip home I used the in-flight wifi to push this <a href=\"https://bitbucket.org/russellballestrini/virt-back/commits/d6dff27323650bf784cc284f676299ffe07953cb\">commit</a> into the cloud from the clouds!  This particular commit re-factored the dom object list generation into a simple-to-use class called Domfetcher.  Domfetcher abstracts the libvirt API and grants access to the following helper methods:\n</p>\n\n<p>\n<strong> get_all_doms( )</strong><br/>\n  Return a list of all dom objects\n</p>\n\n<p>\n<strong> get_doms_by_names( guest_names=[] )</strong><br/>\n  Accept a list of guest_names, return a list of related dom objects\n</p>\n\n<p>\n<strong> get_running_doms( )</strong><br/>\n  Return a list of running dom objects\n</p>\n\n<p>\n<strong> get_shutoff_doms( )</strong><br/>\n  Return a list of shutoff but defined dom objects\n</p>\n\n<p>\n<strong>This is an example of how to use Domfetcher:</strong>\n</p>\n\n<p>\n<pre lang=\"python\">import virtback\n\n# optionally supply hypervisor uri\ndomfetcher = virtback.Domfetcher()\n\ndoms = domfetcher.get_running_doms()\n\nfor dom in doms:\n    print dom.name()\n\nfor dom in doms:\n    print dom.info()\n\nfor dom in doms:\n    print dom.shutdown()\n\n</pre>\n</p>\n\n\nAs always thanks for reading!",
    "date": "2013-03-02 15:04:01",
    "timestamp": 1362254641,
    "comments": [],
    "metadata": {
      "_spost_short_title": null,
      "_series_part": "3",
      "_edit_last": "1"
    }
  },
  "automatic-event-hangout-with-cron": {
    "name": "automatic-event-hangout-with-cron",
    "id": "3139",
    "link": "http://russell2.ballestrini.net/automatic-event-hangout-with-cron/",
    "title": "Automatic event hangout with cron",
    "content": "<p><strong>Create an online only, hangout event</strong></p>\n\n<p>\nCreate a new event with a date far into the future, like the year 2015.  Go to the event's options > advanced and enable 'this event is online only' which will create a unique Hangout URI.\n</p>\n\n<p>\n<strong>Create a cronjob</strong>\n</p>\n<p> \nCreate a cronjob on each device to open the web browser Monday-Friday at 12:49pm and open the unique Hangout URI.\n</p>\n\n<p>\nFirefox cron example:\n<pre># Run Firefox at 12:49 EST each weekday and open hangout URI\n49 12 * * 1-5 export DISPLAY=:0 && /usr/bin/firefox 'hangout-uri'\n</pre>\n</p>\n\n<p>\nChromium-browser cron example:\n<pre># Run Chromium-browser at 12:49 EST each weekday and open hangout URI\n49 12 * * 1-5 export DISPLAY=:0 && /usr/bin/chromium-browser 'hangout-uri'\n</pre>\n</p>",
    "date": "2013-04-09 09:17:09",
    "timestamp": 1365513429,
    "comments": [],
    "metadata": {
      "_edit_last": "1",
      "_spost_short_title": null,
      "_wp_old_slug": "use-cron-and-events-to-automatically-join-a-google-hangouts"
    }
  },
  "guido-name-dropped-tornado-python-tulip-and-pep-3156": {
    "name": "guido-name-dropped-tornado-python-tulip-and-pep-3156",
    "id": "3148",
    "link": "http://russell2.ballestrini.net/guido-name-dropped-tornado-python-tulip-and-pep-3156/",
    "title": "Guido name dropped tornado python tulip and pep-3156",
    "content": "<p>\nPycon 2013 was excellent, in fact it was my first one I have attended.\n</p>\n\n<p>\nI found it odd that django and Pyramid had plenty of talks but nobody mentioned tornado.\n</p>\n\n<p>\nThe only person that brought up tornado was Guido himself, who has been researching and developing async python since December 12th, 2012.  Guido wants to add async API libraries to python core, and has been comparing his work to twisted and more importantly tornado.  He is leveraging the existing solutions to stay on the correct track. \n</p>\n\n<p>\nAsync API's in Python core!  This news was extra exciting for me, because I have already learned the power of async by messing around with tornado.  Since the talk many people have approached me and asked for more information about async API's.\n</p>\n\n<p>\nMy favorite use for async is long polling for web applications.  Here is a diagram that shows how great tornado is:\n</p>\n\n<p>\n<a href=\"/wp-content/uploads/2013/03/2013-03-22-tornado-callback-long-polling-event-loop-scaled.png\"><img src=\"/wp-content/uploads/2013/03/2013-03-22-tornado-callback-long-polling-event-loop-scaled.png\" alt=\"2013-03-22-tornado-callback-long-polling-event-loop-scaled\" width=\"1400\" height=\"609\" class=\"aligncenter size-full wp-image-3149\" /></a>\n</p>\n\n\n<p>\nThis type of polling can support thousands of concurrent users, all connected and waiting for a callback event to occur.  I used tornado to build <a href=\"http://four2go.gumyum.com\" title=\"http://four2go.gumyum.com\">four2go</a>, a real-time and multi-player browser-based-game.</p>",
    "date": "2013-03-22 22:07:59",
    "timestamp": 1364004479,
    "comments": [],
    "metadata": {
      "_edit_last": "1",
      "_spost_short_title": null
    }
  },
  "survey-baby-monkey": {
    "name": "survey-baby-monkey",
    "id": "3189",
    "link": "http://russell2.ballestrini.net/survey-baby-monkey/",
    "title": "Survey Baby Monkey",
    "content": "<img src=\"/wp-content/uploads/2013/04/baby-monkey-fxhp.jpg\" alt=\"baby-monkey-fxhp\" width=\"800\" height=\"693\" class=\"aligncenter size-full wp-image-3190\" />",
    "date": "2013-04-26 13:47:38",
    "timestamp": 1366998458,
    "comments": [],
    "metadata": {
      "_edit_last": "1",
      "_spost_short_title": null
    }
  },
  "honey-i-just-deleted-linkpeek-com": {
    "name": "honey-i-just-deleted-linkpeek-com",
    "id": "3199",
    "link": "http://russell2.ballestrini.net/honey-i-just-deleted-linkpeek-com/",
    "title": "Honey!  I just DELETED LinkPeek.com",
    "content": "<p>\nDuring the day I am an ops sys-admin.  During the night I am a husband, father of two, and a CEO of a bootstrapped start-up.  After launch, my first project was to schedule regular backups of user data and archive off-site.  My goal was to <a href=\"http://russell.ballestrini.net/virt-back-restoring-from-backups/\">create backups but never need them</a>.  Boy was I lucky ...\n</p>\n\n<p>\nYes, leave it to me to inadvertently delete the VPS root disk.  One of the major cloud providers places the \"rename\" and \"remove\" disk buttons right next to each other and I learned a nasty habit of clearing pop-ups without reading them (thanks Windows).\n</p>\n\n<blockquote>\"Honey!  I just deleted LinkPeek.com\"</blockquote>\n\n<p>\nThe horror... My stomach felt like I took a tumble in a roller-coaster.  Instantly I tossed off my developer hat and put on my operations hat.  I checked the off-site backups.  I had nightly dumps of MongoDB and weekly tar backups of the /etc partition.  The user data was in MongoDB and most of the system configuration information was in the tar.  I used the tar to recover 2 upstart scripts, 2 supervisord scripts, 2 complex nginx confs, an ssl cert, and the pyramid production.ini.\n</p>\n\n<p>\nI set out to stand up a new server, re-install the needed packages, recover the user data, and restore the service. After 1.5 hours of feverishly typing [linkpeek-hover uri=\"http://linkpeek.com/\" text=\"LinkPeek\"] was back online.\n</p>\n\n<p>\n<strong>What I learned and my plan going forward</strong>\n</p>\n\n<p>\nIf you are a small team or a start-up, you must have somebody dedicated to operations.  Without backups I would not have been able to gracefully recover.  Most likely I would have reimbursed the existing members and shuttered the doors.  \n</p>\n\n<p>\nThis experience was eye-opening.  In my next couple of posts I will explain how I <a href=\"/automatic-backups/\">create and maintain backups</a> and my next project will implement a configuration management and provisioning system.\n</p>\n\n<p>\nThis system will allow me to:  \n</p>\n\n<ol>\n\t<li>take out the human element of recovery</li>\n\t<li>significantly reduce the time-to-recover from a catastrophic failure</li>\n\t<li>test disaster recovery procedures before needing them</li>\n\t<li>provision development and production environments without effort</li>\n\t<li>have a reproducible blueprint of \"how to build a LinkPeek server\"</li>\n</ol>",
    "date": "2013-05-26 13:19:46",
    "timestamp": 1369588786,
    "comments": [],
    "metadata": {
      "_edit_last": "1",
      "_spost_short_title": null
    }
  },
  "high-load-and-cpu-usage-craftbukkit-compared-to-vanilla-minecraft": {
    "name": "high-load-and-cpu-usage-craftbukkit-compared-to-vanilla-minecraft",
    "id": "3244",
    "link": "http://russell2.ballestrini.net/high-load-and-cpu-usage-craftbukkit-compared-to-vanilla-minecraft/",
    "title": "High load and CPU usage craftbukkit compared to vanilla minecraft",
    "content": "<p>I started researching the best ways to use <a href=\"http://bobbylikeslinux.net/salt-minecraft-fun.html\">salt to provision minecraft servers</a>.  I wrote a salt state formula for the vanilla minecraft server deployment.  The deployment worked out great so I decided to try my luck with plugins.</p>\n\n<p>In order to use plugins and mods we need to use a customized server package.  I decided to try provisioning a craftbukkit server and quickly noticed something was very wrong.  Vanilla Craftbukkit (with no plugins) produces very high load and CPU usage.</p>\n\n<p>\nHere is a chart for proof.  The spike is when I stopped the vanilla minecraft server and started the craftbukkit minecraft server:</p>\n\n<p>\n<a href=\"/wp-content/uploads/2013/06/minecraft-vs-craftbukkit.png\"><img src=\"/wp-content/uploads/2013/06/minecraft-vs-craftbukkit.png\" alt=\"minecraft-vs-craftbukkit\" width=\"508\" height=\"390\" class=\"aligncenter size-full wp-image-3245\" /></a>\n</p>",
    "date": "2013-06-20 23:35:39",
    "timestamp": 1371785739,
    "comments": [],
    "metadata": {
      "_edit_last": "1",
      "_spost_short_title": null
    }
  },
  "add-a-custom-header-to-all-salt-managed-files-using-pillar-and-jinja-templates": {
    "name": "add-a-custom-header-to-all-salt-managed-files-using-pillar-and-jinja-templates",
    "id": "3258",
    "link": "http://russell2.ballestrini.net/add-a-custom-header-to-all-salt-managed-files-using-pillar-and-jinja-templates/",
    "title": "Add a custom header to all Salt managed files using pillar and jinja templates",
    "content": "<p>\nSalt-stack (salt) provides a solution for centralized configuration management and remote execution.  One of the most basic things Salt provides is the ability to manage the contents of a file or a directory of files.  Using Salt we can dictate the state of our minions and as a result we also gain auto-healing of configuration files.\n</p>\n\n<p>\nSalt will clobber local changes to managed files and force the state to reflect the version in configuration management.  In an effort to avoid confusing the uninformed, I place a header on each managed file which announces \"THIS FILE IS MANAGED BY SALT\".\n</p>\n\n<p>\nTo avoid repeating myself in each managed file, I came up with the following centralized solution -\n<p>\n\n<p>\nFirst, I give all minions access to the headers pillar tree in top.sls:\n</p>\n<pre lang=\"yaml\">base:\n  '*':\n    - headers\n</pre>\n\n<p>\nNext, I create a couple headers in in headers/init.sls:\n</p>\n<pre lang=\"yaml\">\nheaders:\n  salt:\n    file: |\n        ################################\n        # THIS FILE IS MANAGED BY SALT #\n        ################################\n    directory: |\n        #####################################\n        # THIS DIRECTORY IS MANAGED BY SALT #\n        #####################################\n</pre>\n\n<p>\nThen, I parse each managed file in the state tree with jinja, for example hosts/init.sls:\n</p>\n\n<pre lang=\"yaml\">/etc/hosts:\n  file.managed:\n    source: salt://hosts/hosts\n    user: root\n    group: group\n    mode: 644\n    template: jinja\n</pre>\n\n<p>\nFinally, in each file served by salt I add the jinja header substitution, for example hosts/hosts:\n<p>\n\n<pre lang=\"yaml\">{{pillar['headers']['salt']['file']}}\n127.0.0.1\t\tlocalhost.localdomain localhost\n::1\t\tlocalhost6.localdomain6 localhost6\n</pre>\n\nI use this same technique to declare most configuration options as parameters, like hostnames, IP addresses, ports, and more.",
    "date": "2013-07-01 14:11:02",
    "timestamp": 1372702262,
    "comments": [
      {
        "id": 178951,
        "parent_id": 0,
        "author_ip": "127.0.0.1",
        "author": "HenryTK",
        "email": "henry@guitarscholar.co.uk",
        "content": "This is super handy as I didn't know you could inject multi-line pillar values. Thanks.",
        "date": "2016-02-10 05:11:33",
        "timestamp": 1455099093
      },
      {
        "id": 175335,
        "parent_id": 172790,
        "author_ip": "127.0.0.1",
        "author": "Russell Ballestrini",
        "email": "russell.ballestrini@gmail.com",
        "content": "Yes, it still works for me.",
        "date": "2015-11-08 20:50:21",
        "timestamp": 1447033821
      },
      {
        "id": 172790,
        "parent_id": 0,
        "author_ip": "127.0.0.1",
        "author": "wilonkayt",
        "email": "no@spam.com",
        "content": "Does this currently still work for you I keep getting is not formed as a list when trying this as is in the tutorial. Thanks,",
        "date": "2015-10-16 18:14:45",
        "timestamp": 1445033685
      }
    ],
    "metadata": {
      "_stcr@_henry@guitarscholar.co.uk": "2016-02-10 05:11:33|Y",
      "_wp_old_slug": "add-a-custom-header-to-all-salt-managed-files-using-a-pillar-templates",
      "_edit_last": "1",
      "_spost_short_title": null,
      "_stcr@_no@spam.com": "2015-10-16 18:14:45|Y"
    }
  },
  "create-your-own-fleet-of-servers-with-digital-ocean-and-salt-cloud": {
    "name": "create-your-own-fleet-of-servers-with-digital-ocean-and-salt-cloud",
    "id": "3293",
    "link": "http://russell2.ballestrini.net/create-your-own-fleet-of-servers-with-digital-ocean-and-salt-cloud/",
    "title": "Create your own fleet of servers with Digital Ocean and salt-cloud",
    "content": "<p>\nHave you heard about Digital Ocean?  They offer a polished user interface, KVM guests with SSD storage, and an API to interact with a cloud of hypervisors.  API integration got you down?  Don't worry, salt-cloud has already integrated Digital Ocean among it's list of providers!  The rest of this post illustrates the steps I took to configure salt-cloud to work with Digital Ocean.\n</p>\n\n<p>\nThis guide assumes the following: You already have a salt-master.  You already have a <a href=\"https://www.digitalocean.com/?refcode=27e015299dc7 \">Digital Ocean</a> account.\n</p>\n\n<p><b>Step one,</b> install the most recent version of salt-cloud (salt-cloud==2015.5.0) on the salt-master:\n\n<pre lang=\"bash\">\nsudo apt-get install salt-cloud\n\n# or if you prefer ...\npip install salt-cloud==2015.5.0\n\n# last verify it was successfully installed\nsalt-cloud --version\n</pre>\n</p>\n\n<p><b>Step two,</b> configure salt-cloud.  Salt-cloud uses the following files YAML files for configuration:</p>\n\n<p>\n<i>/etc/salt/cloud.conf.d/main.conf</i>\n<br/>\n&nbsp; &nbsp; This is the main configuration file.  I have the following statements:\n<pre lang=\"yaml\">\nminion:\n    master: master.foxhop.net\n    append_domain: foxhop.net\n</pre>\n<p/>\n\n<p>\n<i>/etc/salt/cloud.providers/do.conf</i>\n<br/>\n&nbsp; &nbsp; This is a provider configuration file for Digital Ocean (do). Collect your client_key and personal_access_token (api_key) from the Digital Ocean user dashboard.  Also create an SSH key and add the public key using the dashboard:\n<pre lang=\"yaml\"># For Digital Ocean\ndo:\n  provider: digital_ocean\n  client_key: MyClientKeyLiftedFromDashboard\n  personal_access_token: MyAPIKeyLiftedFromDashboard\n  ssh_key_file: /keys/digital-ocean-salt-cloud\n  ssh_key_name: digital-ocean-salt-cloud.pub\n</pre>\n<p/>\n\n<p>\n<i>/etc/salt/cloud.profiles/do.conf</i>\n<br/>\n&nbsp; &nbsp; This is the Digital Ocean profiles configuration file.  We will create just two profiles for now, but you can create unlimited named combinations.\n<pre lang=\"yaml\">ubuntu-12-04-do-512:\n  provider: do\n  image: ubuntu-12-04-x64\n  size: 512mb\n  location: nyc1\n\nubuntu-14-04-do-512:\n  provider: do\n  image: ubuntu-14-04-x64\n  size: 512mb\n  location: nyc1\n</pre>\n<p/>\n\n<p>\n<i>ssh_key_file</i><br/>\n&nbsp; &nbsp; This is your private SSH key located on your salt-master\n<p/>\n\n<p>\n<i>ssh_key_name</i><br/>\n&nbsp; &nbsp; This is the name of the public key you added in your Digital Ocean dashboard\n<p/>\n\n<p>\n<i>size</i><br/>\n&nbsp; &nbsp; The size or plan you would like to provision,\n512mb is the smallest plan\n</p>\n\n<p>\n<i>location</i><br/>\n&nbsp; &nbsp; The geographical region, location, and/or data center\n</p>\n\n<p>\n<i>image</i><br/>\n&nbsp; &nbsp; The operating system image\n</p>\n\n<p>\nAfter you configure the do provider in /etc/salt/cloud.providers you gain access to the following commands: \n<pre lang=\"bash\">salt-cloud --list-sizes do\nsalt-cloud --list-locations do\nsalt-cloud --list-images do\nsalt-cloud --help\n</pre>\n</p>\n\n<p><strong>Lets provision a new cloud server!</strong></p>\n<pre lang=\"bash\">salt-cloud --profile ubuntu-14-04-do-512 deejay\n</pre>\n\n<p>If all goes well you should have a newly provisioned server bootstrapped with salt-minion.  The new minion's keys are already added to the salt-master.  Now you just need to run highstate!</p>",
    "date": "2013-07-02 22:20:40",
    "timestamp": 1372818040,
    "comments": [
      {
        "id": 16173,
        "parent_id": 0,
        "author_ip": "127.0.0.1",
        "author": "Ifiok Jr",
        "email": "ifiokotung@gmail.com",
        "content": "This is sooo useful to me right now! Thanks Russell",
        "date": "2013-09-11 02:46:08",
        "timestamp": 1378881968
      },
      {
        "id": 18473,
        "parent_id": 0,
        "author_ip": "127.0.0.1",
        "author": "NA",
        "email": null,
        "content": "Just side note, make sure you copy your key exactly as it was created from ssh-keygen into digital ocean. It'll save you from having to debug that.",
        "date": "2013-12-18 02:28:13",
        "timestamp": 1387351693
      },
      {
        "id": 17704,
        "parent_id": 0,
        "author_ip": "127.0.0.1",
        "author": "Pablo Carranza",
        "email": "pablo@vdevices.com",
        "content": "I tried both <code>easy_install salt-cloud==0.8.9</code> and <code>pip install salt-cloud==0.8.9</code>; but received the following responses: (i) <code>-bash: easy_install: command not found</code> and <code>-bash: pip: command not found</code>, respectively.\n\nIs there a prerequisite package/program I need to install, first?",
        "date": "2013-10-22 22:02:48",
        "timestamp": 1382493768
      },
      {
        "id": 17715,
        "parent_id": 17704,
        "author_ip": "127.0.0.1",
        "author": "Russell Ballestrini",
        "email": "russell.ballestrini@gmail.com",
        "content": "Yes, depending on your operating system, you might need to install 'pip' or 'easy_install'.\nBoth of these tools are used to install Python applications.\n\nFor Ubuntu to get the easy_install command run:\n<pre>\nsudo apt-get install python-setuptools\n</pre>\n\nFor Ubuntu to get the pip command run:\n\n<pre>\nsudo apt-get install python-pip\n</pre>",
        "date": "2013-10-23 15:09:14",
        "timestamp": 1382555354
      }
    ],
    "metadata": {
      "_edit_last": "1",
      "_spost_short_title": null,
      "_stcr@_pablo@vdevices.com": "2013-10-22 22:02:48|Y"
    }
  },
  "understanding-salt-stack-user-and-group-management": {
    "name": "understanding-salt-stack-user-and-group-management",
    "id": "3340",
    "link": "http://russell2.ballestrini.net/understanding-salt-stack-user-and-group-management/",
    "title": "Understanding Salt Stack user and group management",
    "content": "This state will create a user:\n\n<pre lang=\"yaml\">russell:\n  user: \n    - present</pre>\n\nThis state will create a user and a group.  This also makes the user part of the group, and handles creating the group first:\n\n<pre lang=\"yaml\">russell:\n  group:\n    - present\n  user: \n    - present\n    - groups:\n      - russell \n    - require:\n      - group: russell\n</pre>\n\nThis state handles user and group generation along with password and ssh-key maintenance.  This is all done securely using pillar to parameterize arguments:\n\n\n<pre lang=\"yaml\">\n# This state will create users accounts \n#\n# This state requires a pillar named 'users' with data formatted like:\n# \n# users:\n#\n#  tusername:\n#    fullname: Test Username\n#    uid: 1007\n#    gid: 1007\n#    groups:\n#      - sudo\n#      - ops\n#    crypt: $password-hash-sha512-prefered\n#    pub_ssh_keys:\n#      - ssh-rsa list-of-public-keys tusername-sm\n#\n#  anotheruser: ... snipped ...\n\n# loop over all users presented by pillar:\n# create user's group, create user, then add pub keys\n{% for username, details in pillar.get('users', {}).items() %}\n{{ username }}:\n\n  group:\n    - present\n    - name: {{ username }}\n    - gid: {{ details.get('gid', '') }}\n\n  user:\n    - present\n    - fullname: {{ details.get('fullname','') }}\n    - name: {{ username }}\n    - shell: /bin/bash\n    - home: /home/{{ username }}\n    - uid: {{ details.get('uid', '') }}\n    - gid: {{ details.get('gid', '') }}\n    - crypt: {{ details.get('crypt','') }}\n    {% if 'groups' in details %}\n    - groups:\n      {% for group in details.get('groups', []) %}\n      - {{ group }}\n      {% endfor %}\n    - require:\n      {% for group in details.get('groups', []) %}\n      - group: {{ group }}\n      {% endfor %}\n    {% endif %}\n\n  {% if 'pub_ssh_keys' in details %}\n  ssh_auth:\n    - present\n    - user: {{ username }}\n    - names:\n    {% for pub_ssh_key in details.get('pub_ssh_keys', []) %}\n      - {{ pub_ssh_key }}\n    {% endfor %}\n    - require:\n      - user: {{ username }}\n  {% endif %}\n\n{% endfor %}\n\n</pre>",
    "date": "2013-07-08 13:51:30",
    "timestamp": 1373305890,
    "comments": [
      {
        "id": 17939,
        "parent_id": 0,
        "author_ip": "127.0.0.1",
        "author": "Stephen Wood",
        "email": "smwood4@gmail.com",
        "content": "Worked like a charm. Thanks for sharing this state recipe.",
        "date": "2013-11-04 17:59:10",
        "timestamp": 1383605950
      },
      {
        "id": 86788,
        "parent_id": 0,
        "author_ip": "127.0.0.1",
        "author": "luupux",
        "email": "luupuxall@gmail.com",
        "content": "Many tHanks This is Very Good example for novice user",
        "date": "2014-11-06 16:11:27",
        "timestamp": 1415308287
      },
      {
        "id": 182223,
        "parent_id": 0,
        "author_ip": "127.0.0.1",
        "author": "Highloop",
        "email": "igonzalezvaliente@gmail.com",
        "content": "Just curious about the - crypt: argument; \nlooking on the doc, I can't find it. The most similar one is - password...\nIs that yet right for 2015.8.8 ?",
        "date": "2016-05-11 11:11:13",
        "timestamp": 1462979473
      },
      {
        "id": 178471,
        "parent_id": 0,
        "author_ip": "127.0.0.1",
        "author": "LerpDurp",
        "email": "matthew.herzog@gmail.com",
        "content": "What should the name of the file be and what pathname should it have? I know these could be almost anything, but a hint would help the novice.",
        "date": "2016-01-23 15:57:28",
        "timestamp": 1453582648
      }
    ],
    "metadata": {
      "_stcr@_igonzalezvaliente@gmail.com": "2016-05-11 11:11:13|Y",
      "_stcr@_matthew.herzog@gmail.com": "2016-01-23 15:57:28|Y",
      "_stcr@_smwood4@gmail.com": "2013-11-04 17:59:10|Y",
      "_edit_last": "1",
      "_spost_short_title": null,
      "_stcr@_luupuxall@gmail.com": "2014-11-06 16:11:27|Y"
    }
  },
  "configuration-management-vs-remote-execution": {
    "name": "configuration-management-vs-remote-execution",
    "id": "3351",
    "link": "http://russell2.ballestrini.net/configuration-management-vs-remote-execution/",
    "title": "Configuration Management vs Remote Execution",
    "content": "<img src=\"/wp-content/uploads/2013/07/config-mangement-vs-remote-execution.png\" alt=\"config-mangement-vs-remote-execution\" class=\"aligncenter\" style=\"box-shadow: 0px 0px 0px 0px;-webkit-box-shadow: 0px 0px 0px 0px;moz-box-shadow: 0px 0px 0px 0px;\" />\n\n<br/>\n\n<p><strong>What is configuration management?</strong></p>\n\n<p>\nIn a perfect world configuration management provides a centralized, revision controlled, self-documented, change management location for manifests and formulas which both define how to build a complete system and organize a means of knowledge transfer.  An infrastructure perfectly described in configuration management allows any single part of the system to be created, reproduced, multiplied, self-healed or even re-purposed. \n</p>\n\n<p><strong>When should I use configuration management?</strong></p>\n\n<p>\nUse configuration management whenever you intend to permanently alter system state or infrastructure data.\n</p>\n\n    <p>\n    <em>System state - the current state of the system:</em> \n      <ul>\n\t<li>operating system (distro,kernel,patches,hot-fixes)</li>\n\t<li>software installed (base,role-specific)</li>\n\t<li>services running (base,role-specific)</li>\n\t<li>user access</li>\n\t<li>etc</li>\n      </ul>\n    </p>\n\n    <p>\n    <em>Infrastructure data - the information that describes the infrastructure:</em>\n      <ul>\n\t<li>asset information (models,specs,IP,DNS,rack-elevation,etc)</li>\n\t<li>roles (app,web,db,proxy,load-balancer,etc)</li>\n\t<li>current allocations (allocated,unallocated,number of servers in each role,etc)</li>\n\t<li>etc</li>\n      </ul>\n    </p> \n\n<p>\n<strong>What is remote execution?</strong>\n</p>\n\n<p>\nRemote execution is the act of issuing commands to one or more remote systems.  The most popular remote execution system in use today is SSH.  SSH is great for maintaining a small group of dissimilar systems.  Once the system count grows and similar roles present themselves, start looking at something like Fabric.  Fabric is a python framework which builds on top of the SSH protocol and makes it possible to invoke the same command on hundreds of servers sequentially.  Once the fleet count reaches thousands of servers and commands must run in parallel, look at Salt-stack's remote execution layer written with python and ZeroMQ. \n</p>\n\n<p>\n<strong>When should I use remote execution?</strong>\n</p>\n\n<p>\n  Only use remote execution for ad hoc reports, data collection, or for temporary tests which have no expectation of persistence after research is complete.  Frequent data collection jobs work best when implemented in a metrics collection or monitoring system. (I'll save that for another post)\n</p>\n\n<p>\n<strong>When should I not use remote execution?</strong>\n</p>\n\n  <p>\n  Do not use remote execution to change the state of the system or the data which describes the infrastructure!  If a remote execution job changes either state or data it should be placed into the configuration management for the reasons mentioned above.\n  </p>",
    "date": "2013-07-11 23:15:05",
    "timestamp": 1373598905,
    "comments": [],
    "metadata": {
      "_edit_last": "1",
      "_spost_short_title": null
    }
  },
  "control-a-mongodb-collection-in-configuration-management": {
    "name": "control-a-mongodb-collection-in-configuration-management",
    "id": "3450",
    "link": "http://russell2.ballestrini.net/control-a-mongodb-collection-in-configuration-management/",
    "title": "Control a MongoDB collection in configuration management",
    "content": "<p>This post explains how to use configuration management (Salt Stack) to completely control a MongoDB collection.  In our example we want to control a store's collection of plans.</p>\n\n<p>First we create a JSON representation of the collection.</p> \n\n<p>\n<strong>mongodb/plan.json:</strong>\n<pre lang=\"json\">\n{ \n  \"_id\" : { \"$oid\" : \"4ef8b9e2be329f491d98f74b\" },\n  \"cost\" : 20, \"description\" : \"development\",\n  \"name\" : \"good\", \"count\" : 6000\n}\n{ \n  \"_id\" : { \"$oid\" : \"4ef8b9e8be329f491d98f74c\" },\n  \"cost\" : 60, \"description\" : \"freelancers\",\n  \"name\" : \"better\", \"count\" : 36000 \n}\n{ \n  \"_id\" : { \"$oid\" : \"4ef8b9f0be329f491d98f74d\" },\n  \"cost\" : 180, \"description\" : \"production\",\n  \"name\" : \"best\", \"count\" : 162000\n}\n</pre>\n</p>\n\n<p>\nNext we configure a salt state formula to manage the JSON file and watch it for changes.</p>\n\n<strong>mongodb/init.sls:</strong>\n<pre lang=\"yaml\">\n# install mongodb server\nmongodb-server:\n  pkg:\n    - installed\n\n# manage the store's plan.json\n/tmp/plan.json:\n  file.managed:\n    - source: salt://mongodb/plan.json\n    - user: root\n    - group: root\n    - mode: 644\n\n# import the plan collection if it changes\nimport-plan-collection:\n    cmd.wait:\n      - name: mongoimport --db=store --collection=plan --upsert /tmp/plan.json\n      - require:\n        - pkg: mongodb-server\n      - watch:\n        - file: /tmp/plan.json\n\n</pre>\n</p>\n\n<p>\n<em>Now whenever plan.json is altered in configuration management, the file on the minion will update which will trigger a mongoimport with upsert to occur.</em>\n</p>\n\n<p>\nOptionally, we could replace <code>--upsert </code> with <code>--drop </code> which will drop the collection before re-importing thus removing stale records.\n</p>\n\n<p>We now have a version controlled JSON file in configuration management and the power of MongoDB Document Objects in our application code!\n</p>",
    "date": "2013-10-14 21:39:55",
    "timestamp": 1381801195,
    "comments": [],
    "metadata": {
      "_edit_last": "1",
      "_spost_short_title": null,
      "_stcr@_j.a.c.kw.a.ng.niha.o.maa.l.im.19.58@gmail.com": "2013-11-28 00:20:25|Y"
    }
  },
  "postfix-salt-state-formula": {
    "name": "postfix-salt-state-formula",
    "id": "3519",
    "link": "http://russell2.ballestrini.net/postfix-salt-state-formula/",
    "title": "Postfix Salt State Formula",
    "content": "<img src=\"/wp-content/uploads/2013/10/mysza.gif\" alt=\"postfix-config-management\" class=\"alignright\" style=\"padding-right: 80px;\" />\n\n\n<p>The following formula was tested on Ubuntu and Debian however it would not take much work to test on other operating systems.</p>\n\n<p>\nThis state formula will install postfix and mutt.  The postfix service will watch various configuration files for changes and restart accordingly.\n</p>\n\n<p>\nThis formula will also manage and watch the /etc/aliases file and invoke the <em>newaliases</em> command to initialize or re-initialize the alias database.  This formula will also manage and watch the /etc/postfix/virtual file and invoke the <em>postmap</em> command to create or update a managed Postfix lookup table.  \n<p>\n\n<strong>postfix/init.sls:</strong>\n\n<pre lang=\"yaml\">\n# Install mutt and postfix mutt packages.\n#\n# This formula supports setting an optional:\n#\n#  * 'aliases' file \n#  * 'virtual' map file\n#\n# Both aliases and virtual use a pillar data schema\n# which takes the following form: \n# \n# postfix:\n#   aliases: |\n#       postmaster: root\n#       root: testuser\n#       testuser: russell@example.com\n#   virtual: |\n#       example.com             this is a comment\n#       test1@example.com       me@example.com\n#       test2@example.com       me@example.com\n#       \n\n# install mutt\nmutt:\n  pkg:\n    - installed\n\n# install postfix have service watch main.cf\npostfix:\n  pkg:\n    - installed\n  service:\n    - running\n    - enable: True\n    - watch:\n      - pkg: postfix\n      - file: /etc/postfix/main.cf\n\n# postfix main configuration file\n/etc/postfix/main.cf:\n  file.managed:\n    - source: salt://postfix/main.cf\n    - user: root\n    - group: root\n    - mode: 644\n    - template: jinja\n    - require:\n      - pkg: postfix\n\n# manage /etc/aliases if data found in pillar\n{% if 'aliases' in pillar.get('postfix', '') %}\n/etc/aliases:\n  file.managed:\n    - source: salt://postfix/aliases\n    - user: root\n    - group: root\n    - mode: 644\n    - template: jinja\n    - require:\n      - pkg: postfix\n\nrun-newaliases:\n  cmd.wait:\n    - name: newaliases\n    - cwd: /\n    - watch:\n      - file: /etc/aliases\n{% endif %}\n\n# manage /etc/postfix/virtual if data found in pillar\n{% if 'virtual' in pillar.get('postfix', '') %}\n/etc/postfix/virtual:\n  file.managed:\n    - source: salt://postfix/virtual\n    - user: root\n    - group: root\n    - mode: 644\n    - template: jinja\n    - require:\n      - pkg: postfix\n\nrun-postmap:\n  cmd.wait:\n    - name: /usr/sbin/postmap /etc/postfix/virtual\n    - cwd: /\n    - watch:\n      - file: /etc/postfix/virtual\n{% endif %}\n \n</pre>\n\n\n<strong>postfix/aliases:</strong>\n\n<pre>\n# Managed by config management\n# See man 5 aliases for format\n{{pillar['postfix']['aliases']}}\n</pre>\n\n<strong>postfix/virtual:</strong>\n\n<pre>\n# Managed by config management\n{{pillar['postfix']['virtual']}}\n</pre>\n\n<strong>postfix/main.cf:</strong>\n\n<pre>\n# Managed by config management\n# See /usr/share/postfix/main.cf.dist for a commented, more complete version\n\n# Debian specific:  Specifying a file name will cause the first\n# line of that file to be used as the name.  The Debian default\n# is /etc/mailname.\n#myorigin = /etc/mailname\n\nsmtpd_banner = $myhostname ESMTP $mail_name\nbiff = no\n\n# appending .domain is the MUA's job.\nappend_dot_mydomain = no\n\n# Uncomment the next line to generate \"delayed mail\" warnings\n#delay_warning_time = 4h\n\nreadme_directory = no\n\n# TLS parameters\nsmtpd_tls_cert_file=/etc/ssl/certs/ssl-cert-snakeoil.pem\nsmtpd_tls_key_file=/etc/ssl/private/ssl-cert-snakeoil.key\nsmtpd_use_tls=yes\nsmtpd_tls_session_cache_database = btree:${data_directory}/smtpd_scache\nsmtp_tls_session_cache_database = btree:${data_directory}/smtp_scache\n\n# See /usr/share/doc/postfix/TLS_README.gz in the postfix-doc package for\n# information on enabling SSL in the smtp client.\n\nmyhostname = {{ grains['fqdn'] }}\nalias_maps = hash:/etc/aliases   \nalias_database = hash:/etc/aliases\nmydestination = {{ grains['fqdn'] }}, localhost\nrelayhost = \nmynetworks = 127.0.0.0/8 [::ffff:127.0.0.0]/104 [::1]/128\nmailbox_size_limit = 0\nrecipient_delimiter = +\ninet_interfaces = all\n\n{% if 'virtual' in pillar.get('postfix','') %}\nvirtual_alias_maps = hash:/etc/postfix/virtual\n{% endif %}\n\n</pre>",
    "date": "2013-10-17 21:28:28",
    "timestamp": 1382059708,
    "comments": [],
    "metadata": {
      "_edit_last": "1",
      "_spost_short_title": null
    }
  },
  "simplify-deployments-with-upstart-and-uwsgi": {
    "name": "simplify-deployments-with-upstart-and-uwsgi",
    "id": "3534",
    "link": "http://russell2.ballestrini.net/simplify-deployments-with-upstart-and-uwsgi/",
    "title": "Simplify deployments with Upstart and uWSGI",
    "content": "<p>\nAs you know from my previous post, I recently <a href=\"http://russell.ballestrini.net/honey-i-just-deleted-linkpeek-com/\">deleted LinkPeek.com</a> and after struggling to get it back online, I vowed to start utilizing configuration management.  During this exercise, I noticed that the architecture I use in production seems overly complicated.\n</p> \n\n<p>\n<strong>The current production deployment stack:</strong>\n</p>\n\n<ul>\n\t<li>Nginx listen on 80/443 proxy upstream 9901/9902</li>\n\t<li>Upstart => Supervisord => Cherrypy/Paste listen on 9901/9902</li>\n</ul>\n\n<p>\n<em>Each of these services and processes have their own configuration files which must work together.  Upstart needs to know the location of Supervisord's configuration files.  Supervisord needs to know the location of Cherrypy/PasteDeploy's configuration files.  Supervisord also must bring up a specified number of worker processes who listen on a pool of ports.  Nginx needs to proxy upstream to that pool of worker ports.</em>\n</p>\n\n<p>\nThis architecture seems difficult to automate because of the numerous places errors may sneak in.  I started researching an alternative architecture and stumbled upon Nicholas Piel's <a href=\"http://nichol.as/benchmark-of-python-web-servers\">Benchmark of Python Web Servers</a>.  The graphs Nicholas compiled allowed me to narrow down a list of potential replacements.\n</p>\n\n<p>\nI chose to review uWSGI first because it was a top contender on every chart.  After briefly testing uWSGI, I halted my explorations and selected it as the winner.  I know you probably feel that was a premature decision but uWSGI fit what I was looking for.\n</p>  \n\n<p>\n<strong>The new simplified stack:</strong>\n</p>\n\n<ul>\n\t<li>Nginx listen on 80/443 proxy upstream 5200</li>\n\t<li>Upstart => uWSGI listen on 5200</li>\n</ul>\n\n<p>\nThe uWSGI server possesses great performance out-of-the-box but also presents many options for fine tuning.  These options may be specified either directly in the Upstart script using flags or in a configuration file (like a Pyramid .ini).  Either way, the stack uses less files, less processes, and less complexity then the original architecture.  For <a href=\"https://linkpeek.com\">LinkPeek</a> deployments I decided to place all uWSGI related configuration directly in the Upstart script which I embedded below:\n</p>\n\n<p>\n<strong>linkpeek/weblinkpeek.conf | Upstart for starting uWSGI</strong>\n<pre>\ndescription \"Start the uWSGI master for the LinkPeek WEB\"\n\nstart on runlevel [2345]\nstop on runlevel [!2345]\n\nrespawn\n\n# run service as linkpeek user\nsetuid linkpeek\nsetgid linkpeek\n\n# uWSGI command to execute and treat as a daemon\nexec /path/to/linkpeek/web/env/bin/uwsgi --master --processes=4 --http=127.0.0.1:5200 --die-on-term --logto2=/path/to/linkpeek/web/uwsgi.log --virtualenv=/path/to/linkpeek/web/env --ini-paste=/path/to/linkpeek/web/linkpeek/production.ini\n</pre>\n\n</p>\n\nThe web application is now daemonized and listening on port 5200 with 4 workers.  I leave implementing the Nginx front end to the reader.",
    "date": "2013-10-19 22:35:51",
    "timestamp": 1382236551,
    "comments": [],
    "metadata": {
      "_edit_last": "1",
      "_spost_short_title": null
    }
  },
  "backup-all-virtual-machines-on-a-smartos-hypervisor-with-smart-back-sh": {
    "name": "backup-all-virtual-machines-on-a-smartos-hypervisor-with-smart-back-sh",
    "id": "3586",
    "link": "http://russell2.ballestrini.net/backup-all-virtual-machines-on-a-smartos-hypervisor-with-smart-back-sh/",
    "title": "Backup all virtual machines on a SmartOS hypervisor with smart-back.sh",
    "content": "<p>\nThis post will explain how to create a cronjob to backup of every virtual machine on a SmartOS hypervisor.\n</p>\n\n<p>\n<strong>Create</strong> the following bash script in /opt/smart-back.sh:\n<pre>\n#!/usr/bin/bash\n\n# Backup all virtual machines on a SmartOS hypervisor\n# Author:  russell@ballestrini.net\n# Website: http://russell.ballestrini.net/\n\n# Backup directory without trailing slash\nbackupdir=/opt/backups\n\n# temp dir where we ZFS send and gzip before moving to backupdir  \ntmpdir=/opt\n\nsvcadm enable autofs\n\nfor VM in `vmadm list -p -o alias,uuid`\n  do\n    # create an array called VM_PARTS splitting on ':'\n    IFS=':' VM_PARTS=($VM)\n\n    # create some helper varibles for alias and uuid\n    alias=${VM_PARTS[0]}\n    uuid=${VM_PARTS[1]}\n\n    # echo \"Backup started for $VM\"\n    vmadm send $uuid > $tmpdir/$alias\n\n    # echo \"Starting $VM\"\n    vmadm start $uuid\n\n    gzip $tmpdir/$alias\n    mv $tmpdir/$alias.gz $backupdir/$alias.gz\n\n  done \n\n</pre>\n</p>\n\n<p>\n<strong>Create</strong> a cronjob entry to schedule the backups:\n<pre>\ncrontab -e\n</pre>\n<pre>\n2 15 * * * /usr/bin/bash /opt/smart-back.sh\n</pre>\n</p>\n\n<p>\nIf I expand on this script much more, I plan to stick it into revision control.\n</p>\n\n<p>\nIf you look closely, I have also added a hack to enable autofs (svcadm enable autofs) which lets me automount an NFS share on my remote FreeNAS by setting <code>backupdir=/net/[ip-or-fqdn-of-freenas]/mnt/zfs-mirror/backup/vms</code>. \n</p>\n\n<p>\nWe have scheduled a backup of each virtual machine on your SmartOS hypervisor!\n</p>\n\n<p>\nIf or when the time comes to restore a VM from a backup, use the following:\n<pre>\nvmadm receive -f /path/to/backup-file\n</pre>\nJust make sure the VM doesn't currently exist on the hypervisor.\n</p>\n\n<p>\nThis strategy is great for complete backups of machines which could be used during a manual migration, or if corruption happened to the VM and we wanted to restore to a previous version.\n</p>",
    "date": "2013-10-27 20:45:47",
    "timestamp": 1382921147,
    "comments": [
      {
        "id": 20328,
        "parent_id": 0,
        "author_ip": "127.0.0.1",
        "author": "Peter",
        "email": "polson@oncalladvisors.com",
        "content": "Hey, \nAre you still using this script for backing up Smartos VM's?  Did you ever update it or add it to VCS?\n\nThanks!",
        "date": "2014-03-10 17:40:33",
        "timestamp": 1394487633
      },
      {
        "id": 143985,
        "parent_id": 132563,
        "author_ip": "127.0.0.1",
        "author": "Greg",
        "email": "gregzartman@gmail.com",
        "content": "Brandon,\n\nDo you have a copy of your script someplace?",
        "date": "2015-04-28 02:08:46",
        "timestamp": 1430201326
      },
      {
        "id": 144161,
        "parent_id": 143985,
        "author_ip": "127.0.0.1",
        "author": "Brandon",
        "email": "brandon@null.pub",
        "content": "Greg,\n\n  Sure, you can find it here: https://gist.github.com/baetheus/ea4dd6e8dab1877868ff",
        "date": "2015-04-28 23:04:42",
        "timestamp": 1430276682
      },
      {
        "id": 132563,
        "parent_id": 0,
        "author_ip": "127.0.0.1",
        "author": "Brandon",
        "email": "brandon@null.pub",
        "content": "Hey, I built a script that does something similar.\nHowever, my script also has flags for either storing the vm locally, or storing it remotely via scp or ftp.",
        "date": "2015-03-05 23:06:28",
        "timestamp": 1425614788
      }
    ],
    "metadata": {
      "_edit_last": "1",
      "_spost_short_title": null,
      "_series_part": "1",
      "_stcr@_polson@oncalladvisors.com": "2014-03-10 17:40:33|Y",
      "_stcr@_brandon@null.pub": "2015-03-05 23:06:28|Y",
      "_stcr@_gregzartman@gmail.com": "2015-04-28 02:08:46|Y"
    }
  },
  "how-i-added-two-seagate-240g-ssds-as-smartos-l2arc": {
    "name": "how-i-added-two-seagate-240g-ssds-as-smartos-l2arc",
    "id": "3604",
    "link": "http://russell2.ballestrini.net/how-i-added-two-seagate-240g-ssds-as-smartos-l2arc/",
    "title": "How I added two Seagate 240G SSDs as SmartOS L2ARC",
    "content": "<p>\n<strong>How I added two Seagate 240G SSDs as SmartOS L2ARC</strong>\n</p>\n\n<p>\n<ol>\n  <li>removed icepacks from two western digital velociraptors</li>\n  <li>installed ssds into icepacks</li>\n  <li>installed icepacks into HP hotswap trays</li>\n  <li>installed trays into HP prolaient g6 server</li>\n</ol>\n</p>\n\n<p>\n<strong>How to list all drive installed in Solaris, Open Solaris, or SmartOS</strong>\n</p>\n<p>\n<pre>\niostat -eE\n</pre>\n<pre>\nformat\n</pre>\n</p>\n\n<p>\n<pre>\nAVAILABLE DISK SELECTIONS:\n       0. c1t0d0 <ATA-ST2000DM001-1CH1-CC26-1.82TB>\n          /pci@0,0/pci103c,330b@1f,2/disk@0,0\n       1. c1t1d0 <ATA-ST2000DM001-1CH1-CC26-1.82TB>\n          /pci@0,0/pci103c,330b@1f,2/disk@1,0\n       2. c1t2d0 <ATA-ST240HM000-1G515-C675 cyl 37375 alt 2 hd 224 sec 56>\n          /pci@0,0/pci103c,330b@1f,2/disk@2,0\n       3. c1t3d0 <ATA-ST240HM000-1G515-C675 cyl 37375 alt 2 hd 224 sec 56>\n          /pci@0,0/pci103c,330b@1f,2/disk@3,0\n</pre>\n</p>\n\n<p>\n<strong>list zpools:</strong>\n</p>\n<p>\n<pre>\nzpool list\n</pre>\n<pre>\nNAME    SIZE  ALLOC   FREE  EXPANDSZ    CAP  DEDUP  HEALTH  ALTROOT\nzones  1.81T  41.4G  1.77T         -     2%  1.00x  ONLINE  -\n</pre>\n</p>\n\n<p>\n<strong>Add the two 240G SSDs as L2ARC devices:</strong>\n\n<pre>\nzpool add zones cache c1t2d0 c1t3d0\n</pre>\n</p>\n\n<p>\n<strong>Look at the iostats of the drives in the zpool</strong>\n<pre>\nzpool iostat -v\n</pre>\n<pre>\n               capacity     operations    bandwidth\npool        alloc   free   read  write   read  write\n----------  -----  -----  -----  -----  -----  -----\nzones       41.4G  1.77T      5     46  78.7K   730K\n  mirror    41.4G  1.77T      5     46  78.7K   730K\n    c1t0d0      -      -      0     15  41.5K   733K\n    c1t1d0      -      -      0     15  41.6K   733K\ncache           -      -      -      -      -      -\n  c1t2d0    14.6M   224G      0      3  2.48K   273K\n  c1t3d0    45.5M   224G      0      6  2.48K   852K\n----------  -----  -----  -----  -----  -----  -----\n</pre>\n</p>\n\n<p>\nThis machine is a test hypervisor and after reviewing the output from <code>zpool iostat -v</code> over the last couple days, I'm pretty sure adding L2ARC to this box was not needed, but it was a great learning experience.\n</p>",
    "date": "2014-01-05 14:56:36",
    "timestamp": 1388951796,
    "comments": [],
    "metadata": {
      "_edit_last": "1",
      "_spost_short_title": null
    }
  },
  "hackathon-2013-virtualization": {
    "name": "hackathon-2013-virtualization",
    "id": "3614",
    "link": "http://russell2.ballestrini.net/hackathon-2013-virtualization/",
    "title": "Hackathon 2013 Virtualization",
    "content": "<p>\nAs a warning before we dive into things, this post is less of a formal publication and more of a stream of conscience.\n</p>\n\n<p>\nMy employer <a href=\"http://newcars.com/jobs/\">newcars.com</a> has allowed the technical staff to host hackathon! Over the past couple weeks I have had quite a few ideas tumbling around in my head:\n\n<ul>\n\t<li>Standup a central logging server</li>\n         My colleague <a href=\"http://bobbylikeslinux.net/\">Bobby</a> ran with this idea, he posted his progress here: https://bitbucket.org/robawt/robawt-salt-logstash/overview\n\t<li>Standup Sensu for monitoring</li>\n        <strong>Update:</strong> I tested setup a test environment with <a href=\"http://russell.ballestrini.net/sensu-salt\">sensu-salt and salt-cloud</a>.\n\t<li>Bake-off and document some KVM virtualization hypervisors (Ubuntu or SmartOS)</li>\n\t<li>Test Docker and document findings</li>\n</ul>\n</p>\n\n<p>\nUltimately I have chosen to dedicate my time to testing out virtualization on the new Cisco UCS Blade servers. I plan to test Ubuntu KVM first. \n</p>\n\n<p>\n<strong>KVM (Ubuntu 12.04)</strong>\n</p>\n\n<p>\nI decided to use a Salt-stack configuration management formulas to document how I transformed <code>kvmtest02</code> (a regular Ubuntu 12.04 server) into a KVM hypervisor.  I use <code>kvmtest0a</code> and <code>kvmtest02b</code> when refering to the virtual machines living on the hypervisor.  Here is the formula:\n</p>\n\n<p>\n<strong>kvm/init.sls:</strong>\n<pre lang=\"yaml\">\n# https://help.ubuntu.com/community/KVM\n# Ubuntu server a KVM - Kernel Virtual Machine hypervisor\n\n# the official ubuntu document suggests we install the following\nkvm-hypervisor:\n  pkg.installed:\n    - names:\n      - qemu-kvm\n      - libvirt-bin\n      - ubuntu-vm-builder\n      - bridge-utils\n\n# we need to make all of our ops people part of the libvirtd group\n# so that they may create and manipulate guests. we skip this for now\n# and assume all virtual machines will be owned by root.\n\n# these are optional packages which gives us a GUI for the hypervisor\nvirt-manager-and-viewer:\n  pkg.installed:\n    - names:\n      - virt-manager \n      - virt-viewer \n    - require:\n      - pkg: kvm-hypervisor\n\n# create a directory to hold virtual machine image files\nkvm-image-dir:\n  file.directory:\n    - name: /cars/vms\n    - user: root\n    - group: root\n    - mode: 775\n\n</pre>\n</p>\n\n<p>\nI used the following to install the formula to the test hypervisor:\n\n<pre lang=\"bash\">\nsalt 'kvmtest02.example.com' state.highstate\n</pre>\n</p>\n\n<p>\nThe salt highstate reported everything was good (green), so I moved on to setting up the bridge networking interface.  At this point I don't want to figure out the logistics setting up the network bridge in configuration management, so I simply manually edited <code>/etc/network/interfaces</code> to look like this (substitute your own network values):\n<pre lang=\"text\">\n\n# The loopback network interface\nauto lo\niface lo inet loopback\n\n# The primary network interface\n#auto eth0\n#iface eth0 inet manual\n\n# https://help.ubuntu.com/community/KVM/Networking\n# create a bridge so guest VMs may have their own identities\nauto br0\niface br0 inet static\n    address XXX.XX.89.42\n    netmask 255.255.255.0\n    network XXX.XX.89.0\n    broadcast XXX.XX.89.255\n    gateway XXX.XX.89.1\n\n    # dns-* options are implemented by the resolvconf package\n    dns-nameservers XXX.XX.254.225 XXX.XX.254.225\n    dns-search example.com\n\n    # bridge_* options are implemented by bridge-utils package\n    bridge_ports eth0\n    bridge_stp off\n    bridge_fd 0\n    bridge_maxwait 0\n</pre>\n\nThen, I crossed my fingers and reloaded the network stack using this command:\n\n<pre>\n/etc/init.d/networking restart\n</pre>\n\nI also used this to \"bounce\" the bridge network interface:\n\n<pre>\nifdown br0 && ifup br0\n</pre>\n\nI verified with <code>ifconfig</code>.\n\n</p>\n\n\n<p>\nI'm ready to create my first VM.  There are many different ways to boot the VM and install the operating system.  KVM is fully virtualized so nearly any operating system may be install on the VM.\n</p>\n\n<p>\nIf you have not already, please get familiarized with the following two commands: \n<ol>\n  <li>virsh</li>\n  <li>virt-install</li>\n</ol>\n</p>\n\n<p>\nThe <code>virsh</code> command is an unified tool / API for working with hypervisors that support the libvirt library.  Currently <code>virsh</code> supports Xen, QEmu, KVM, LXC, OpenVZ, VirtualBox and VMware ESX.  For more information run <code>virsh help</code>.\n</p>\n\n<p>\nThe <code>virt-install</code> command line tool is used to create new KVM, Xen, or Linux container guests using the \"libvirt\" hypervisor management library. For more information run <code>man virt-install</code>.\n</p>\n\n<blockquote>\nWoah, virsh and virt-install both support LXC?\n</blockquote>\n\n<p>\nWe have decided to only support Ubuntu 12.04 at this time, so obviously we will choose that for our guest's OS.  Now we need to decide on an installation strategy. We may use the following techniques to perform an install: \n<ul>\n  <li>boot from local CD-rom</li>\n  <li>boot from local ISO</li>\n  <li>boot from PXE server on our local vLAN</li>\n  <li>boot from netboot image from anywhere in the world</li>\n</ul>\n\nWe will choose the PXE boot strategy because our vLAN environment already uses that for physical hosts.\n</p>\n\n<p>\nWe will use the <code>virt-install</code> helper tool to create the virtual machine's \"hardware\" with various flags.  Lets document the creation of this guest in a simple bash script so we may reference it again in the future.\n</p>\n\n</strong>/tmp/create-kvmtest02-a.sh:</strong>\n</p>\n<pre lang=\"bash\">\nHOSTNAME=kvmtest02-a\nDOMAIN=example.com\n\nsudo virt-install \\\n  --connect qemu:///system \\\n  --virt-type kvm \\\n  --name $HOSTNAME \\\n  --vcpu 2 \\\n  --ram 4096 \\\n  --disk /cars/vms/$HOSTNAME.qcow2,size=20 \\\n  --os-type linux \\\n  --graphics vnc \\\n  --network bridge=br0,mac=RANDOM \\\n  --autostart \\\n  --pxe\n</pre>\n</p>\n\n<p>\nThis was not used, but shows the flags to perform a netboot from Internet:\n<pre lang=\"bash\" style=\"font-size: 0.8em;\">\n--location=http://archive.ubuntu.com/ubuntu/dists/raring/main/installer-amd64/ \\\n--extra-args=\"auto=true priority=critical keymap=us locale=en_US hostname=$HOSTNAME domain=$DOMAIN url=http://192.168.1.22/my-debconf-preseed.txt\"\n</pre>\n</p>\n\n<p>\nI created the vm:\n<pre lang=\"bash\">\nbash /tmp/create-kvmtest02-a.sh\n</pre>\n</p>\n\n<p>\n<code>virt-install</code> drops you into the \"console\" of the VM, but this will not work yet, so we use ctrl+] to break out and get back to our hypervisor.  Use <code>virsh list</code> to list all the currently running VMs.\n\nLets use <code>virt-viewer</code> to view the VMs display.  For this we need to SSH to the hypervisor and forward our display to our workstation, we do this with the <code>-X</code> flag. For example:\n\n<pre lang=\"bash\">  \nssh -X kvmtest02\n</pre>\n\nNow we can launch <code>virt-viewer</code> on the remote hypervisor, and the GUI will be drawn on our local X display!\n\n<pre lang=\"bash\">  \nvirt-viewer kvmtest02-a\n</pre>\n\nOnce I got that to work, I also tested <code>virt-manager</code> which gives a GUI to control all guests on the remote hypervisor.\n\n<pre lang=\"bash\">  \nvirt-manager\n</pre>\n\n</p>\n\n\n Now we need to determine the auto-generated MAC Address of the new virtual machine.\n\n<pre lang=\"bash\">\nvirsh dumpxml kvmtest02-a | grep -i \"mac \"\n  mac address='52:54:00:47:86:8e'\n</pre>\n\nWe need to add this MAC address to our PXE server's DHCP configuration to allocate the IP and tell it where to PXE-boot from.\n</p>\n\n<p>\nDuring a real deployment we would get an IP address allocated and an A record and PTR setup for new servers.  This is a test and I will be destroying all traces of this virtual machine after presenting during the hackathon, so for now I'm going to skip the DNS entries and \"steal\" an IP address.  I must be VERY careful not to use an IP address already in production.  First use dig to find an IP without a record, then attempt to ping and use NMAP on the IP.\n</p>\n\n<pre lang=\"bash\">\ndig -x XXX.XX.89.240 +short\nping XXX.XX.89.240\nnmap XXX.XX.89.240 -PN\n</pre>\n\n<p>\nThe IP address checked out, it didn't have a PTR, it didn't respond to pings, and using nmap proved there were no open ports.  I'm very confident this IP address is not in use.\n</p>\n\n<p>\nI added a record to our DHCP / PXE server for this Virtual Machine.  I attempted multiple times to pxe boot the VM, but the network stack was never automatically configured...  The DHCP server was discovering the new VMs MAC and offering the proper IP address, as noted by these log lines:\n\n<pre lang=\"bash\">\nDec 13 07:57:43 pxeserver60 dhcpd: DHCPDISCOVER from 52:54:00:47:86:8e via eth0\nDec 13 07:57:43 pxeserver60 dhcpd: DHCPOFFER on xxx.xx.89.240 to 52:54:00:47:86:8e via eth0\nDec 13 07:57:44 pxeserver60 dhcpd: DHCPDISCOVER from 52:54:00:47:86:8e via eth0\nDec 13 07:57:44 pxeserver60 dhcpd: DHCPOFFER on xxx.xx.89.240 to 52:54:00:47:86:8e via eth0\nDec 13 07:57:48 pxeserver60 dhcpd: DHCPDISCOVER from 52:54:00:47:86:8e via eth0\nDec 13 07:57:48 pxeserver60 dhcpd: DHCPOFFER on xxx.xx.89.240 to 52:54:00:47:86:8e via eth0\n</pre>\n\nI wasted about 4 hours attempting to troubleshoot and diagnose why the VM wouldn't work with DHCP.  I ended the night without any guests online...\n</p>\n\n<p>\n<strong>The Next DAY!</strong>\n</p>\n\n<p>\nSo today I decided to stop trying to get DHCP and PXE working.  I downloaded an Ubuntu server ISO to the hypervisor, and used <code>virt-manager</code> to mount the ISO on the guest and booted for a manual operating system install.  \n\n<p>\nThis did two things, it proved that the hypervisor's network bridge <code>br0</code> worked for static network assigned settings and that something between the DHCP server and the hypervisor was preventing the <code>DHCPOFFER</code> answer from getting back to the VM.  I looked into iptables firewall, removed apparmor, removed SELINUX and reviewed countless logs looking for hints... then moved on...\n</p>\n\n<p>\nI was able to get salt-minion installed on the vm using our post-install-salt-minion.sh script, which I manually downloaded from the salt master.  But to keep this test self contained, I pointed the VM's salt-minion to <code>kvmtest02</code> which we already had setup as a test salt-master.\n</p>\n\n<p>\nThe salt-master saw the salt-minion's key right away, so I decided to target an install.  This what I applied to the VM:\n</p>\n\n<p>\nsalt/top.sls:\n<pre lang=\"yaml\">\n  'kvmtest02.example.com':\n    - kvm\n\n  'kvmtest02b.example.com':\n    - virtualenv\n    - python-ldap\n    - nginx\n    - the-gateway\n</pre>\n\n<p>\nsalt-pillar/top.sls\n</p>\n<pre lang=\"yaml\">\n  # kvmtest02b gateway in a VM experiment\n  'kvmtest02b.example.com':\n    - nginx\n    - the-gateway.alpha\n    - deployment-keys.the-gateway-alpha\n</pre>\n</p>\n\n<p>\nThe stack was successfully deployed to the VM and proved that virtual machines are a viable solution for stage or production.  It also gave me the change to test out this particular deployment again and found a few gotchas we need to create maintenance tickets for.  \n</p>\n\n<p>\nWithout configuration management, it would have taken weeks to deploy this custom application stack.  The install with configuration management took less then 10 minutes!  \n</p>\n\n<p>\nOne of the KVM related snags I ran into was that Nginx does some fun calculations with cpu cache to determine hash table sizes.  As a temporary work around, until I can devote more research time, I raised up the following three hash table directives in the http section of </code>nginx.conf</code>:\n</p>\n\n<pre>\n    server_names_hash_bucket_size 512;\n    types_hash_bucket_size 512;\n    types_hash_max_size 4096;\n</pre>\n\n</p>\n\n<p>\n<strong>SmartOS</strong>\n</p>\n\n<p>\n<strong>snippet from /etc/dhcp/dhcpd.conf</strong>\n<pre>\n# SmartOS hypervisor group to boot image\ngroup \"smartos-hypervisors\" {\n  next-server xxx.xx.89.71;\n\n   host smrtest01-eth0 {\n        hardware ethernet 00:25:B5:02:07:DF;\n        option host-name \"ncstest01\";\n        fixed-address smrtest01.example.com;\n\n        if exists user-class and option user-class = \"iPXE\" {\n           filename = \"smartos/menu.ipxe\";\n        } else {\n           filename = \"smartos/undionly.kpxe\";\n        }\n    }\n\n}\n\n</pre>\n\n<pre>\nmkdir /cars/tftp/smartos\ncd /cars/tftp/smartos\nwget http://boot.ipxe.org/undionly.kpxe\nwget https://download.joyent.com/pub/iso/platform-latest.tgz\ntar -xzvf platform-latest.tgz\nmv platform-20130629T040542Z 20130629T040542Z\nmkdir platform\nmv i86pc/ platform/\n</pre>\n\ncreate boot menu that we referenced, <code>vim /cars/tftp/smartos/menu.ipxe</code>\n\n<pre>\n#!ipxe\n\nkernel /smartos/20130629T040542Z/platform/i86pc/kernel/amd64/unix\ninitrd /smartos/20130629T040542Z/platform/i86pc/amd64/boot_archive\nboot\n</pre> \n\nMake sure to replace platform version with current.\n\n</p>\n\n<p>\nI was able to get the blade server to PXE boot the image, but it seems SmartOS doesn't really support the SANs.  SmartOS really expects to see local disks, and to build a ZFS pool on top of that.  Basically SmartOS could be used to build a SAN, so they didn't put much effort in supporting SANs.  After I figured this out I abandoned this test.  We could revist this again, using one of the Dell servers, or use it to stand up a really powerful Alpha server environment.\n</p>\n\n<p>\n<strong>LXC</strong>\n</p>\n \n<p>\nRun /usr/bin/httpd in a linux container guest (LXC). Resource usage is capped at 512 MB of ram and 2 host cpus:\n\n<pre lang=\"bash\">\nvirt-install \\\n--connect lxc:/// \\\n--name lxctest02-a \\\n--ram 512 \\\n--vcpus 2 \\\n--init /usr/bin/httpd\n</pre>\n</p>\n \n\n<p>\n<strong>Discussion points</strong>\n</p>\n<p>\n<ul>\n<li>\nWhy doesn't DHCP work on bridge?\n</li>\n<li>\nIf we use virtualization, we need to come up with a plan for IP addresses, like possibly allocate ~5 IP addresses to a hypervisor host\n</li>\n<li>\nWe need to come up with a naming convention for guests, in testing I appended a letter to the hypervisor name <code>kvmtest02</code> so the guests names were <code>kvmtest02a</code> and <code>kvmtest02b</code>, is this plausible going forward?\n</li>\n</ul>\n</p>\n\n<p>\n<strong>If I had more time ...</strong>\n</p>\n<p>\n<ul>\n<li>\nI would have liked to test out LXC\n</li>\n<li>\nI would have liked to test out Docker\n</li>\n<li>\nI would have liked to test out physical to virtual migrations\n</li>\n</p>",
    "date": "2013-12-13 20:56:00",
    "timestamp": 1386986160,
    "comments": [],
    "metadata": {
      "_edit_last": "1",
      "_spost_short_title": null
    }
  },
  "sensu-salt": {
    "name": "sensu-salt",
    "id": "3723",
    "link": "http://russell2.ballestrini.net/sensu-salt/",
    "title": "sensu-salt",
    "content": "<p>\nA while back I explained how to <a href=\"http://russell.ballestrini.net/create-your-own-fleet-of-servers-with-digital-ocean-and-salt-cloud/\">Create your own fleet of servers with Digital Ocean and salt-cloud</a>.  Today I will extend that post and show how I deployed a test environment for <a href=\"http://sensuapp.org/\">Sensu, an open source monitoring framework</a>. \n</p>\n<p>\nBefore I test out new infrastructure software, I always attempt to write deployment manifests.  I subject myself to this exercise for the following reasons:\n<ul>\n  <li>to get better at configuration management (self growth)</li>\n  <li>to enable quick setup and tear down of test environments (save money)</li>\n  <li>to make sure the new software may be deployed and maintained in configuration management (a must)</li>\n  <li>to document the install process and leave comments (my notes double as automation scripts!)</li>\n  <li>to allow knowledge transfer (sharing is caring, you're welcome!)</li>\n</ul>\n</p>\n\n<p>\nThe state formulas in this post were tested on Ubuntu 12.04.3 x64bit.\n</p>\n\n<p>\n<strong>Clone or fork the Salt-State tree</strong>\n</p>\n<p>\n<a href=\"https://github.com/sensu/sensu-salt\">https://github.com/sensu/sensu-salt</a>\n<pre>\ngit clone https://github.com/sensu/sensu-salt.git\n</pre>\n</p>\n\n<p>\n<strong>Declare deployment targets</strong>\n</p>\n\n<p>\nBefore deployment, we must declare some targets in the top.sls file:\n</p>\n<p>\n<pre lang=\"yaml\">\n  '*':\n    - git\n\n  'sensu-client*':\n    - sensu.client\n\n  'sensu-server*':\n    - rabbitmq.server\n    - redis.server\n    - sensu.server\n    - sensu.client\n</pre>\n</p>\n\n<p>\n<strong>Stand up Sensu test environment</strong>\n</p>\n<p>\nI used the following <code>salt-cloud</code> command to create the sensu monitor server:\n<pre>\nsalt-cloud --profile ubuntu_do sensu-server && salt 'sensu-server' state.highstate\n</pre>\n</p>\n\n<p>\nOnce the sensu-server was live, I altered the <code>client-config.json</code> and modified the RabbitMQ host with the new <code>sensu-server</code>'s IP or DNS record.\n</p>\n<p>\nI then spun up 4 sensu-clients using the following command:\n<pre>\nsalt-cloud -P --profile ubuntu_do sensu-client1 sensu-client2 sensu-client3 sensu-client4 && salt 'sensu-client*' state.highstate\n</pre>\n</p>\n\n<p>\nThis caused 4 cloud servers to be spawned in parallel, and when the provisioning finished, they instantly appeared in the <code>sensu-dashboard</code> which runs on the <code>sensu-server</code>.\n</p>\n\n<p>\n<strong>Tear down Sensu test environment</strong>\n</p>\n\n<p>\nLater when I was done messing with writing checks, I used the following <code>salt-cloud</code> commands to destroy the sensu server and clients:\n<pre>\nsalt-cloud --destroy sensu-server\nsalt-cloud --destroy sensu-client1 sensu-client2 sensu-client3 sensu-client4\n</pre>\n</p>",
    "date": "2013-12-17 11:35:20",
    "timestamp": 1387298120,
    "comments": [
      {
        "id": 18457,
        "parent_id": 0,
        "author_ip": "127.0.0.1",
        "author": "James Thigpen",
        "email": "james.r.thigpen@gmail.com",
        "content": "There's a couple other sensu salt states you can check out also:\n\nhttps://github.com/blast-hardcheese/blast-salt-states/tree/master/sensu\nhttps://github.com/tateeskew/salt-states/tree/master/sensu\n\nI used the tateeskew one when I as experimenting with sensu, and it was pretty easy to get up and going.",
        "date": "2013-12-17 14:44:21",
        "timestamp": 1387309461
      },
      {
        "id": 18460,
        "parent_id": 0,
        "author_ip": "127.0.0.1",
        "author": "Russell Ballestrini",
        "email": "russell.ballestrini@gmail.com",
        "content": "@James Thanks for the list, I didn't know about them.  I guess it doesn't hurt to have a few examples floating around.",
        "date": "2013-12-17 16:40:30",
        "timestamp": 1387316430
      }
    ],
    "metadata": {
      "_edit_last": "1",
      "_spost_short_title": null,
      "_stcr@_james.r.thigpen@gmail.com": "2013-12-17 14:44:21|Y"
    }
  },
  "heka-world": {
    "name": "heka-world",
    "id": "3777",
    "link": "http://russell2.ballestrini.net/heka-world/",
    "title": "Heka, World!",
    "content": "<p>\nThis post serves as a \"Hello World\" for the <a href=\"https://github.com/mozilla-services/heka\" target=\"_blank\">data collection and processing software called Heka</a>.  Heka is written in Go and was open sourced by Mozilla, the same fabulous group that brings us Firefox!\n</p>\n\n<p>\nI intend to use Heka to replace Logstash agents by sending logs directly to ElasticSearch and continuing to use Kibana3 for visualizations.  Also I aim to start collecting metrics and sending to a central Whisper back-end to fuel Graphite charts.  All that we need to make Heka take on these responsibilities is one binary and some custom configuration.\n</p>\n\n<p>\n<strong>Heka:</strong> Hello, World!\n</p>\n\n<p>\nThis mostly contrived example will show how to use Heka to watch <code>/tmp/input.log</code> and write to <code>/tmp/output.log</code>.\n</p>\n\n<p>\n<strong>Step 1:</strong> install Heka\n</p>\n<p> \nCompile from source or install the <a href=\"https://github.com/mozilla-services/heka/releases\">Heka package</a> for your operating system.\n</p>\n\n<p>\n<strong>Step 2:</strong> create a Heka TOML configuration file\n</p>\n<code>/tmp/hello-heka.conf:</code>\n<p>\n<pre>\n[hello_heka_input_log]\ntype = \"LogstreamerInput\"\nlog_directory = \"/tmp\"\nfile_match = 'input\\.log'\n\n[hello_heka_output_log]\ntype = \"FileOutput\"\nmessage_matcher = \"TRUE\"\npath = \"/tmp/output.log\"\nperm = \"664\"\nencoder = \"hello_heka_output_encoder\"\n\n[hello_heka_output_encoder]\ntype = \"PayloadEncoder\"\nappend_newlines = false\n</pre>\n</p>\n\n\n<p>\n<strong>Step 3:</strong> start Hekad and test!\n</p>\n<p>\n<ol>\n<li>in terminal 1: <pre>sudo hekad -config=/tmp/hello-heka.conf</pre></li>\n<li>in terminal 2: <pre>tail -f /tmp/output.log</code></li></pre>\n<li>in terminal 3: <pre>echo 'Heka, World!' >> /tmp/input.log</pre></li>\n</ol>\nLike magic the data appended to <code>input.log</code> will appear in <code>output.log</code>\n</p>\n\n<p>Heka also has great docs and a number of input and output plugins, but don't take my word for it, try it yourself!</p>",
    "date": "2013-12-20 18:25:40",
    "timestamp": 1387581940,
    "comments": [],
    "metadata": {
      "_edit_last": "1",
      "_spost_short_title": null
    }
  },
  "test-game-engine-with-python-and-sfml": {
    "name": "test-game-engine-with-python-and-sfml",
    "id": "3818",
    "link": "http://russell2.ballestrini.net/test-game-engine-with-python-and-sfml/",
    "title": "Test Game Engine with Python and SFML",
    "content": "<p>\nOver this holiday season, Christmas and New Years, I took the time to mess with some Game Development.  I wrote a demo game engine using Python and SFML.  I plan to use this post to track my progress.\n</p>\n\n<strong>Video Evolution Playlist</strong>\n<ul>\n  <li><a href=\"http://www.youtube.com/watch?v=ytEG21b2es4&list=PLPqlh_ebFYDFPv_tIdUb26Bit2Q2nIVrm\">All videos</a></li>\n</ul>\n\n<p>\n<strong>2014-01-01</strong>\n<ul>\n  <li><a href=\"http://www.youtube.com/watch?v=ytEG21b2es4\">User Input, Simple world, Scrolling</a></li>\n  <li><a href=\"http://www.youtube.com/watch?v=LFxfepBSgEc\">Simple Collision detection</a></li>\n</ul>\n</p>\n\n<p>\n<strong>2014-01-02</strong>\n<ul>\n  <li><a href=\"http://www.youtube.com/watch?v=SsAi00wv0vU\">Simple Collision bump vs push</a></li>\n  <li><a href=\"http://www.youtube.com/watch?v=2-FFMP67atU\">Power ups and grow and shrink</a></li>\n</ul>\n</p>",
    "date": "2014-01-02 00:30:29",
    "timestamp": 1388640629,
    "comments": [],
    "metadata": {
      "_spost_short_title": null,
      "_edit_last": "1"
    }
  },
  "nginx-with-ssl-and-mixed-content-errors-with-upstream-wsgi-servers": {
    "name": "nginx-with-ssl-and-mixed-content-errors-with-upstream-wsgi-servers",
    "id": "3405",
    "link": "http://russell2.ballestrini.net/nginx-with-ssl-and-mixed-content-errors-with-upstream-wsgi-servers/",
    "title": "Nginx with SSL and mixed content errors with upstream WSGI servers",
    "content": "<p>\nMixed content errors occur because Nginx (the front-end server) communicates to the upstream WSGI server using http.  WSGI does not know (or care) about the SSL session between Nginx and the user.  The WSGI server will naively generate URIs and serve assets as http.\n</p>\n\n<p>\nTo fix mixed content errors, we need to communicate the inbound request scheme or configure the WSGI server to always use https.\n</p>\n\n<p>\n<strong>waitress</strong>\n</p>\n\n<p>\nWaitress is meant to be a production-quality pure-Python WSGI server with very acceptable performance.\nIt commonly powers Pyramid and Substance D deployments.</p>\n\n<p>\nTo configure waitress to always use https in code:\n</p>\n<p>\n<pre>\nfrom waitress import serve\nserve(wsgiapp, host='0.0.0.0', port=8080, url_scheme='https')\n</pre>\n</p>\n\n<p>\nconfigure waitress to always use https in paste deploy compatible configuration file:\n</p>\n\n<p>\n<pre>\n[server:main]\nhost = 127.0.0.1\nport = 6543\nurl_scheme = https\n</pre>\n</p>",
    "date": "2014-05-08 16:28:05",
    "timestamp": 1399580885,
    "comments": [],
    "metadata": {
      "_spost_short_title": null,
      "_edit_last": "1",
      "_stcr@_regantk816@gmail.com": "2014-07-24 02:14:49|Y"
    }
  },
  "configuration-management-and-the-golden-image": {
    "name": "configuration-management-and-the-golden-image",
    "id": "3609",
    "link": "http://russell2.ballestrini.net/configuration-management-and-the-golden-image/",
    "title": "Configuration Management and the Golden Image",
    "content": "<p>\nWhen operations first became a thing, system administrators stood up servers using a base image from their favourite distribution.  Things were done manually.  Some administrators created their own distros, some wrote customised shell scripts to be run once-and-only-once to provision software and settings. This method worked, but it was slow, manual, and the human element caused defects.  Then the request came in to stand up 100 servers the exact same way.  \n</p>\n\n<p>\nSystem admins resolved this large request by coming up with a Standard Operating Environment (SOE) image which would end up becoming the \"golden image\".  This golden image was the source of truth, and we built hundreds, thousands of machines this way.  All systems were the same, or rather started out the same, but it didn't take long for deviations occur. \n(Norton Ghost, DD, ISOs)\n http://en.wikipedia.org/wiki/Standard_Operating_Environment\n</p>\n\n<p>\nThe golden image by itself was a flawed idea. The task of creating a golden image was difficult and required a lot of work.  Not only was it technical, but there was a lot of politics involved in what was worthy of inclusion.  We didn't want to add cruft to every machine.  Also the golden image was only updated every couple of years, so it would quickly become outdated and it still required provisioning scripts.  Systems already in production didn't get configuration updates that were recently added to the golden image.  There had to be a better way, and there was...\n</p>\n\n<p>\nSome novel and smart system administrators who also knew how to program decided that they didn't want to maintain a golden image and deal with all the headaches involved and opted to use the light weight distribution image and then build sophisticated remote execution software to manage and maintain each server's configuration.  Later on they built configuration management systems on top of this remote execution layer and were finally able to keep each system up-to-date, regardless of when it was deployed.  They were geniuses and everyone who knew anything quickly rushed to implement remote execution and configuration management.  It was the right way to manage servers, until the cloud drifted in.\n(SaltStack, Ansible, Puppet, Chef, CFEngine)\n</p>\n\n<p>\nIn the day of cloud computing, we needed to scale up and down servers in seconds.  A complex configuration manifest could take hours to run from start to finish.  Configuration management was too slow.  Each server needed to download, install, and configure the software stack, in real-time.  Sure we could spin up multiple machines in parallel, but it was still slow.  We started to look back at the golden age of the golden image, when a server was built and booted in moments.  How could we pair the speed of the golden image with the flexibility of configuration management?\n</p>\n\n<p>\nIn the future could we use configuration management manifests and revision control to document how to build the golden image and then take a snapshot?  Could we overlay multiple layers or dataset of images on top of each other?  Perhaps we take a step way back and create golden images with a combination of a highly customizable distribution like Gentoo and configuration management.  \n(Joyant SmartOS zone datasets, Docker container images, Vagrant box files, AWS AMI, Digital Ocean Snapshots, etc)\n</p>",
    "date": "2014-02-21 17:19:10",
    "timestamp": 1393021150,
    "comments": [
      {
        "id": 163358,
        "parent_id": 0,
        "author_ip": "127.0.0.1",
        "author": "Joaquin Menchaca",
        "email": "joaquin.it@gmail.com",
        "content": "I really like this article, it sums up the transition from golden images and change configuration.  I wonder what role Kickstart, Jumpstart, FAI (Fully Automatic Install) fulfill in the historical sense?  And how gold image patterns apply to Docker, which feels like a hybrid imaging and packaging patterns....",
        "date": "2015-08-16 17:16:51",
        "timestamp": 1439759811
      },
      {
        "id": 166179,
        "parent_id": 0,
        "author_ip": "127.0.0.1",
        "author": "Rogerio",
        "email": "habutre@gmail.com",
        "content": "Great point! Flexibility vs Speed, everyone is facing this challenges and probably in cloud's time the speed is the most important characteristic to be take note. But the flexibility and on-the-fly changes are met with the configuration management tools which can be more helpful and agile than prepare a new version of our golden image.\nBoth are important, so I would support the best of both! A golden image as start point and configuration management as a single shot to maintenance goals.",
        "date": "2015-08-31 10:15:37",
        "timestamp": 1441030537
      }
    ],
    "metadata": {
      "_oembed_310daa2197602fbc0c9ad739a16441b0": "{{unknown}}",
      "_edit_last": "1",
      "_spost_short_title": null,
      "_oembed_84d868a6e762bade597dabc529b465ff": "{{unknown}}",
      "_oembed_4f8cf2cebd0742e5140b1d379358cfce": "{{unknown}}",
      "_stcr@_habutre@gmail.com": "2015-08-31 10:15:37|Y",
      "_stcr@_joaquin.it@gmail.com": "2015-08-16 17:16:51|Y"
    }
  },
  "how-to-reset-hp-ilo-lights-out-user-and-password-settings-with-ipmitools": {
    "name": "how-to-reset-hp-ilo-lights-out-user-and-password-settings-with-ipmitools",
    "id": "3840",
    "link": "http://russell2.ballestrini.net/how-to-reset-hp-ilo-lights-out-user-and-password-settings-with-ipmitools/",
    "title": "How to reset HP iLO Lights-Out User and Password Settings with IPMItool",
    "content": "<p>\nDo NOT follow guides that suggest to make a DOS boot disk, this is over complicated.\n</p>\n\n<p>\nUse the <code>ipmitool</code> which ships with most Unix based operating systems.  I tested on SmartOS and Ubuntu Linux.  Use a live boot disk if you must.\n</p>\n\n<p>\n<ol>\n\t<li>\nRun <code>ipmitool user list</code> to list all users installed on the iLO.\n</li>\n\t<li>\nChoose a user ID from the list and run <code>ipmitool user set password [ID] <new-password></code>.\n</li>\n\t<li>\nLast, make sure the user is enabled with <code>ipmitool user enable [ID]</code>.\n</li>\n</ol>\n\n\n</p>\n\n<p>\n<strong>Here is a complete set of commands I used to reset user ID 6:</strong>\n</p>\n\n<p>\n<pre>\n[root@1c-c1-de-f0-ad-36 /opt]# ipmitool user list\nID  Name             Callin  Link Auth  IPMI Msg   Channel Priv Limit\n2   ROUSER           true    false      false      Unknown (0x00)\n3   USERID           true    false      false      Unknown (0x00)\n4   OEM              true    false      false      Unknown (0x00)\n5   Operator         true    false      false      Unknown (0x00)\n6   admin            true    false      false      Unknown (0x00)\n... truncated ...\n15  admin            true    false      false      Unknown (0x00)\n16  OEM              true    false      false      Unknown (0x00)\n\n[root@1c-c1-de-f0-ad-36 /opt]# ipmitool user set password 6 mynew-password\n\n[root@1c-c1-de-f0-ad-36 /opt]# ipmitool user enable 6\n</pre>\n</p>\n\n<p>\nNext I logged into the web GUI and cleaned up this crazy list of users.  You may also need to change the privileges on a user ID to grant admin level access.  Run <code>ipmitool user</code> for a list of possible sub-commands.\n</p>\n\n<p>\nAfter I finished poking around with the web GUI, I decided to learn a bit more about IPMI and the <code>ipmitool</code> command.\n</p>\n\n<p>\nI figured out how to get sensor information over the LAN using this command:\n</p>\n<p>\n<pre>sudo ipmitool -I lan -H guy-ilo.foxhop.net -U admin sensor</pre> \n</p>\n\n\n<p>\nI was even able to log into the server OS via Serial-over-LAN (SOL) using this command:\n</p>\n\n<p>\n<pre>sudo ipmitool -I lanplus -H guy-ilo.foxhop.net -U admin sol activate</pre></p>\n</p>\n\n<p>\nSerial-over-LAN completely resolves the need for a complicated JAVA/KVM (keyboard video mouse) setup, I was able to reboot the server and watch the machine POST over the serial connection!  I uploaded a video showing <a href=\"http://www.youtube.com/watch?v=xAFjbKAzB4s\">Serial-Over-LAN with IPMItools to an HP Proliant DL160 G6 running SmartOS</a>.  Now you don't need to shell out $229+ on an Advanced 1 yr single server Licence for HP Lights-Out 100i (LO100i)!\n</p>\n\n\n<strong>Update:</strong> just documenting a few other useful commands for myself here.\n\nPower on and off the server chassis:\n\n<blockquote>\n<pre>\nsudo ipmitool -I lan -H guy-ilo.foxhop.net -U admin chassis power status\nsudo ipmitool -I lan -H guy-ilo.foxhop.net -U admin chassis power on\nsudo ipmitool -I lan -H guy-ilo.foxhop.net -U admin chassis power soft\nsudo ipmitool -I lan -H guy-ilo.foxhop.net -U admin chassis power off\nsudo ipmitool -I lan -H guy-ilo.foxhop.net -U admin chassis power cycle\n</pre>\n</blockquote>\n\n<br/>\nAlso check this out for firmware upgrades: (SPP 2014.09.0) SPP2014090.2014_0827.10.iso\n<br/>\n\n<br/>\nftp://ftp.hp.com/pub/softlib2/software1/cd-generic/p1513074441/v101117/SPP2014090.2014_0827.10.iso\n<br/>\n\n<br/>\n<p><strong>Update:</strong></p>\n\nI ended up flashing the my P410 smart array raid card with CP019316.scexe [6.00] (SPP 2013.02.0) because the newer patches wouldn't run properly.  This was enough however to get my 3TB disks to show up in the raid configuration!\n\n<p>\nCP023869.scexe [6.60] (SPP 2014.09.0) is the next version of the firmware.  I restarted the machine which I believe is important between firmware flashing.  I was able to use the scexe to flash from 6.00 to 6.60!\n</p>\n\n<p>\nI used CP021014.scexe [4.26] to upgrade my DL180 G6 ILO from 4.22 to 4.26. Cool but I don't notice any changes besides the version number...\n</p>\n\n<strong>Update:</strong> apparently after you know the ILO username and password you may also use SSH to connect and manage the server:\n\n\n<blockquote>\n<pre>\nssh admin@guy-ilo.foxhop.net\nadmin@guile-ilo.foxhop.net's password: \n\nLights-Out 100 Management\nCopyright 2005-2007 ServerEngines Corporation\nCopyright 2006-2007 Hewlett-Packard Development Company, L.P.\n\n/./-> help\nRoot Directory\n\n/./-> show\n    /./\n    Targets\n        system1\n        map1\n        \n    Properties\n        \n    Verbs\n        cd\n        version\n        exit\n        show\n        help\n\n/./-> cd system1\n/./system1/-> show\n    /./system1/\n    Targets\n        oemhp_sensors\n        oemhp_frus\n        console1\n        led1\n        \n    Properties\n        name=DL180(Aspen)    _R\n        enabledstate=enabled\n        \n    Verbs\n        cd\n        version\n        exit\n        show\n        reset\n        start\n        stop\n        help\n\n</pre>\n</blockquote> \n\nYou can even trigger the server OS to stop change run levels or mess with chassis power for more extreme measures.\n\n<blockquote>\n<pre>\n/./system1/-> stop\nSystem1 stopped.\n</pre>\n</blockquote>",
    "date": "2014-01-25 09:18:46",
    "timestamp": 1390659526,
    "comments": [
      {
        "id": 166685,
        "parent_id": 0,
        "author_ip": "127.0.0.1",
        "author": "Matthew Matchen",
        "email": "mmatchen@gmail.com",
        "content": "Hi Russel,\n\nSo glad you posted this! We went through four hours of troubleshooting chaos before stumbling across your post. Truly, it was the easiest solution!\n\nThank you!",
        "date": "2015-09-03 12:20:01",
        "timestamp": 1441297201
      },
      {
        "id": 129817,
        "parent_id": 0,
        "author_ip": "127.0.0.1",
        "author": "Jan Veitch",
        "email": "jan@veitch.ca",
        "content": "Thank you, Russell. Yours was the only resource that led me in the correct direction to finally reset the unknown password on our iLO100.",
        "date": "2015-02-24 14:09:57",
        "timestamp": 1424804997
      },
      {
        "id": 129848,
        "parent_id": 129817,
        "author_ip": "127.0.0.1",
        "author": "Russell Ballestrini",
        "email": "russell.ballestrini@gmail.com",
        "content": "My pleasure, I'm very glad that this article helped!",
        "date": "2015-02-24 17:47:08",
        "timestamp": 1424818028
      },
      {
        "id": 168021,
        "parent_id": 167886,
        "author_ip": "127.0.0.1",
        "author": "Russell Ballestrini",
        "email": "russell.ballestrini@gmail.com",
        "content": "Try to use an Ubuntu 14.04 LTS live CD.",
        "date": "2015-09-11 22:47:15",
        "timestamp": 1442026035
      },
      {
        "id": 168024,
        "parent_id": 166685,
        "author_ip": "127.0.0.1",
        "author": "Russell Ballestrini",
        "email": "russell.ballestrini@gmail.com",
        "content": "My pleasure, I\u2019m very glad that this article helped!",
        "date": "2015-09-11 22:51:50",
        "timestamp": 1442026310
      },
      {
        "id": 167886,
        "parent_id": 0,
        "author_ip": "127.0.0.1",
        "author": "Jesper",
        "email": "jtw@energinet.dk",
        "content": "Hi Russell\nI do not have success with you command on my server. Can you see what I am doing wrong?\nThe OS is Suse 10 with HP agents 9.10 installed\n\nMP-server:/opt # ipmitool user list\nCould not open device at /dev/ipmi0 or /dev/ipmi/0 or /dev/ipmidev/0: No such file or directory\nGet User Access command failed (channel 14, user 1)\n\nRegards\nJesper",
        "date": "2015-09-11 04:24:41",
        "timestamp": 1441959881
      }
    ],
    "metadata": {
      "_edit_last": "1",
      "_spost_short_title": null,
      "_stcr@_zsxdswewsdsa@outlook.com": "2014-06-15 18:36:43|Y",
      "_stcr@_jan@veitch.ca": "2015-02-24 14:09:57|Y",
      "_stcr@_jtw@energinet.dk": "2015-09-11 04:24:41|Y"
    }
  },
  "the-three-deployment-management-strategies": {
    "name": "the-three-deployment-management-strategies",
    "id": "3870",
    "link": "http://russell2.ballestrini.net/the-three-deployment-management-strategies/",
    "title": "The Three Deployment Management Strategies",
    "content": "There are three deployment management strategies that could be used to maintain a system.  Each has pros and cons which I outline in this document.\n\n<p>\n<dl>\n\n  <dt><strong>run once</strong></dt>\n  <dd>\n  <p>\n  A proceedure that is run once and only once to setup a system's configuration values and settings.  A semaphore or flag generally blocks repeated executions to prevent an undesirable outcome.  \n  </p>\n  <p>\n  Development Difficulty: LOW - binary state aware (has run/has not run)\n  </p>\n  </dd>\n\n  <dt><strong>run always</strong></dt>\n  <dd>\n  <p>\n  A proceedure that is safe to run multiple times but uses a brute force method to setup a system's configuration and settings.  It will self-heal at the expense of clobbering existing state.\n  </p>\n  <p>\n  Development Difficulty: MEDIUM - not state aware - attention must be spent to allow multiple executions\n  </p>\n  </dd>\n\n  <dt><strong>run as needed</strong></dt>\n  <dd>\n  <p>\n  A proceedure that is safe to run multiple times and checks a system's configuration and settings before it sets a system's configuration or settings.  \"checks before it steps\".  It will self-heal only the values it needs to.\n  </p>\n  <p>\n  Development Difficulty: HIGH - state aware - must know how to both get and set values and make desicions based on what it finds.\n  </p>\n  </dd>\n\n</dl>",
    "date": "2014-02-03 15:06:24",
    "timestamp": 1391457984,
    "comments": [],
    "metadata": {
      "_edit_last": "1",
      "_spost_short_title": null
    }
  },
  "mysql-back": {
    "name": "mysql-back",
    "id": "3897",
    "link": "http://russell2.ballestrini.net/mysql-back/",
    "title": "mysql-back",
    "content": "<p>\n<code>mysql-back</code>is a backup utility script to dump (backup) and gzip every MySQL database on a host.  \n</p>\n\n<p>I use <code>mysql-back</code> in combination with cron to perform regular database dumps of MySQL servers to the <code>/archive/db</code> partition on localhost.  I then have a central long term storage server that collects the <code>/archive</code> partition from every host.\n</p>\n\n<p>\n<strong>mysql-back:</strong>\n<pre>\n#!/bin/bash\n\nUSER=\"root\"\nPASSWORD=\"\"\nTODAY=`date +%Y-%m-%d`\nOUTPUTDIR=\"/archive/db/$TODAY\"\nMYSQLDUMP=`which mysqldump`\nMYSQL=`which mysql`\nGZIP=`which gzip`\n\nmkdir $OUTPUTDIR\n\n# get a list of databases\ndatabases=`$MYSQL --user=$USER --password=$PASSWORD \\\n -e \"SHOW DATABASES;\" | tr -d \"| \" | grep -v Database`\n\n# dump each database in turn\nfor db in $databases; do\n    $MYSQLDUMP --force --opt --user=$USER --password=$PASSWORD \\\n    --databases $db > \"$OUTPUTDIR/$db.sql\"\ndone\n\n# compress all of the files\n$GZIP $OUTPUTDIR/*\n\n# clean up all dumps 30 days old\nfind /archive/db -ctime +30 -type d | xargs rm -rf\n</pre>\n</p>\n\n<p>\nIf you ever need to restore, like in my case when I moved all my databases over to a brand new Percona MySQL SmartOS zone, I used the following command:\n\n<pre>\nzcat *sql.gz | mysql -u root -p\n</pre>\n\n</p>",
    "date": "2014-02-07 16:39:34",
    "timestamp": 1391809174,
    "comments": [],
    "metadata": {
      "_edit_last": "1",
      "_spost_short_title": null
    }
  },
  "tar-back": {
    "name": "tar-back",
    "id": "3900",
    "link": "http://russell2.ballestrini.net/tar-back/",
    "title": "tar-back",
    "content": "<p>\n<code>tar-back</code> is a backup utility to tar and gzip target filesystems. \nIt supports a custom retention, filter exclusions, and backup directory. \n</p>\n\n<p>\nI use <code>tar-back</code> in combination with cron to perform regular backups of all localhost filesystems into <code>/archive/fs</code>.  I then have a central long term storage server that collects the <code>/archive</code> partition from every host.\n</p>\n\n<p>\n<strong>tar-back:</strong>\n<pre>\n#!/usr/bin/env python\n\nDESCRIPTION = \"\"\"A backup utility to tar and gzip\ntarget filesystems. Custom retention, filter exclusions and\nbackup directory. \n\"\"\"\n\nfrom os import path\nfrom sys import exit\nfrom shutil import move\nfrom optparse import OptionParser\nimport tarfile\n \ndef filter_exclusions( tarinfo ):\n    \"\"\"Accept tarinfo, return tarinfo or None\"\"\"\n    if o.filters:\n        for filtr in o.filters:\n            if filtr in tarinfo.name:\n                return None\n    return tarinfo\n\ndef exclude_exclusions( filename ):\n    \"\"\"Accept filename, return True or False\"\"\"\n    \"\"\"support for python 2.6 or lower\"\"\"\n    if o.filters:\n        for filtr in o.filters:\n            if filtr in filename:\n                return True\n    return False\n\ndef file_rotate( target, retention = 3 ):\n    \"\"\"file rotation routine\"\"\"\n    for i in range( retention-2, 0, -1 ): # count backwards\n        old_name = \"%s.%s\" % ( target, i )\n        new_name = \"%s.%s\" % ( target, i + 1 )\n        try: move( old_name, new_name  )\n        except IOError: pass\n    move( target, target + '.1' )\n\nif __name__ == '__main__':\n    \n    p = OptionParser()  # create an option parser object\n\n    p.set_description( DESCRIPTION )\n\n    p.add_option( '-t', '--targets',\n      help='list of target filesystems to backup (coma delimited)',\n      default=None, dest='targets', metavar='\"/home\"',\n    )\n\n    p.add_option( '-b', '--backup-dir',\n      help='path to backup directory',\n      dest='backdir', metavar='\"PATH\"', \n    )\n\n    p.add_option( '-f', '--filters',\n      help='list of filter patterns to exclude (coma delimited)',\n      default=None, dest='filters', metavar='\".mp3\"',\n    )\n\n    p.add_option( '-r', '--retention', \n      help='backups to retain [default: 3]',\n      default=3, type='int', dest='retention', metavar='amount',\n    )\n\n    o, args = p.parse_args()  # parse options and args\n\n    extension = '.tar.gz'\n  \n    if not o.targets: \n        exit( \"missing filesystem target, run --help\" )\n    if not o.backdir: \n        exit( \"missing backup directory, run --help\" )\n    if not path.isdir( o.backdir ):\n        exit( \"backup-dir %s does not exist\" % o.backdir )\n\n    if o.filters:\n        o.filters = o.filters.split(',')\n\n    o.targets = o.targets.split(',')\n\n    for target in o.targets:\n        slug = target.replace( '/', '-' ).lstrip( '-' ) # change / to - \n        \n        if slug == '': slug = 'r' # / slug is empty (root)\n\n        tarpath = path.join( o.backdir, slug + extension )\n\n        if path.isfile( tarpath ):\n            file_rotate( tarpath, o.retention )\n\n        tar = tarfile.open( tarpath, 'w:gz' ) \n        try:\n            tar.add( target, filter=filter_exclusions )\n        except TypeError:\n            tar.add( target, exclude=exclude_exclusions )\n        tar.close()\n</pre>\n</p>",
    "date": "2014-02-07 16:49:40",
    "timestamp": 1391809780,
    "comments": [],
    "metadata": {
      "_edit_last": "1",
      "_spost_short_title": null
    }
  },
  "automatic-backups": {
    "name": "automatic-backups",
    "id": "3910",
    "link": "http://russell2.ballestrini.net/automatic-backups/",
    "title": "Automatic Backups",
    "content": "<p>\nI maintain many scripts and applications for creating automatic backups or various systems.  This page will act as a hub to each of those solutions.\n</p>\n\n<dl>\n\n  <dt>\n    <strong><a href=\"/tar-back\">tar-back</a></strong>\n  </dt>\n  <dd>tar-back is a backup utility to tar and gzip target filesystems. It supports a custom retention, filter exclusions, and backup directory.</dd>\n\n  <dt>\n    <strong><a href=\"/virt-back-a-python-libvirt-backup-utility-for-kvm-xen-virtualbox/\">virt-back</a></strong>\n  </dt>\n  <dd>virt-back virt-back is a python application that uses the libvirt API to safely shutdown, gzip, and restart guests.  Supports KVM, QEMU, XEN, Virtualbox, and more.</dd>\n\n  <dt>\n    <strong><a href=\"/mysql-back\">mysql-back</a></strong>\n  </dt>\n  <dd>mysql-back is a backup utility script to dump (backup) and gzip every MySQL database on a host.</dd>\n\n  <dt>\n    <strong><a href=\"/backup-all-virtual-machines-on-a-smartos-hypervisor-with-smart-back-sh/\">smart-back</a></strong>\n  </dt>\n  <dd>smart-back is a utility used to backup of every virtual machine on a SmartOS hypervisor.</dd>\n\n</dl>",
    "date": "2014-02-07 17:14:03",
    "timestamp": 1391811243,
    "comments": [],
    "metadata": {
      "_edit_last": "1",
      "_spost_short_title": null
    }
  },
  "you-can-hack-on-freenas-9": {
    "name": "you-can-hack-on-freenas-9",
    "id": "3937",
    "link": "http://russell2.ballestrini.net/you-can-hack-on-freenas-9/",
    "title": "You can hack on FreeNAS 9",
    "content": "<p>\nThis post analyses the FreeNAS 9 code base and discusses the various places users may feel confident to hack on.\n</p>\n\n<p>\nFreeNAS uses the following software stack:\n</p>\n\n<p>\n<dl>\n\n<dt>Django</dt>\n<dd>A Python Web Application Framework which complies with WSGI</dd>\n\n<dt>Nginx</dt>\n<dd>A very fast web server which may act as a reverse proxy server for HTTP, HTTPS, SMTP, POP3, and IMAP protocols, as well as a load balancer and HTTP cache.</dd>\n\n<dt>Dojo Toolkit</dt>\n<dd>The Javascript toolkit used to create widgets and handle client side processing.</dd>\n\n<dt>FreeBSD</dt>\n<dd>FreeBSD is an advanced computer operating system used to power modern servers.</dd>\n\n<p>\nCommon entry points for FreeNAS hacking:\n</p>\n\n<dt>Want to hack on the frontend web application?</dt>\n<dd>Start here if you enjoy Python, or if you really enjoy coding on Django applications: \n<br/>\n<a href=\"https://github.com/freenas/freenas/tree/master/gui\">https://github.com/freenas/freenas/tree/master/gui</a>\n</dd>\n\n<dt>Want to hack on the GUI?</dt>\n<dd>Start here if you are a front end developer and enjoy writing HTML, CSS, and working with Javascript:<br/>\n<a href=\"https://github.com/freenas/freenas/tree/master/gui/templates\">https://github.com/freenas/freenas/tree/master/gui/templates</a>\n</dd>\n\n<dt>Want to change Nginx?</dt>\n<dd>\nTake a look here if you would like to review, change, or tune Nginx on FreeNAS: \n<br/>\n<a href=\"https://github.com/freenas/freenas/tree/master/nanobsd/Files/usr/local/etc/nginx\">https://github.com/freenas/freenas/tree/master/nanobsd/Files/usr/local/etc/nginx</a>\n<br/>This directory holds the nginx \"vhost\" config files and CGI parameters.\n</dd>\n\n<dt>Want to hack on the OS?</dt>\n<dd>Start here, if you know about <code>FreeBSD</code> or operating systems in general: \n<br/>\n<a href=\"https://github.com/freenas/freenas/tree/master/nanobsd\">https://github.com/freenas/freenas/tree/master/nanobsd</a>\n<br/>\nThis directory seems like a customized and completely version controlled nanoBSD install!\n</dd>\n</dl>",
    "date": "2014-05-15 01:06:23",
    "timestamp": 1400130383,
    "comments": [],
    "metadata": {
      "_edit_last": "1",
      "_spost_short_title": null
    }
  },
  "replace-the-nagios-scheduler-and-nrpe-with-salt-stack": {
    "name": "replace-the-nagios-scheduler-and-nrpe-with-salt-stack",
    "id": "3951",
    "link": "http://russell2.ballestrini.net/replace-the-nagios-scheduler-and-nrpe-with-salt-stack/",
    "title": "Replace the Nagios Scheduler and NRPE with Salt Stack",
    "content": "<p>\nNote: I will update this post as I progress.\n</p>\n\n<p>\nSo the idea is to use Salt Stack's remote execution to communicate with all nodes and run the Nagios checks and collect the return output instead of using the NRPE client/service protocol.  This reduces the number of agents running on each host and appears significantly more secure.  Salt Stack uses public/private crypto on top of the ZMQ publisher/subscriber model.  This means the communication transport is very fast, very secure, and all nodes will run checks in parallel!\n</p>\n\n<p>\nFirst, we need to install the Nagios checks and plugins on each host or minion.\nI used the following State Formula on my Ubuntu and Debian hosts: \n</p>\n\n\n<strong>salt://nagios/plugins.sls</strong>\n<blockquote>\n<pre>\nnagios-plugins:\n  pkg:\n    - installed\n\nnagios-plugins-extra:\n  pkg:\n    - installed\n</pre>\n</blockquote>\n\n<strong>salt://top.sls</strong>\n<blockquote>\n<pre>\nbase:\n  '*'\n    - nagios.plugins\n</pre>\n</blockquote>\n\n</p>\n\n\n<p>\nI kicked off a highstate (<code>salt '*' state.highstate</code>) on all minions and eventually they all returned in the affirmative.  We now have all the Nagios plugins and checks installed on each host.\n</p>\n\n<p>\nThis next part is FUN, We run a check on every host concurrently.  \n</p>\n\n<p>\n\n<blockquote>\n\n<p>\nThis particular check counts the number of processes running on each host and warns at 150 and criticals at 200.\n</p>\n\n<p>\n<pre>\nsalt '*' --out=json --static cmd.run_all '/usr/lib/nagios/plugins/check_procs -w 150 -c 200'\n</pre>\n</p>\n\n<p>\nOutput static JSON - Wait for all minions to return and then generate a single JSON object.\n</p>\n\n<p>\n<pre>\n{\n    \"graphite.foxhop.net\": {\n        \"pid\": 24356, \n        \"retcode\": 0, \n        \"stderr\": \"\", \n        \"stdout\": \"PROCS OK: 78 processes\"\n    }, \n    \"akuma.foxhop.net\": {\n        \"pid\": 4610, \n        \"retcode\": 1, \n        \"stderr\": \"\", \n        \"stdout\": \"PROCS WARNING: 158 processes\"\n    }, \n    \"ken.foxhop.net\": {\n        \"pid\": 31254, \n        \"retcode\": 2, \n        \"stderr\": \"\", \n        \"stdout\": \"PROCS CRITICAL: 392 processes\"\n    } \n}\n</pre>\n</p>\n<p>\nReview the <a href=\"http://nagios.sourceforge.net/docs/3_0/pluginapi.html\">Nagios Plugin API</a> for more information about return codes and STDOUT formats.\n</p>\n</blockquote>\n\n\n<p>\nLast, we take this JSON data perform further processing or display it in a meaningful way.\n</p>\n\n<p>\nFor example, we could generate an HTML/JavaScript dashboard from the raw JSON objects.  We could cut metrics out of the STDOUT and persist historic values in a data store.  We could push the data into another system like Graphite or even send an email alerts.\n</p>\n\n<p>\nThis solution has all the perks of Nagios without ANY of the cons!\n</p>\n\n<p>\nReturning the check output data to the CLI isn't super useful for further processing, so I hacked in a way to return data back to the master using the encrypted zeromq bus.  \n</p>\n\n<p>\n<strong>_returners/zeromq_return.py</strong>\n<pre>\n# -*- coding: utf-8 -*-\n'''\nThe zeromq returner will send return data back to the Salt Master over the\nEncrypted 0MQ event bus with a custom tag for filtering on the other end. \n\nBasically after the remote execution finishes, the ret data is \"packaged\" into\na special \"envelope\" which triggers the local Salt Minion Daemon to\nforward the ret data to the Salt Master's event bus. \n\nThe \"package\" basically wraps the ret data and uses the tag 'fire_master'.\n\nFor example, a ret data object from the execution of test.ping\nwould be \"packaged\" like this::\n\n  ret = {\n    'graphite.foxhop.net': true\n  }\n\n  ret['tag'] = 'third-party'\n\n  package = {\n    'events': [ ret ],\n    'tag': None,\n    'pretag': None,\n    'data': None\n  }\n\nThe Salt Minion Daemon will forward this package to the Salt Master\nwhere a 3rd party script may be filtering on the specified internal event tag.\n\nTo use the zeromq returner, append '--return zeromq' to the salt command. ex::\n\n  salt --return zeromq '*' test.ping \n\nTODO:\n\n figure out a way for user to define custom tag for filtering ... \n Most returners use the Salt Minion config file to supply returner\n details... that is not optimal, it would be ideal if the custom tag\n could be supplied on the CLI when the remote execution is run, like::\n\n   --return=zeromq --tag=mytag\n\n'''\n\n# needed to log to log file\nimport logging\n\n# needed for config to opts processing\nimport os\nimport salt.syspaths as syspaths\nfrom salt.config import minion_config\n\n# needed to send events over ZMQ\nimport salt.utils.event\n\nlog = logging.getLogger(__name__)\n\n# needed to define the module's virtual name\n__virtualname__ = 'zeromq'\n\ndef __virtual__():\n    return __virtualname__\n\n\ndef returner(ret):\n    '''\n    Send the return data to the Salt Master over the encrypted\n    0MQ bus with custom tag for 3rd party script filtering.\n    '''\n\n    # get opts from minion config file, supports minion.d drop dir!\n    opts = minion_config(os.path.join(syspaths.CONFIG_DIR, 'minion'))\n\n    # TODO: this needs to be customizable!\n    tag = 'third-party'\n\n    # add custom tag to return data for filtering\n    ret['tag'] = tag\n\n    # multi event example, supports a list of event ret objects.\n    # single event does not currently expand/filter properly on Master side.\n    package = {\n      #'id': opts['id'],\n      'events': [ ret ],\n      'tag': None,\n      'pretag': None,\n      'data': None\n    }\n\n    # opts must contain valid minion ID else it binds to invalid 0MQ socket.\n    event = salt.utils.event.SaltEvent('minion', **opts)\n\n    # Fire event payload with 'fire_master' tag which triggers the\n    # salt-minion daemon to forward payload to the master event bus!\n    event.fire_event(package, 'fire_master')\n</pre> \n</p>\n\n<p>\nNext we run a third party application on the Salt Master which subscribes to our events by filtering on the special tag ('third-party').  \n</p>\n\n<p>\n<strong>listen_to_master_bus.py</strong>\n<pre>\n# event libary for events over ZMQ\nimport salt.utils.event\n\n# create event object, attach to master socket ...\nevent = salt.utils.event.MasterEvent('/var/run/salt/master')\n\ntag = 'third-party'\n\nprint('Listening for events tagged \\'{}\\' on Salt Master bus.'.format(tag))\n\n# generator iterator yields events forever, we filter on tag\nfor data in event.iter_events(tag=tag):\n    print(data)\n</pre>\n\nThis small application just prints the incoming return data, but it could easily be expanded to process the incoming return data and persist it somewhere.\n</p>\n\n<p>\nOpen two terminals on the Salt Master host.\n<blockquote>\n<pre>\n# on terminal 1 run:\npython listen_to_master_bus.py\n</pre>\n</blockquote>\n<blockquote>\n<pre>\n# on terminal 2 run:\nsalt '*' --return zeromq cmd.run_all '/usr/lib/nagios/plugins/check_procs -w 150 -c 200'\n</pre>\nSame remote execution check as before but now our new returner will make data appear in terminal 1!\n</blockquote>\n</p>\n\n<p>\nThis zeromq returner is more of a proof-of-concept.  I think the salt remote execution command line tool should allow end-users to provide a <code>--tag</code> so that data may be feed directly back to a third-party script listening to the Salt Master's event bus which filters on the particular tag.  My next step is to look into what it would take to build in this functionality.\n</p>\n\n<p>\nIn the future I want to rig up the Salt scheduler to invoke these remote execution checks on a steady and predictable cadence.  Nagios historically runs checks every 5 minutes.  The Salt scheduler will allow us schedule different checks with different frequencies. For example, I might want my load checks and metrics to be collected every 10 secs, but my disk capacity usage checked every 2 minutes.  This fine-grain control is super powerful!\n</p>",
    "date": "2014-03-08 15:53:06",
    "timestamp": 1394311986,
    "comments": [
      {
        "id": 63194,
        "parent_id": 0,
        "author_ip": "127.0.0.1",
        "author": "Daniel",
        "email": "dk@danielkvist.net",
        "content": "Wow, this looks really interesting. I'm new to both Salt and Nagios and have created a small setup at my home for testing. How do you get the data into Nagios?",
        "date": "2014-09-15 14:56:34",
        "timestamp": 1410807394
      },
      {
        "id": 95374,
        "parent_id": 92482,
        "author_ip": "127.0.0.1",
        "author": "Russell Ballestrini",
        "email": "russell.ballestrini@gmail.com",
        "content": "Yeah Sensu is pretty awesome, checkout my test here: http://russell.ballestrini.net/sensu-salt/",
        "date": "2014-11-24 09:29:08",
        "timestamp": 1416839348
      },
      {
        "id": 92482,
        "parent_id": 0,
        "author_ip": "127.0.0.1",
        "author": "kuroshi",
        "email": "smjulian@purdue.edu",
        "content": "Have you looked into Sensu? It's sort of similar - it's similar to nagios but uses RabbitMQ for communication. Not quite as nice as ZeroMQ IMO, but sensu itself does work quite nicely in my experience.",
        "date": "2014-11-17 18:57:22",
        "timestamp": 1416268642
      }
    ],
    "metadata": {
      "_stcr@_srinivasdadi2185@gmail.com": "2015-09-21 01:08:07|Y",
      "_edit_last": "1",
      "_spost_short_title": null,
      "_stcr@_dk@danielkvist.net": "2014-09-15 14:56:34|Y",
      "_stcr@_smjulian@purdue.edu": "2014-11-17 18:57:22|Y"
    }
  },
  "filter-salt-stack-return-data-output": {
    "name": "filter-salt-stack-return-data-output",
    "id": "4001",
    "link": "http://russell2.ballestrini.net/filter-salt-stack-return-data-output/",
    "title": "Filter Salt Stack Return Data Output",
    "content": "<p>\nSometimes you only want to see what has changed, and that is OK.\n</p>\n\n<p>\nCreate a file like this:\n</p>\n\n<p>\n<strong>filter.py</strong>\n\n<blockquote>\n<pre>\n#!/usr/bin/python\n\nfrom json import loads\nfrom json import dumps\n\nimport fileinput\n\nstdin_lines = [line for line in fileinput.input()]\n\nret = loads(''.join(stdin_lines))\n\nfor minion_id, data in ret.items():\n    print(minion_id)\n    print('='*len(minion_id))\n    for key, value in ret[minion_id].items():\n        if value['changes'] or value['result'] == False:\n            print('')\n            print(dumps(value, indent=4))\n            print('')\n</pre>\n</blockquote>\n</p>\n\n<p>\nMake the file executable:\n<blockquote>\n<pre>\nchmod 755 filter.py\n</pre>\n</blockquote>\n</p>\n\n<p>\nExecute your remote execution like this:\n</p>\n\n<blockquote>\n<p>\n<pre>sudo salt-call --out=json state.highstate | ./filter.py</pre>\n</p>\n\n<p>\n<pre>sudo salt '*' --out=json  --timeout=60 --static state.highstate | ./filter.py</pre>\n</p>\nThe flags <code>--timeout=60</code> and <code>--static</code> will cause the Salt command to block until the specified seconds for each minion to return results.  We then pipe the returned JSON into our <code>filter.py</code> script to filter out only the changes and failures!\n</blockquote>\n\n<p>\nProfit!\n</p>\n\n<p>\nChange the conditional depending on what you want.  For example, for just failures do this:\n</p>\n\n<blockquote>\n<p>\n<pre>if value['result'] == False:</pre>\n</p>\n</blockquote>\n\n<p>\nExample output:\n</p>\n\n<blockquote>\n<p>\n<pre>\n# sudo salt 'graphite.foxhop.net' --out=json --static --timeout=60 state.highstate | ./filter.py\n\ngraphite.foxhop.net\n===================\n\n{\n    \"comment\": \"File /tmp/taco updated\",\n    \"__run_num__\": 15,\n    \"changes\": {\n        \"diff\": \"New file\",\n        \"mode\": \"0640\"\n    },  \n    \"name\": \"/tmp/taco\",\n    \"result\": true\n}   \n\n</pre>\n</p>\n</blockquote>",
    "date": "2014-03-13 18:21:27",
    "timestamp": 1394749287,
    "comments": [
      {
        "id": 69135,
        "parent_id": 0,
        "author_ip": "127.0.0.1",
        "author": "someguy",
        "email": "random@nonexiste.nt",
        "content": "Thank you. I set up a local Salt on a server using salt-call, using your script in crontab, so for anyone else who might want to do the same thing:\n\nIn order to run salt-call every 20 minutes from crontab and get mail when something gets changed, comment out line 13 and 14, the ones that print minion_id and \"=\" signs, and add this line to /etc/crontab:\n*/20  *   *   *   *  root   salt-call --local state.highstate --out=json -l quiet | /path/to/filter.py",
        "date": "2014-10-01 05:38:27",
        "timestamp": 1412156307
      },
      {
        "id": 129121,
        "parent_id": 0,
        "author_ip": "127.0.0.1",
        "author": "atejedadev",
        "email": "alexis.tejeda@gmail.com",
        "content": "Thanks for the post, useful!\n\nI changed a little bit to have the parsing when a new line gets in (for long term operations) instead to wait for the end stream and then parse the data.\n\nBTW: I used the raw output.\n\n```\nr = { \n    \"True\"  : \"true\", \n    \"False\" : \"false\", \n    \"\\\"\"    : \"\\\\\\\"\",\n    \"\\'\"    : \"\\\"\" \n}\n\nfor l in fileinput.input():\n    for k in r: l = l.replace(k,r[k])\n    j = loads(l)\n    print j\n```",
        "date": "2015-02-20 02:47:53",
        "timestamp": 1424418473
      },
      {
        "id": 129124,
        "parent_id": 129121,
        "author_ip": "127.0.0.1",
        "author": "atejedadev",
        "email": "alexis.tejeda@gmail.com",
        "content": "Related to my previous comment, there's probably missing replacements that can be added down the road. The code is just an example but you will get it.",
        "date": "2015-02-20 02:56:14",
        "timestamp": 1424418974
      },
      {
        "id": 129523,
        "parent_id": 129121,
        "author_ip": "127.0.0.1",
        "author": "Russell Ballestrini",
        "email": "russell.ballestrini@gmail.com",
        "content": "Welcome!",
        "date": "2015-02-22 11:55:57",
        "timestamp": 1424624157
      }
    ],
    "metadata": {
      "_edit_last": "1",
      "_spost_short_title": null,
      "_stcr@_alexis.tejeda@gmail.com": "2015-02-20 02:47:53|Y"
    }
  },
  "irc-bot-foxbot-runs-canned-remote-executions-using-salt-stack": {
    "name": "irc-bot-foxbot-runs-canned-remote-executions-using-salt-stack",
    "id": "4016",
    "link": "http://russell2.ballestrini.net/irc-bot-foxbot-runs-canned-remote-executions-using-salt-stack/",
    "title": "IRC Bot (Foxbot) runs canned remote executions using Salt Stack",
    "content": "<p>I extended my IRC Bot Foxbot today to allow it to run canned remote executions on behalf of users in an IRC channel.  This is only a prototype or proof-of-concept.  Be very careful not to allow users to inject their own commands.  Foxbot must be running on the Salt Master and must be running as the same user that runs the salt-master daemon.</p>\n\n<p>\nThe code lives here: <a href=\"https://bitbucket.org/russellballestrini/foxbot/src/tip/plugins/checks.py\">foxbot/plugins/checks.py</a>\n</p>\n\n<p>\nExample usage:\n</p>\n\n<p>\n<blockquote>\n<pre>\n16:18:25            * | russell checks uptime minion2.foxhop.net\n16:18:25       foxbot | minion2.foxhop.net:  16:18:25 up 496 days, 22:36, 17 users,  load average: 0.33, 0.70, 0.87\n</pre>\n</blockquote>\n</p>\n\n<p>\n<blockquote>\n<pre>\n16:29:10            * | russell checks procs *\n16:29:17       foxbot | minion2.foxhop.net: PROCS WARNING: 165 processes\n16:29:17       foxbot | minion5.foxhop.net: PROCS CRITICAL: 309 processes\n</pre>\n</blockquote>\n</p>",
    "date": "2014-03-15 16:33:50",
    "timestamp": 1394915630,
    "comments": [],
    "metadata": {
      "_edit_last": "1",
      "_spost_short_title": null
    }
  },
  "how-to-patch-heartbleed-openssl-defect-libssl-on-ubuntu": {
    "name": "how-to-patch-heartbleed-openssl-defect-libssl-on-ubuntu",
    "id": "4054",
    "link": "http://russell2.ballestrini.net/how-to-patch-heartbleed-openssl-defect-libssl-on-ubuntu/",
    "title": "How to patch Heartbleed OpenSSL defect (libssl) on Ubuntu",
    "content": "<p>\nLots of people claim that you need to upgrade openssl package, but this will not fix the issue.\n</p>\n\n<p>\nThe issue is not the openssl package, it is one of the libraries that the package relies on (libssl).\nhttp://www.ubuntu.com/usn/usn-2165-1/\n</p>\n\n<p>\nThe output of <code>openssl version -a</code> command should have a <code>built on</code> date older then <code>Mon Apr  7 20:33:29 UTC 2014</code>. After patching openssl we still see the vulnerable date:\n</p>\n\n<pre>\nopenssl version -a | egrep \"OpenSSL|built\"\nOpenSSL 1.0.1 14 Mar 2012\nbuilt on: Tue Aug 21 05:18:48 UTC 2012\n</pre>\n\nNow we patch <code>libssl1.0.0</code>:\n<pre>\nsudo apt-get update\nsudo apt-get install libssl1.0.0\n</pre>\n\nNotice the patched <code>built on</code> date:\n<pre>\nopenssl version -a | egrep \"OpenSSL|built\"\nOpenSSL 1.0.1 14 Mar 2012\nbuilt on: Mon Apr  7 20:33:29 UTC 2014\n</pre>\n\nIn my case I used a Salt remote execution to patch, verify, and restart nginx on all of my 14 hosts:\n\n<pre>\nsudo salt '*' cmd.run 'apt-get update'\n</pre>\n\n<pre>\nsudo salt '*' cmd.run 'apt-get -y install libssl1.0.0'\n</pre>\n\n<pre>\nsudo salt '*' cmd.run 'openssl version -a | egrep \"OpenSSL|built\"'\n</pre>\n\n<pre>\nsudo salt '*' service.restart nginx\n</pre>\n\n<center>\n<img src=\"http://imgs.xkcd.com/comics/heartbleed.png\" alt=\"\" />\n<br/>\nxkcd 1353\n</center>",
    "date": "2014-04-08 22:42:04",
    "timestamp": 1397011324,
    "comments": [],
    "metadata": {
      "_oembed_4818cfd79c740d2d118b2a4772f2a78b": "{{unknown}}",
      "_edit_last": "1",
      "_spost_short_title": null,
      "_wp_old_slug": "how-to-patch-heartbleed-openssl-defect-on-ubuntu",
      "_oembed_24e97cb90c37f80790ab54ab153762c8": "{{unknown}}",
      "_oembed_5271a2e8cc437ecc5272ee3b5e145dba": "{{unknown}}"
    }
  },
  "mailpile-salt-states-for-ubuntu-or-debian": {
    "name": "mailpile-salt-states-for-ubuntu-or-debian",
    "id": "4101",
    "link": "http://russell2.ballestrini.net/mailpile-salt-states-for-ubuntu-or-debian/",
    "title": "Mailpile Salt States for Ubuntu or Debian",
    "content": "<p>\nI wrote these Salt States to install Mailpile on an Ubuntu host.  Fun fact, it took me 20 minutes to write these states and they worked the first time I ran them.  Disclaimer - I used a throw away server and wasn't concerned that buckets of packages were installed to the system instead of using a virtualenv.\n</p>\n\n<p>\n<pre>\ncd /opt/Mailpile\n./mp --set sys.http_host=0.0.0.0\n./mp\n</pre>\n</p>\n\n<p>\nThen open a web browser the IP address of the host running the mp command and follow the prompts to setup the server/client/app.\n</p>\n\n<p>\n<strong>mailpile/init.sls:</strong>\n<pre>\n# Clone the source repository\nmailpile-git-latest:\n  git.latest:\n    - name: https://github.com/pagekite/Mailpile.git\n    - target: /opt/Mailpile\n\n# install the system requirements\nmailpile-system-packages:\n  pkg.installed:\n    - names:\n      - make\n      - python-imaging\n      - python-lxml\n      - python-jinja2\n      - pep8\n      - ruby-dev\n      - yui-compressor\n      - python-nose\n      - spambayes\n      - phantomjs\n      - python-pip\n      - python-mock\n      - python-pexpect\n      {% if grains['lsb_distrib_release']|float >= 14.04 %}\n      - rubygems-integration\n      {% else %}\n      - rubygems\n      {% endif %}\n\n# install some python requirements with pip\nmailpile-pip-packages:\n  pip.installed:\n    - names:\n      - pgpdump\n      - selenium >= 2.40.0\n    - require:\n      - pkg: mailpile-system-packages\n\n# install some ruby requirements with gem\nmailpile-gem-packages:\n  gem.installed:\n    - names:\n      - therubyracer\n      - less\n    - require:\n      - pkg: mailpile-system-packages\n\n</pre>\n</p>",
    "date": "2014-08-15 20:32:32",
    "timestamp": 1408149152,
    "comments": [],
    "metadata": {
      "_edit_last": "1",
      "_spost_short_title": null
    }
  },
  "heka-world2": {
    "name": "heka-world2",
    "id": "4113",
    "link": "http://russell2.ballestrini.net/heka-world2/",
    "title": "Heka, World2!",
    "content": "<p>\nThis article expands on my <a href=\"http://russell.ballestrini.net/heka-world/\">\u201cHello World\u201d for Heka</a> blog post.  Check that one out first if you are new to Heka.\n\n<p>In this guide we introduce using Heka over the network by utilizing two Hekad processes on localhost.  For discussion purposes we name one of the Hekad processes \"sender\" and the other \"receiver\".</p>\n\n<p>\n<ul>\n\t<li>The \"sender\" will watch a log file and emit messages to localhost TCP port 9612.</li>\n\t<li>The \"receiver\" will listen on localhost TCP port 9612 and emit message payloads to file.</li>\n</ul>\n</p>\n\n<p>\n<strong>hello-heka-file-in-tcp-out.toml (sender):</strong>\n<pre>\n# watch /tmp/input.log and output to TCP port 9612 on localhost\n[hello_heka_input_log]\ntype = \"LogstreamerInput\"\nlog_directory = \"/tmp\"\nfile_match = 'input\\.log'\n\n[tcp_out:9612]\ntype = \"TcpOutput\"\nmessage_matcher = \"TRUE\"\naddress = \"127.0.0.1:9612\"\n#encoder = \"hello_heka_output_encoder\"\n#\n#[hello_heka_output_encoder]\n#type = \"PayloadEncoder\"\n#append_newlines = false\n\n</pre>\n</p>\n\n\n<p>\n<strong>hello-heka-tcp-in-file-out.toml (receiver):</strong>\n<pre>\n# listen to TCP port 9612 and emit to /tmp/output.log\n[tcp_in:9612]\ntype = \"TcpInput\"\nparser_type = \"message.proto\"\ndecoder = \"ProtobufDecoder\"\naddress = \":9612\"\n\n[hello_heka_output_log]\ntype = \"FileOutput\"\nmessage_matcher = \"TRUE\"\npath = \"/tmp/output.log\"\nperm = \"664\"\nencoder = \"hello_heka_output_encoder\"\n\n[hello_heka_output_encoder]\ntype = \"PayloadEncoder\"\nappend_newlines = false\n</pre>\n</p>\n\n<p>\nNow that we have config files, let us start our hekad processes,  Open three terminals.\n<ol>\n<li>in terminal 1: <pre>sudo hekad -config=hello-heka-file-in-tcp-out.toml</pre></li>\n<li>in terminal 2: <pre>sudo hekad -config=/tmp/hello-heka-tcp-in-file-out.toml</code></li></pre>\n<li>in terminal 3: <pre>echo 'Heka, World2!' >> /tmp/input.log</pre></li>\n<li>in terminal 3: <pre>cat /tmp/output.log</pre></li>\n</ol>\nAgain, like magic, the data echoed into input.log shows up in output.log.  This time the data traveled over TCP between to separate Hekad processes.  I leave changing the configuration to support separate hosts to the reader.\n</p>\n\n<p>\nBy default TCP sender encodes the message with Protobuf (ProtobufEncoder) and the TCP reciever decodes the message with Protobuf (ProtobufDecoder).\n</p>\n\n<p>\nIn my testing I decided to make the TCP sender use the PayloadEncoder and then instead of using a second hekad process, I used <code>nc -l 9612</code> to listen on the port.  When data was added to <code>/tmp/input.log</code> it showed up in the netcat terminal because hekad was watching the file and emiting just payload portion of the message to TCP 9612 which netcat was listening on.  I left this configuration in the examples above, simply uncomment to reproduce.\n</p>\n\n<p>\nRead this for <a href=\"http://www.foxhop.net/linux-nc-and-python-sockets\">more fun with netcat</a>.\n</p>",
    "date": "2014-08-23 22:54:23",
    "timestamp": 1408848863,
    "comments": [],
    "metadata": {
      "_edit_last": "1",
      "_spost_short_title": null
    }
  },
  "reality-shattered": {
    "name": "reality-shattered",
    "id": "4132",
    "link": "http://russell2.ballestrini.net/reality-shattered/",
    "title": "reality shattered",
    "content": "<center>\n<img src=\"/wp-content/uploads/2014/09/2014-09-17-reality-shattered.png\" />\n</center>",
    "date": "2014-09-18 21:11:07",
    "timestamp": 1411089067,
    "comments": [],
    "metadata": {
      "_edit_last": "1",
      "_spost_short_title": null,
      "_wp_old_slug": "4132"
    }
  },
  "rabbits": {
    "name": "rabbits",
    "id": "4136",
    "link": "http://russell2.ballestrini.net/rabbits/",
    "title": "rabbits",
    "content": "<center>\n<img src=\"/wp-content/uploads/2014/09/2014-09-18-rabbits.png\" />\n</center>",
    "date": "2014-09-18 21:11:44",
    "timestamp": 1411089104,
    "comments": [],
    "metadata": {
      "_edit_last": "1",
      "_spost_short_title": null
    }
  },
  "set-root-password-smartos-percona-mysql-zone": {
    "name": "set-root-password-smartos-percona-mysql-zone",
    "id": "4154",
    "link": "http://russell2.ballestrini.net/set-root-password-smartos-percona-mysql-zone/",
    "title": "Set Root Password SmartOS Percona MySQL Zone",
    "content": "<p>\nI used project-fifo to launch the <code>percona (14.2.0)</code> MySQL dataset.  I couldn't get into the MySQL instance so I reached out on IRC.  Johngrasty, a friendly guy in the <code>#smartos</code> IRC channel, provided a command to display the randomly generated MySQL password emitted to the zone-init log:\n</p>\n\n<pre>\ncat /var/svc/log/system-zoneinit\\:default.log | grep MYSQL_PW\n</pre>\n\n<p>\nI used this initial password to get into the <code>mysql></code> shell and changed it with this SQL statement:\n</p>\n\n<pre>\nmysql> SET PASSWORD = PASSWORD('clear-text-password');\n</pre>\n\n<p>\nJohngrasty also supplied a snippet of JSON which shows how to declare root MySQL password:\n</p>\n\n<pre>\n{\n  ... truncated ...\n  \"customer_metadata\": {\n    \"salt-master\": \"10.0.0.101\",\n    \"salt-id\": \"mysql1\"\n  },\n  \"internal_metadata\": {\n    \"mysql_pw\": \"mypassword\"\n  }\n}\n</pre>\n\n<p>\nI looked for help in the </code>#project-fifo</code> channel as to why the GUI does not work for assigning initial MySQL password. MerlinDMC, the author and operator of <a href=\"https://datasets.at\" title=\"datasets.at\">datasets.at</a>, gave me the following command to run in the Global Zone to look at a particular zone's metadata:\n</p>\n\n<pre>\ncat /zones/uuid-of-zone-goes-here/config/metadata.json\n</pre>\n\n<p>\nTurns out the GUI is placing the <code>\"mysql_pw\"</code> parameter into <code>\"customer_metadata\"</code> instead of <code>\"internal_metadata\"</code> which is invalid.\n</p>",
    "date": "2014-10-11 00:12:32",
    "timestamp": 1413000752,
    "comments": [
      {
        "id": 74219,
        "parent_id": 74135,
        "author_ip": "127.0.0.1",
        "author": "Russell Ballestrini",
        "email": "russell.ballestrini@gmail.com",
        "content": "dev-0b11be1, Wed Aug 27 21:27:57 2014 UTC",
        "date": "2014-10-11 19:33:27",
        "timestamp": 1413070407
      },
      {
        "id": 74135,
        "parent_id": 0,
        "author_ip": "127.0.0.1",
        "author": "Mark",
        "email": "nitronap@gmail.com",
        "content": "This was meant to be fixed in fifo by licenser, I brought this issue to his attention awhile back when Joyent moved all password stuff in internal metadata. Which version of fifo you running?",
        "date": "2014-10-11 15:04:27",
        "timestamp": 1413054267
      }
    ],
    "metadata": {
      "_edit_last": "1",
      "_spost_short_title": null,
      "_stcr@_nitronap@gmail.com": "2014-10-11 15:04:27|Y"
    }
  },
  "migrating-mongodb-from-ubuntu-to-smartos": {
    "name": "migrating-mongodb-from-ubuntu-to-smartos",
    "id": "4183",
    "link": "http://russell2.ballestrini.net/migrating-mongodb-from-ubuntu-to-smartos/",
    "title": "Migrating MongoDB from Ubuntu to SmartOS",
    "content": "I installed the mongodb 14.2.0 (<code>uuid a5775e36-2a02-11e4-942a-67ae7a242985</code>) dataset and launched a new zone.  The zone automatically creates a username and password for admin and \"quickbackup\".  You can find these passwords by running the following command inside the zone:\n\n<pre>\ncat /var/svc/log/system-zoneinit\\:default.log | grep -i mon\n</pre>\n\nFirst thing I did was disable authentication by modifying <code>/opt/local/etc/mongod.conf</code>:\n\n<pre>\n#auth = true\nnoauth = true\n</pre>\n\nThen I restarted MongoDB to re-read its configuration:\n\n<pre>svcadm restart mongodb</pre>\n\nNext I attempted to restore the database BSON files, with the following command:\n\n<pre>\nmongorestore --db=taco taco/taco.bson\n</pre>\n\nBut I got the following error:\n\n<pre>\nconnected to: 127.0.0.1\nterminate called after throwing an instance of 'std::runtime_error'\n  what():  locale::facet::_S_create_c_locale name not valid\nAbort (core dumped)\n</pre>\n\nAfter some research I learned that I needed to export the following variable before running the restore:\n\n<pre>\nexport LC_ALL=C\n</pre>",
    "date": "2014-10-11 20:52:17",
    "timestamp": 1413075137,
    "comments": [],
    "metadata": {
      "_wp_old_slug": "migrating-a-mongodb-from-ubuntu-to-smartos",
      "_edit_last": "1",
      "_spost_short_title": null
    }
  },
  "turn-python-dict-into-a-keyvalue-string": {
    "name": "turn-python-dict-into-a-keyvalue-string",
    "id": "4207",
    "link": "http://russell2.ballestrini.net/turn-python-dict-into-a-keyvalue-string/",
    "title": "Turn python dict into a key=value string and back again",
    "content": "<p>\nI'm currently refactoring a script that tags AWS resources and I came up with this one liner to generate pretty output.  It basically turns <code>{'tag1':'value1','tag2':'value2'}</code> into <code>tag1=value1, tag2=value2</code>.  Here is the code:\n</p>\n\n<p>\n<pre>\n', '.join(['='.join(key_value) for key_value in {'a':'1','b':'2'}.items() ])\n</pre>\n</p>\n\nOh and here is a function if you love this!\n\n<p>\n<pre>\ndef dict_to_key_value(data, sep='=', pair_sep=', '):\n    \"\"\"turns {'tag1':'value1','tag2':'value2'} into tag1=value1, tag2=value2\"\"\"\n    return pair_sep.join([sep.join(key_value) for key_value in data.items()])\n</pre>\n</p>\n\nCareful, this might blow up on dictionaries that nest other objects.  Also here is a test:\n\n<p>\n<pre>\ndef test_dict_to_key_value():\n    data = {'tag1':'value1','tag2':'value2'}\n    pretty_str = dict_to_key_value(data)\n    assert('tag1=value1' in pretty_str)\n    assert('tag2=value2' in pretty_str)\n    assert('tag1=value1, tag2=value2' in pretty_str)\n    not_as_pretty = dict_to_key_value(data,'x','x')\n    assert('tag1xvalue1xtag2xvalue2' in not_as_pretty)\n</pre>\n</p>\n\n<p>\nHere is the inverse, taking a list of key_value strings and returning a dictionary of the data:\n</p>\n\n<p>\n<pre>\ndef key_value_to_dict(key_value_list, sep='=', pair_sep=',' ):\n    \"\"\" \n    Accept a key_value_list, like::\n\n      key_value_list = ['a=1,b=2', 'c=3, d=4', 'e=5']\n\n    Return a dict, like::\n\n      {'a':'1', 'b':'2', 'c':'3', 'd':'4', 'e':'5'}\n    \"\"\"\n    d = {}\n    for speclist in key_value_list:\n        for spec in speclist.strip().split(','):\n            key, value = spec.strip().split('=')\n            d[key] = value\n    return d\n</pre>\n</p>\n\n<p>\nAnd of course a test to prove it works how we expect:\n</p>\n\n<p>\n<pre>\ndef test_key_value_to_dict():\n    key_value_list = ['a=1,b=2', 'c=3, d=4', 'e=5']\n    desired_result = {'a':'1', 'b':'2', 'c':'3', 'd':'4', 'e':'5'}\n    assert(key_value_to_dict(key_value_list) == desired_result)\n</pre>\n</p>",
    "date": "2014-11-09 22:02:23",
    "timestamp": 1415588543,
    "comments": [],
    "metadata": {
      "_edit_last": "1",
      "_spost_short_title": null
    }
  },
  "dealing-with-pagination-in-python": {
    "name": "dealing-with-pagination-in-python",
    "id": "4218",
    "link": "http://russell2.ballestrini.net/dealing-with-pagination-in-python/",
    "title": "Dealing with pagination in Python",
    "content": "So I'm working with an API (AWS ElastiCache) that offers mandatory pagination of results.  I need to get all results, so I took some time to work out this logic.\n\n<p>\n<pre>\ndef combine_results(function, key, marker=0, **kwargs):\n    \"\"\"deal with manditory pagination of AWS result descriptions\"\"\"\n    results = []\n    while marker != None:\n        result = function(marker = marker, **kwargs)\n        marker = nested_lookup('Marker', result)[0]\n        results += nested_lookup(key, result)\n    return results\n</pre>\n</p>\n\n<p>\nNot only is the AWS ElastiCache API paginated but it also appears deeply nested in lists and dicts.\nI use this to burn it with fire:\n</p>\n\n<pre>\n\ndef nested_lookup(key, dictionary):\n    \"\"\"Lookup a key in a nested dictionary, return a list of values\"\"\"\n    return list(_nested_lookup(key, dictionary))\n\ndef _nested_lookup(key, dictionary):\n    \"\"\" \n    Lookup a key in a nested dictionary, return value\n\n    Authors: Dougles Miranda and Russell Ballestrini\n    \"\"\"\n    if isinstance(dictionary, list):\n        for d in dictionary:\n            for result in _nested_lookup(key, d): \n                yield result\n\n    if isinstance(dictionary, dict):\n        for k, v in dictionary.iteritems():\n            if k == key:\n                yield v\n            elif isinstance(v, dict):\n                for result in _nested_lookup(key, v): \n                    yield result\n            elif isinstance(v, list):\n                for d in v:\n                    for result in _nested_lookup(key, d): \n                        yield result\n</pre>\n\n</pre>\n\n<p>The end result is we have access to paginated and deeply nested data with a simple to use function:</p>\n\n<pre>\n>>> from lib import combine_results, nested_lookup\n>>> d = elasticache_connection.describe_cache_clusters()\n>>> nested_lookup('CacheClusterId', d)\n[u'demo04-a-redis', u'demo04-b-redis', u'demo06-a-redis', u'demo06-b-redis', u'test-a-memcached', u'test-b-redis', u'ops01-redis', u'qa01-redis', u'ops02-redis', u'qa02-redis', u'int01-a-redis', u'int01-b-redis', u'ops03-redis', u'ops04-redis']\n</pre>\n\n<p>\nHere are some unit tests to prove these functions work like expected:\n</p>\n\n<pre>\nfrom unittest import TestCase\n\nfrom lib.util import (\n  combine_results,\n  nested_lookup,\n  _nested_lookup,\n)\n\ndef my_func_that_paginates(max_results=3, marker=0):\n    \"\"\"this function sort of mocks the paginated AWS description results\"\"\"\n    data = [\n      {'desired_key' : 0},\n      {'desired_key' : 1},\n      {'desired_key' : 2},\n      {'desired_key' : 3},\n      {'desired_key' : 4},\n      {'desired_key' : 5},\n      {'desired_key' : 6},\n      {'desired_key' : 7},\n      {'desired_key' : 8},\n      {'desired_key' : 9},\n    ]\n    new_marker = marker + max_results\n    if new_marker > len(data):\n        # last page!\n        page = data[marker:]\n        return {'results' : page, 'Marker' : None}\n    page = data[marker:new_marker]\n    return {'results' : page, 'Marker' : new_marker}\n\nclass TestCombineResults(TestCase):\n\n    def test_combine_results_returns_all_results(self):\n        expected_set = {0, 1, 2, 3, 4, 5, 6, 7, 8, 9}\n        f = my_func_that_paginates\n        result_set = set(combine_results(f, 'desired_key'))\n        self.assertSetEqual(expected_set, result_set)\n\nclass TestNestedLookup(TestCase):\n\n    def setUp(self):\n        self.subject_dict = {'a':1,'b':{'d':100},'c':{'d':200}}\n\n    def test_nested_lookup(self):\n        results = nested_lookup('d', self.subject_dict)\n        self.assertEqual(2, len(results))\n        self.assertIn(100, results)\n        self.assertIn(200, results)\n        self.assertSetEqual({100,200}, set(results))\n\n    def test_nested_lookup_wrapped_in_list(self):\n        results = nested_lookup('d', [{}, self.subject_dict, {}])\n        self.assertEqual(2, len(results))\n        self.assertIn(100, results)\n        self.assertIn(200, results)\n        self.assertSetEqual({100,200}, set(results))\n\n    def test_nested_lookup_wrapped_in_list_in_dict_in_list(self):\n        results = nested_lookup('d', [{}, {'H' : [self.subject_dict]} ])\n        self.assertEqual(2, len(results))\n        self.assertIn(100, results)\n        self.assertIn(200, results)\n        self.assertSetEqual({100,200}, set(results))\n\n    def test_nested_lookup_wrapped_in_list_in_list(self):\n        results = nested_lookup('d', [ {}, [self.subject_dict, {}] ])\n        self.assertEqual(2, len(results))\n        self.assertIn(100, results)\n        self.assertIn(200, results)\n        self.assertSetEqual({100,200}, set(results))\n</pre>\n\n<p>With this test, the steps of the algorithm looks like this:</p>\n\n<pre>\n{'Marker': 3, 'results': [{'desired_key': 0}, {'desired_key': 1}, {'desired_key': 2}]}\n3\n[0, 1, 2]\n[0, 1, 2]\n{'Marker': 6, 'results': [{'desired_key': 3}, {'desired_key': 4}, {'desired_key': 5}]}\n6\n[3, 4, 5]\n[0, 1, 2, 3, 4, 5]\n{'Marker': 9, 'results': [{'desired_key': 6}, {'desired_key': 7}, {'desired_key': 8}]}\n9\n[6, 7, 8]\n[0, 1, 2, 3, 4, 5, 6, 7, 8]\n{'Marker': None, 'results': [{'desired_key': 9}]}\nNone\n[9]\n[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\nok\n</pre>",
    "date": "2014-11-13 21:06:54",
    "timestamp": 1415930814,
    "comments": [],
    "metadata": {
      "_edit_last": "1",
      "_spost_short_title": null
    }
  },
  "build-release-pipelines-on-s3-with-s3p": {
    "name": "build-release-pipelines-on-s3-with-s3p",
    "id": "4225",
    "link": "http://russell2.ballestrini.net/build-release-pipelines-on-s3-with-s3p/",
    "title": "Build release pipelines on S3 with s3p",
    "content": "<p>\nThis weekend I finished my first sprint on s3p which is a Python library and CLI application that manages release pipelines on AWS S3.  I put a lot of effort into the <a href=\"https://github.com/russellballestrini/s3p/blob/master/readme.rst\" title=\"s3p readme.rst\" target=\"_blank\">readme.rst</a> file, so look there for usage and examples.\n</p>\n\n<p>The main purpose of s3p is to use code to enforce process when promoting releases in the pipeline.  Another goal was to make the CLI tool dead simple to use.  As a side effect, I ended up using composition to extend boto.s3's Key and Bucket classes to produce <a href=\"https://github.com/russellballestrini/s3p/blob/master/s3p/release.py\">S3Release</a> and <a href=\"https://github.com/russellballestrini/s3p/blob/master/s3p/pipeline.py\">S3Pipeline</a>.</p> \n\n<p>I plan to incorporate s3p into Teamcity build jobs.  At work I would like to use s3p to replace bash+s3cmd in pipeline management.  Once s3p has a bit more production use, I will likely sprint on it again.</p>",
    "date": "2014-11-24 14:39:44",
    "timestamp": 1416857984,
    "comments": [],
    "metadata": {
      "_series_part": "4",
      "_edit_last": "1",
      "_spost_short_title": null,
      "_wp_old_slug": "build-release-pipelines-on-aws-s3-with-s3p"
    }
  },
  "custom-rundeck-hipchat-notification-templates": {
    "name": "custom-rundeck-hipchat-notification-templates",
    "id": "4242",
    "link": "http://russell2.ballestrini.net/custom-rundeck-hipchat-notification-templates/",
    "title": "Custom Rundeck HipChat notification templates",
    "content": "<p>\nToday I built a GUI and workflow around Ansible using Rundeck.  Tonight I started diving into sending HipChat notifications and after a bit of research, I managed to create a custom notification template for each Rundeck project.\n</p>\n\n<p>\nModify your project's configuration file, on Ubuntu it was in <code>/var/rundeck/projects/pname/etc/project.properties</code>, and add the following line to the bottom:\n</p>\n\n<p>\n<blockquote>\n<pre>framework.plugin.Notification.HipChatNotification.messageTemplateLocation=/var/rundeck/projects/pname/etc/custom-hipchat-template.ftl</pre>\nNote: replace <code>pname</code> with your project name.</blockquote>\n</p>\n\n<p>\nI ended up producing a single line chat notification template.  I uploaded it here to act as an example: <a href=\"http://pad.yohdah.com/311/rundeck-hipchat-custom-notification-template\" title=\"Custom Rundeck HipChat Notification Template\" target=\"_blank\">Custom Rundeck HipChat Notification Template\n</a></p>\n\n<p>\nThe template syntax and renderer is called FreeMarker. Rundeck and the HipChat plugin pass many different context Map hash objects to FreeMarker for use in the templates.  In this case I display \"VPC name\" selected by the user when starting the job.\n</p>\n\n<p>\nReferences:\n<ul>\n\t<li><a href=\"http://rundeck.org/plugins/2013/06/24/hipchat-notification.html\" title=\"Rundeck HipChat Notification Plugin\">Rundeck HipChat Notification Plugin</a></li>\n\t<li><a href=\"https://github.com/hbakkum/rundeck-hipchat-plugin\" title=\"Rundeck HipChat Notification Plugin Repo\">Rundeck HipChat Notification Plugin</a> (User Guide and Default Template)</li>\n</p>",
    "date": "2014-12-03 01:37:51",
    "timestamp": 1417588671,
    "comments": [],
    "metadata": {
      "_edit_last": "1",
      "_spost_short_title": null
    }
  },
  "autofs-net-automount-stopped-working": {
    "name": "autofs-net-automount-stopped-working",
    "id": "4268",
    "link": "http://russell2.ballestrini.net/autofs-net-automount-stopped-working/",
    "title": "autofs /net automount stopped working",
    "content": "<p>\nSo autofs randomly stopped working on one of my Ubuntu hosts (this issue has been found on Arch as well so its most likely a change upstream).  I found this error in the logs:\n</p>\n\n<p>\n<pre>\nattempting to mount entry /net/freenas.example.net\nget_exports: lookup(hosts): exports lookup failed for freenas.example.net\nkey \"freenas.example.net\" not found in map source(s).\nfailed to mount /net/freenas.example.net\n</pre>\n</p>\n\n<p>\nThe fix was to replace the following line in /etc/auto.master from\n</p>\n\n<p>\n<pre>\n/net   -hosts\n</pre>\n</p>\n\n<p>\nto this\n</p>\n\n<p>\n<pre>\n/net /etc/auto.net --timeout=60\n</pre>\n</p>\n\n<p>\nand then restart autofs (sudo service autofs restart).\n</p>",
    "date": "2014-12-21 22:38:18",
    "timestamp": 1419219498,
    "comments": [
      {
        "id": 178193,
        "parent_id": 0,
        "author_ip": "127.0.0.1",
        "author": "Joe HIggins",
        "email": "higgins@netfog.com",
        "content": "Thanks, this saved me on Raspbian GNU/Linux 8.0 (jessie).",
        "date": "2016-01-17 05:19:45",
        "timestamp": 1453025985
      },
      {
        "id": 157840,
        "parent_id": 0,
        "author_ip": "127.0.0.1",
        "author": "rduke15",
        "email": "rduke15@gmail.com",
        "content": "Thanks. Same problem on Debian Jessie (8.1). The bug is said to be fixed (https://bugs.debian.org/cgi-bin/bugreport.cgi?bug=743939), but not in Debian stable. Your workaround is fine.",
        "date": "2015-07-08 09:46:38",
        "timestamp": 1436363198
      }
    ],
    "metadata": {
      "_stcr@_higgins@netfog.com": "2016-01-17 05:19:45|Y",
      "_edit_last": "1",
      "_spost_short_title": null,
      "_stcr@_rduke15@gmail.com": "2015-07-08 09:46:38|Y"
    }
  },
  "risk-process-and-balance": {
    "name": "risk-process-and-balance",
    "id": "4274",
    "link": "http://russell2.ballestrini.net/risk-process-and-balance/",
    "title": "Risk, Process, and Balance",
    "content": "<p>The operations of a company will have intrinsic risk.  Risk occurs each time we decide to take an action or an inaction.  This means that anything we choose to do, or not do, has associated risk.\n</p>\n\n<p>\nAn organization which has an unhealthy aversion to risk has a much higher chance of failure.  As time goes on the intolerance of taking on additional risk to accomplish goals, will render a company petrified.  Stuck in the fear of change.\n</p>\n\n<p>\nIn order to become unstuck, a company and it's employees must become great at calculating risk and assessment.  In order to make a choice on what to work on, and what to not work on, they will need to research and collect data and continuously track progress and feedback.  The company must use this data and feedback to move uncalculated bad risk into calculated good risk.  Simply thinking through the problem and change before hand can uncover ways to accomplish the end result in the least risky way.\n</p>\n\n<p>\nThe biggest objection to the risk assessment phase is typically time constraints.  A great company will learn how to rapidly assess and reduce the risk of action.  They reduce risk by: gathering requirements, planning, building process, using process to drive automation, and iterating on automation to speed up feedback loops.  These fast feedback loops will allow the company and it's employees to fail smaller and more frequently and facilitate reflection to optimize the pipeline.  This means even more calculated and low risk actions!\n</p>\n\n<p>\n<blockquote>\nSo, a company should add more process to reduce risk? Not quite.\n</blockquote>\n</p>\n\n<p>\nA process, when left unchecked or added for the wrong reasons, will become more detrimental then blindly performing an action without calculating the risk.  Below I outline the key differences between a good process and a bad process.\n</p>\n\n<p>\nA good process will:\n</p>\n\n<p>\n<ul>\n\t<li>reduce risk but not at the expense of blocking or preventing progress</li>\n\t<li>be fast to facilitate quick feedback loops</li>\n\t<li>have potential for automation and must <u>not</u> be tedious or manual (think approvals)</li>\n\t<li>promote small and frequent changes</li>\n\t<li>support both fail forward and fail backward</li>\n\t<li>only fail backward to reduce time-to-recover</li>\n\t<li><u>not</u> be pushed down from upper management, rather upper management should help set goals and requirements, while the team builds and decides on process</li>\n\t<li>be questioned and reviewed often to promote iteration to find and fix inefficiencies</li>\n</ul>\n</p>\n\n<p>\nA bad process will:</p>\n\n<p>\n<ul>\n\t<li>block real work from getting done</li>\n\t<li>prevent creative and innovative solutions</li>\n\t<li>have a higher chance of being ignored, skipped, or held in contempt</li>\n\t<li>punish people who take risks (make change) and promote people who do nothing</li>\n\t<li>cause more issues then it solves</li>\n\t<li>become a crutch or a security blanket (a false sense of security)</li>\n</ul>\n</p>\n\nI feel strongly that the essence behind the Agile and DevOps movements comes from the constant battle of balancing risk with process.  This constant desire for balance in the organization's environment will lead to innovative ideas, tools, and workflows.  These ideas, tools and workflows can not function properly if transplanted into a company who does not have the desire to find the optimal balance.  Put simply, a company cannot buy Agile or DevOps, that culture needs to grow from with in.",
    "date": "2015-01-10 22:35:47",
    "timestamp": 1420947347,
    "comments": [],
    "metadata": {
      "_edit_last": "1",
      "_spost_short_title": null,
      "_wp_old_slug": "risk-and-process"
    }
  },
  "set-postgres-user-password-on-postgresql-smartos-zone": {
    "name": "set-postgres-user-password-on-postgresql-smartos-zone",
    "id": "4325",
    "link": "http://russell2.ballestrini.net/set-postgres-user-password-on-postgresql-smartos-zone/",
    "title": "Set postgres user password on PostgreSQL SmartOS Zone",
    "content": "<p>\nConnect to zone and determine the auto generated password for postgres user:\n</p>\n\n<p>\n<pre>\ncat /var/svc/log/system-zoneinit\\:default.log | grep PGSQL_PW\n</pre>\n</p>\n\n<p>\ndocument the result and log into postgres with the following command, entering the password when prompted:\n</p>\n\n<p>\n<pre>\n[root@psql ~]# psql --user postgres\n</pre>\n</p>\n\n<p>\nAlter the postgres role's password:\n</p>\n\n<p>\n<pre>\npostgres=# ALTER ROLE postgres UNENCRYPTED PASSWORD 'new-password';\n</pre>\n</p>\n\n<p>\nNow exit (<code>\\q</code>) then try to log in with the new password.\n</p>\n\n<p>\nIn my case I was setting up PostgreSQL for testing out Zabbix monitoring system.  So I did the following as the postgres user:\n</p>\n\n<p>\n<pre>\npostgres=# CREATE USER zab UNENCRYPTED PASSWORD 'zabby'\npostgres=# CREATE DATABASE zab OWNER zab;\n</pre>\n</p>",
    "date": "2015-01-21 22:28:16",
    "timestamp": 1421897296,
    "comments": [],
    "metadata": {
      "_edit_last": "1",
      "_spost_short_title": null
    }
  },
  "securely-publish-jenkins-build-artifacts-on-salt-master": {
    "name": "securely-publish-jenkins-build-artifacts-on-salt-master",
    "id": "4331",
    "link": "http://russell2.ballestrini.net/securely-publish-jenkins-build-artifacts-on-salt-master/",
    "title": "Securely publish Jenkins build artifacts on Salt Master",
    "content": "<p>\nDo you want a secure setup for publishing and staging build artifacts from a Jenkins build server to a Salt Master?  This guide describes my fully automated pipeline to transport binaries using Salt's encrypted \"bus\".\n</p>\n\n<p>\nWe start off with some Salt States to stand up a Jenkins build server \"client\":\n</p>\n\n<p>\n<b>jenkins/client.sls:</b>\n<pre>\n# http://russell.ballestrini.net/securely-publish-jenkins-build-artifacts-on-salt-master/\n# manage jenkins user, home dir, and Jenkins \"master\" public SSH key.\njenkins:\n  user.present:\n    - fullname: jenkins butler\n    - shell: /bin/bash\n    - home: /home/jenkins\n\n  file.directory:\n    - name: /home/jenkins\n    - user: jenkins\n    - group: jenkins\n    - require:\n      - user: jenkins\n\n  ssh_auth.present:\n    - user: jenkins\n    - name: {{ pillar.get('jenkins-public-key') }}\n    - require:\n      - user: jenkins\n\n# Manage a script to push artifacts to Salt Master.\n# Note: jenkins user should _not_ have ability to change this file.\n/home/jenkins/salt-call-put-artifacts-onto-salt-master.sh:\n  file.managed:\n    - user: root\n    - group: root\n    - mode: 755\n    - contents: |\n        echo salt-call cp.push_dir \"$PWD\" glob='*.tar.gz'\n        salt-call cp.push_dir \"$PWD\" glob='*.tar.gz'\n        echo salt-call cp.push \"$PWD/commit-hash.txt\"\n        salt-call cp.push \"$PWD/commit-hash.txt\"\n    - require:\n      - file: jenkins\n\n# Allow jenkins to run push script as root via sudo.\njenkins-sudoers:\n  file.append:\n    - name: /etc/sudoers\n    - text:\n      - \"jenkins    ALL = NOPASSWD: /home/jenkins/salt-call-put-artifacts-onto-salt-master.sh\"\n</pre>\n</p>\n\n<p>\nOn the Salt Master we must enable MinionFS and restart Salt Master process:\n<pre>\nfileserver_backend:\n  - roots\n  - minion\n\nfile_recv: True\n</pre>\n</p>\n\n<p>We then use this command as the last build task in every Jenkins build job:\n<pre>sudo /home/jenkins/salt-call-put-artifacts-onto-salt-master.sh</pre>\nThis causes the file to be staged on the Salt Master on a successful build.\n</p>\n\n<p>\nThe Salt States to deploy the software to production, look something like this:\n<pre>\n# extract tarball from Salt Master using MinionFS.\nextract-tarball-for-mysite:\n  archive.extracted:\n    - name: /www/mysite\n    - archive_format: tar\n    - source: salt://ubuntu-jenkins.foxhop.net/home/jenkins/workspace/job-name/env.tar.gz\n    - user: uwsgi\n    - group: uwsgi\n    # I want to always extract, not sure a better way.\n    - if_missing: /dev/taco\n    - require:\n      - service: make-mysite-dead-for-release\n</pre>\n</p>\n\n<p>I also have build triggers which monitor remote git/hg repos for changes.  Pushing code triggers a build which tests my code base and securely publishes to my Salt Master.  When the time comes to perform a release, all I have to do is run highstate, because the pipeline did all the other work for me!</p>",
    "date": "2015-02-22 12:39:27",
    "timestamp": 1424626767,
    "comments": [],
    "metadata": {
      "_edit_last": "1",
      "_spost_short_title": null
    }
  },
  "migrating-libvirt-kvm-guest-to-smartos-kvm-guest": {
    "name": "migrating-libvirt-kvm-guest-to-smartos-kvm-guest",
    "id": "4360",
    "link": "http://russell2.ballestrini.net/migrating-libvirt-kvm-guest-to-smartos-kvm-guest/",
    "title": "Migrating libvirt KVM guest to SmartOS KVM guest",
    "content": "<p>\nThe following tutorial documents how to migrate a libvirt/KVM guest from Ubuntu to SmartOS.  \n</p>\n\n<p>\nakuma:\n  Ubuntu Hypervisor\n</p>\n\n<p>\nguy:\n  SmartOS Hypervisor\n</p>\n\n<p>\nsagat:\n  guest to migrate\n</p>\n\n<p>\nThese commands were run on akuma:\n</p>\n\n<pre>\nWORKDIR=/KVMROOT/migrate\nsudo mkdir $WORKDIR\ncd $WORKDIR\n\n# this tool simply stops and tarballs up the qcow and xml for libvirt KVM guest.\nsudo /usr/local/bin/virt-back --path=$WORKDIR --backup sagat\n\nsudo tar -xzvf sagat.tar.gz\n\nsudo qemu-img convert -O raw sagat.qcow2 sagat.raw\nsudo qemu-img convert -O raw sagat-var.qcow2 sagat-var.raw\n\nscp sagat*.raw root@guy:/opt/tmp\n</pre>\n\n<p>These commands were run on guy:</p>\n\nCreate the following file on guy - setup-ubuntu-sagat.json:\n\n<pre>\n{\n  \"brand\": \"kvm\",\n  \"resolvers\": [\n    \"192.168.1.22\",\n    \"8.8.4.4\"\n  ],\n  \"ram\": \"4096\",\n  \"vcpus\": \"4\",\n  \"alias\": \"sagat\",\n  \"hostname\": \"sagat\",\n  \"nics\": [\n    {\n      \"nic_tag\": \"admin\",\n      \"ip\": \"192.168.1.50\",\n      \"netmask\": \"255.255.255.0\",\n      \"gateway\": \"192.168.1.254\",\n      \"model\": \"virtio\",\n      \"primary\": true\n    }\n  ],\n  \"disks\": [\n    {\n      \"boot\": true,\n      \"model\": \"virtio\",\n      \"size\": 10240\n    },\n    {\n      \"model\": \"virtio\",\n      \"size\": 20480\n    }\n  ]\n}\n</pre>\n\nWe then create a new KVM guest on guy:\n\n<pre>\n[root@guy /opt/setup-jsons]# vmadm create -f setup-ubuntu-sagat.json \nSuccessfully created VM aa0f603c-9572-4cb0-b96f-4c79eb431223\n</pre>\n\nList out the virtual block devices on the new KVM guest: \n\n<pre>\n[root@guy /opt/setup-jsons]# vmadm info aa0f603c-9572-4cb0-b96f-4c79eb431223 block\n{\n  \"block\": [\n    {\n      \"device\": \"virtio0\",\n      \"locked\": false,\n      \"removable\": false,\n      \"inserted\": {\n        \"ro\": false,\n        \"drv\": \"raw\",\n        \"encrypted\": false,\n        \"file\": \"/dev/zvol/rdsk/zones/aa0f603c-9572-4cb0-b96f-4c79eb431223-disk0\"\n      },\n      \"type\": \"hd\"\n    },\n    {\n      \"device\": \"virtio1\",\n      \"locked\": false,\n      \"removable\": false,\n      \"inserted\": {\n        \"ro\": false,\n        \"drv\": \"raw\",\n        \"encrypted\": false,\n        \"file\": \"/dev/zvol/rdsk/zones/aa0f603c-9572-4cb0-b96f-4c79eb431223-disk1\"\n      },\n      \"type\": \"hd\"\n    }\n  ]\n}\n</pre>\n\n<p>\nStop the new guest, it needs to be off for the restore.\n<pre>\n[root@guy /opt/setup-jsons]# vmadm stop aa0f603c-9572-4cb0-b96f-4c79eb431223\nSuccessfully completed stop for VM aa0f603c-9572-4cb0-b96f-4c79eb431223\n</pre>\n</p>\n\n<p>\nUse dd to:\n\n<ul>\n\t<li>write sagat.raw to disk0</li>\n\t<li>write sagat-var.raw to disk1</li>\n</ul>\n</p>\n\n<pre>\ndd if=sagat.raw of=/dev/zvol/rdsk/zones/aa0f603c-9572-4cb0-b96f-4c79eb431223-disk0 bs=1M\ndd if=sagat-var.raw of=/dev/zvol/rdsk/zones/aa0f603c-9572-4cb0-b96f-4c79eb431223-disk1 bs=1M\n</pre>\n\nLastly start the new guest up, and check it out:\n\n<pre>\nvmadm start aa0f603c-9572-4cb0-b96f-4c79eb431223\n</pre>",
    "date": "2015-04-28 22:17:20",
    "timestamp": 1430273840,
    "comments": [],
    "metadata": {
      "_edit_last": "1",
      "_spost_short_title": null
    }
  },
  "setting-region-programmatically-in-boto3": {
    "name": "setting-region-programmatically-in-boto3",
    "id": "4363",
    "link": "http://russell2.ballestrini.net/setting-region-programmatically-in-boto3/",
    "title": "Setting region programmatically in Boto3",
    "content": "<p>\nAt work I'm looking into the possibility of porting parts of our AWS automation codebase from Boto2 to Boto3. We desire to perform this port because Boto2's record and result pagination appears defective.\n</p>\n\n<p>\nI started to familiarize myself with Boto3 by using the Interactive Python interpreter.\nHere I show myself trying to connect to the RDS AWS endpoint following the docs:\n</p>\n\n<p>\n<pre>\n>>> import boto3\n>>> rds = boto3.client('rds')\nTraceback (most recent call last):\n...\nNoRegionError: You must specify a region.\n</pre>\n</p>\n\n<p>\nWhat? \nNo region, ok how do I set a region?\nWell it turns out the docs want you to configure a region in a config file.\nThis will not work for me, I need to set the region programatically...\n</p>\n\n<p>\nSo after stumbling around in the botocore source code I found the following 2 solutions.\n</p>\n\n<p>\n<strong>Solution 1 - Set region_name when creating client:</strong>\n</p>\n\n<p>\n<pre>\n>>> import boto3\n>>> rds = boto3.client('rds', region_name='us-west-2')\n</pre>\n</p>\n\n<p>\n<strong>Solution 2 - Set default region_name on the session:</strong>\n</p>\n\n<p>\n<pre>\n>>> import boto3\n>>> rds = boto3.setup_default_session(region_name='us-west-2')\n>>> rds = boto3.client('rds')\n</pre>\n</p>\n\n<p>\nIt seems Boto3 has two types of interfaces, clients and resources.\n</p>\n\n<p>\nClients return description objects and appear lower level.\nDescription objects seem like AWS XML responses transformed into Python Dicts/Lists.\n</p>\n\n<p>\nResources return higher level Python objects and like Instances with stop/start methods. \n</p>\n\n<p>\nAt a quick glance, both clients and resources seem to properly implement pagination automatically!\n</p>\n\n<p>\n<pre>\n>>> # client interface.\n>>> ec2 = boto3.client('ec2', region_name='us-west-2')\n>>> idesc = ec2.describe_instances()\n>>> len(idesc['Reservations'])\n273\n>>> idesc['ResponseMetadata']\n{'HTTPStatusCode': 200, 'RequestId': '7e431ff7-xxxx-xxxx-xxxx-xxxxxxxxxxxxx'}\n\n>>> # resource interface.\n>>> ec2 = boto3.resource('ec2', region_name='us-west-2')\n>>> for instance in ec2.instances.all():\n>>>     print instance, instance.tags\n</pre>\n</p>\n\n<p>\nThat all for now, I hope this helps!\n</p>",
    "date": "2015-04-24 14:07:53",
    "timestamp": 1429898873,
    "comments": [
      {
        "id": 151525,
        "parent_id": 0,
        "author_ip": "127.0.0.1",
        "author": "Kevin",
        "email": "kevin.wessel@melbourneit.com.au",
        "content": "Nice work Russell!\n\nI was looking at setting the region within the filters for the describe_volume class. \n\nTurns out we have to specify it when establishing the client :)",
        "date": "2015-06-03 22:13:38",
        "timestamp": 1433384018
      }
    ],
    "metadata": {
      "_series_part": "3",
      "_edit_last": "1",
      "_spost_short_title": null,
      "_stcr@_kevin.wessel@melbourneit.com.au": "2015-06-03 22:13:38|Y"
    }
  },
  "a-python-script-which-searches-for-available-interpreters": {
    "name": "a-python-script-which-searches-for-available-interpreters",
    "id": "4398",
    "link": "http://russell2.ballestrini.net/a-python-script-which-searches-for-available-interpreters/",
    "title": "A Python script which searches for available interpreters",
    "content": "<p>\nThis post describes how to write a polyglot -- in this case a script which runs as valid Bash or Python, to search for available Python interpreters.  \n\nThe script initially runs as Bash but upon finding a first match, the script will call itself again this time using the expected Python interpreter in interactive mode!\n</p>\n\n<p>\nAnd now, for the polyglot code:\n</p>\n\n<pre>\n#!/bin/sh\n\n# reinvoke this script with bpython -i, ipython -i, or python -i\n# reference: http://unix.stackexchange.com/a/66242\n''':'\nif type bpython >/dev/null 2>/dev/null; then\n  exec bpython -i \"$0\" \"$@\"\nelif type ipython >/dev/null 2>/dev/null; then\n  exec ipython -i \"$0\" \"$@\"\nelse\n  exec python -i \"$0\" \"$@\"\nfi\n'''\nfrom helpers import (\n  base_parser,\n  get_hvpc_from_args,\n)\nparser = base_parser('Interactive Python interpreter & connection to hvpc')\nargs = parser.parse_args()\nhvpc = get_hvpc_from_args(args)\nprint('\\nYou now have access to the hvpc object, for example: hvpc.roles\\n')\n</pre>\n\n<p>\nIn this case you can see that we setup the interactive interpreter's environment to create an hvpc (Husky VPC) object for exploration.\n</p>\n\n<p>If you want a pure python version that doesn't use a bash/python polygot, checkout this code I wrote: https://github.com/russellballestrini/botoform/blob/master/botoform/plugins/repl.py</p>",
    "date": "2015-05-08 10:34:29",
    "timestamp": 1431095669,
    "comments": [],
    "metadata": {
      "_series_part": "5",
      "_edit_last": "1",
      "_spost_short_title": null
    }
  },
  "smartos-ubuntu-guest-apt-get-not-working-because-ipv6": {
    "name": "smartos-ubuntu-guest-apt-get-not-working-because-ipv6",
    "id": "4405",
    "link": "http://russell2.ballestrini.net/smartos-ubuntu-guest-apt-get-not-working-because-ipv6/",
    "title": "SmartOS Ubuntu guest, apt-get not working because IPv6",
    "content": "<p>\nTurns out I don't have IPv6 setup properly in my network so when apt attempts to connect to the Internet it tries IPv6 and fails.\n</p>\n\n<p>\nTo disable IPv6 on the ubuntu guest, add this to end of /etc/sysctl.conf and restart the guest:\n</p>\n\n<p>\nsudo vim /etc/sysctl.conf:\n</p>\n\n<p>\n<pre>\nnet.ipv6.conf.all.disable_ipv6 = 1\nnet.ipv6.conf.default.disable_ipv6 = 1\nnet.ipv6.conf.lo.disable_ipv6 = 1\n</pre>\n</p>\n\n<p>\nThis was a hacky work around mostly because although I desire to have IPv6 working, I desire to get this VM running more...\n</p>\n\n<p>\nThanks for reading, leave comments please.\n</p>",
    "date": "2015-05-09 20:29:29",
    "timestamp": 1431217769,
    "comments": [],
    "metadata": {
      "_edit_last": "1",
      "_spost_short_title": null
    }
  },
  "change-default-gateway-on-all-smartos-zones-and-kvm-guests": {
    "name": "change-default-gateway-on-all-smartos-zones-and-kvm-guests",
    "id": "4416",
    "link": "http://russell2.ballestrini.net/change-default-gateway-on-all-smartos-zones-and-kvm-guests/",
    "title": "Change default gateway on all SmartOS Zones and KVM guests",
    "content": "<p>\nToday I needed to change the default gateway on every Zone and KVM guest on my SmartOS hypervisor because I switched my ISP and as a result my gateway changed from 192.168.1.254 to 192.168.1.1. After changing one guest I got lazy and put together this script.\n</p>\n\n<p>\n<strong>update-all-gateways.sh</strong>\n</p>\n\n<p>\n<pre>\nfor VM in `vmadm list -p -o alias,uuid`\n\n  do\n    # create an array called VM_PARTS splitting on ':'\n    IFS=':' VM_PARTS=($VM)\n\n    # create some helper varibles for alias and uuid\n    alias=${VM_PARTS[0]}\n    uuid=${VM_PARTS[1]}\n\n    mac=`vmadm get $uuid | json nics | json -a mac`\n\n    echo \"\n{\n   \\\"update_nics\\\": [\n      {\n        \\\"mac\\\": \\\"$mac\\\",\n        \\\"gateway\\\": \\\"192.168.1.1\\\"\n      }\n   ]\n}\n\" | vmadm update $uuid\ndone\n\n</pre>\n</p>\n\n<pre>\n[root@hypervisor /opt/setup-jsons/updates]# bash update-all-gateways.sh \nSuccessfully updated VM 211b992b-a448-40b4-94c9-xxxxxxxxxxxx\nSuccessfully updated VM 31baa6a5-aa98-4750-80df-xxxxxxxxxxxx\nSuccessfully updated VM 65d176b4-c36d-4cbf-b6ed-xxxxxxxxxxxx\nSuccessfully updated VM aa0f603c-9572-4cb0-b96f-xxxxxxxxxxxx\nSuccessfully updated VM ad928301-f3e1-4fe8-a1c1-xxxxxxxxxxxx\nSuccessfully updated VM b82a257e-5628-46db-aee4-xxxxxxxxxxxx\nSuccessfully updated VM da72b638-51de-4d7d-9853-xxxxxxxxxxxx\nSuccessfully updated VM ee42bf30-51ce-4ae2-915b-xxxxxxxxxxxx\n</pre>\n\n<p>\nYou are welcome!\n</p>\n\n<p>\nAlso to change the global zone (head node) default gateway, edit /usbkey/config and then run the following commands:\n</p>\n\n<pre>\n[root@hypervisor /]# route delete default 192.168.1.254\ndelete net default: gateway 192.168.1.254\n\n[root@hypervisor /]# route add default 192.168.1.1\nadd net default: gateway 192.168.1.1\n\n[root@hypervisor /]# netstat -r\n</pre>",
    "date": "2015-06-01 20:34:21",
    "timestamp": 1433205261,
    "comments": [],
    "metadata": {
      "_edit_last": "1",
      "_spost_short_title": null,
      "_wp_old_slug": "change-default-gateway-all-smartos-zones-and-kvm-guests"
    }
  },
  "my-mentor": {
    "name": "my-mentor",
    "id": "4426",
    "link": "http://russell2.ballestrini.net/my-mentor/",
    "title": "My Mentor",
    "content": "<p>\nThe original purpose of this post was to recognize the teachers and mentors which have helped shape me. I planned to write about how Mr. Cassidy, my high school drafting teacher, taught me the importance precision. I prepared to explain how Mr. Mercuri, my college computer science professor, ignited my desire to engineer software. But I could not. I knew deep down that one person in particular affected me more then the rest.\n</p>\n\n<p>\nThis person could fix anything and could even talk through a broken heart. He didn't know the internals of computers but the hacker spirit sweats from his veins. He taught me how to problem solve, many times in unconventional ways. I once watched him clear brush with an old snow plow and cut our work day in half. If he didn't have access to a tool, he would furnish one out of raw materials and spare parts, like a real life MacGyver. With a little bit of thought and ingenuity he built anything he imagined.\n</p>\n\n<p>\nSometimes he would use tough love to prove a point. For example, when I had a sun burn in middle school, he taught me to push through pain and not let my team down even though my jersey was scratching my raw skin. He showed me how to work hard and how to have great work ethic. He taught me the importance of \"saving for a rainy day\" but also knew how to have fun and loved parties. He taught me self control and how to make sacrifices for payouts in the future.\n</p>\n\n<p>\nHe taught me the fundamentals of kindness and guided me on the righteous path. He gave great advice, and even though I didn't always listen, he respected my thoughts and wishes. If he ever caused an accident he would always admit fault. He emits honor, honesty, and courage and never lies to or manipulates the people around him. Although we have had our differences, I think about you each day.\n</p>\n\n<p>\nI love you. Thank you and happy father's day, dad. \n</p>",
    "date": "2015-06-21 13:36:19",
    "timestamp": 1434908179,
    "comments": [],
    "metadata": {
      "_edit_last": "1",
      "_spost_short_title": null
    }
  },
  "working-with-botocores-awsconfig": {
    "name": "working-with-botocores-awsconfig",
    "id": "4453",
    "link": "http://russell2.ballestrini.net/working-with-botocores-awsconfig/",
    "title": "Working with botocore's ~/.aws/config",
    "content": "<p>\nI ran into a <a href=\"https://github.com/boto/botocore/issues/435\">bug</a> in botocore and this post will serve to document a work around as well as show how to use botocore session object to work with the values stored in ~/.aws/config. \n</p>\n\n<p>\nPretend you have an aws config with two accounts for two separate projects, like so:\n</p>\n\n<p>\n*~/.aws/config:*\n\n<pre>\n[profile project1]\naccount_id = 111111111111\naws_access_key_id=THISISNOTMYACCESSKEY1\naws_secret_access_key=THISISNOTMYSECRETKEY1\n# Optional, to define default region for this profile.\nregion=us-west-1\n\n[profile project2]\naccount_id = 222222222222\naws_access_key_id=THISISNOTMYACCESSKEY2\naws_secret_access_key=THISISNOTMYSECRETKEY2\n# Optional, to define default region for this profile.\nregion=us-west-2\n</pre>\n</p>\n\n<p>\nNow instead of using a single object, we create multiple objects, one for each profile we intend to use.\n</p>\n\n<p>\n<pre>\n>>> import botocore.session\n>>> session1 = botocore.session.get_session()\n>>> session2 = botocore.session.get_session()\n>>> session1.profile = 'project1'\n>>> session2.profile = 'project2'\n>>> session1.get_credentials().access_key\n'THISISNOTMYACCESSKEY1'\n>>> session2.get_credentials().access_key\n'THISISNOTMYACCESSKEY2'\n</pre>\n</p>\n\n<p>\nAlso figured out how to get at the `account_id` integer:\n</p>\n\n<p>\n<pre>\n>>> session1.get_scoped_config()['account_id']\n'111111111111'\n</pre>\n</p>\n\n<p>\nHere is another algorithm that returns a list of sessions objects, one for each profile listed in the config.\n</p>\n\n<p><pre>\n>>> import botocore.session\n>>> sessions = []\n>>> aws_config = botocore.session.get_session().full_config\n>>> for profile_name in aws_config['profiles']:\n...     session = botocore.session.get_session()\n...     session.profile = profile_name\n...     sessions.append(session)\n</pre></p>",
    "date": "2015-07-01 18:14:02",
    "timestamp": 1435788842,
    "comments": [
      {
        "id": 183506,
        "parent_id": 0,
        "author_ip": "127.0.0.1",
        "author": "Udhay Prakash Pethakamsetty",
        "email": "uday3prakash@gmail.com",
        "content": "session1.get_scoped_config()['account_id']  didn't work for me. For session1.get_scoped_config(), i got \n{'aws_access_key_id': 'THISISNOTMYACCESSKEY2',\n 'aws_secret_access_key': 'THISISNOTMYSECRETKEY2',\n 'region': 'us-west-2'}\nlike this. There is not 'account_id' here. Correct me, if i am wrong. \n\n2. Also, could you let me know whether we can achieve the above task with boto3 module too",
        "date": "2016-05-30 11:01:03",
        "timestamp": 1464620463
      }
    ],
    "metadata": {
      "_stcr@_uday3prakash@gmail.com": "2016-05-30 11:01:03|Y",
      "_series_part": "2",
      "_edit_last": "1",
      "_spost_short_title": null
    }
  },
  "filtering-aws-resources-with-boto3": {
    "name": "filtering-aws-resources-with-boto3",
    "id": "4460",
    "link": "http://russell2.ballestrini.net/filtering-aws-resources-with-boto3/",
    "title": "Filtering AWS resources with Boto3",
    "content": "<p>\nThis post will be updated frequently when as I learn more about how to filter AWS resources using Boto3 library.\n</p>\n\n<p>\n<strong>Filtering VPCs by tags</strong>\n</p>\n\n<p>\nIn this example we want to filter a particular VPC by the \"Name\" tag with the value of 'webapp01'.\n</p>\n\n<pre>\n>>> import boto3\n>>> boto3.setup_default_session(profile_name='project1')\n>>> ec2 = boto3.resource('ec2', region_name='us-west-2')\n>>> filters = [{'Name':'tag:Name', 'Values':['webapp01']}]\n>>> webapp01 = list(ec2.vpcs.filter(Filters=filters))[0]\n>>> webapp01.vpc_id\n'vpc-11111111'\n</pre>\n\nYou can also filter on the value of the 'tag-key' or the 'tag-value' like so:\n\n<pre>\n>>> taco_key_filter = [{'Name':'tag-key', 'Values':['taco']}]\n>>> nacho_value_filter = [{'Name':'tag-value', 'Values':['nacho']}]\n</pre>\n\nYou can also filter on multiple 'Values'. In this example want 2 VPCs named 'webapp01' and 'webapp02':\n\n<pre>\n>>> filters = [{'Name':'tag:Name', 'Values':['webapp01','webapp02']}]\n>>> list(ec2.vpcs.filter(Filters=filters))\n[ec2.Vpc(id='vpc-11111111'), ec2.Vpc(id='vpc-22222222')]\n</pre>\n\nYou can also use the '*' wildcard to glob up results in your filter.  In this example we want all 3 VPCs named 'webapp01', 'webapp02' and 'webapp03':\n\n<pre>\n>>> filters = [{'Name':'tag:Name', 'Values':['webapp*']}]\n>>> list(ec2.vpcs.filter(Filters=filters))\n[ec2.Vpc(id='vpc-11111111'), ec2.Vpc(id='vpc-22222222'), ec2.Vpc(id='vpc-33333333')]\n</pre>",
    "date": "2015-07-02 17:03:35",
    "timestamp": 1435871015,
    "comments": [],
    "metadata": {
      "_series_part": "1",
      "_spost_short_title": null,
      "_edit_last": "1"
    }
  },
  "list-all-installed-package-names-in-python": {
    "name": "list-all-installed-package-names-in-python",
    "id": "4469",
    "link": "http://russell2.ballestrini.net/list-all-installed-package-names-in-python/",
    "title": "List all installed package names in Python",
    "content": "<pre>\npkgs = lambda : list(__import__('pkg_resources').working_set)\n\npkg_names = lambda : [x.project_name for x in pkgs()]\n\npkg_versions = lambda : [x.project_name + '==' + x.version for x in pkgs()]\n</pre>\n\n<pre>\n>>> pkg_names()\n['ansible', 'pycrypto', 'PyYAML', 'Jinja2', '...truncated...', 'virt-back', 'Werkzeug', 'xmltodict']\n</pre>\n\n<pre>\n>>> pkg_versions()\n['ansible==1.7', 'pycrypto==2.6.1', '...truncated...', 'virt-back==0.1.0', 'xmltodict==0.9.2']\n</pre>",
    "date": "2015-07-04 22:15:04",
    "timestamp": 1436062504,
    "comments": [],
    "metadata": {
      "_spost_short_title": null,
      "_edit_last": "1"
    }
  },
  "boto3-get-main-route-table": {
    "name": "boto3-get-main-route-table",
    "id": "4486",
    "link": "http://russell2.ballestrini.net/boto3-get-main-route-table/",
    "title": "Boto3 get main route table",
    "content": "While developing Botoform I ran into an issue with Boto3 where I couldn't easily get the \"main\" route table of a VPC.  I ended up adding a <a href=\"https://github.com/russellballestrini/botoform/blob/master/botoform/enriched/vpc.py\">get_main_route_table</a> method to do the duty.\n\n<pre>\n    def get_main_route_table(self):\n        \"\"\"Return the main (default) route table for VPC.\"\"\"\n        main_route_table = []\n        for route_table in list(self.route_tables.all()):\n            for association in list(route_table.associations.all()):\n                if association.main == True:\n                    main_route_table.append(route_table)\n        if len(main_route_table) != 1:\n            raise Exception('cannot get main route table! {}'.format(main_route_table))\n        return main_route_table[0]\n</pre>",
    "date": "2015-10-16 12:15:34",
    "timestamp": 1445012134,
    "comments": [],
    "metadata": {
      "_wp_old_slug": "4486-2",
      "_spost_short_title": null,
      "_edit_last": "1",
      "_series_part": "6"
    }
  }
}